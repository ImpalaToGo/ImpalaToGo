<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- Generated by Apache Maven Doxia at Jan 7, 2014 -->
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Llama, High Level Design</title>
    <style type="text/css" media="all">
      @import url("./css/maven-base.css");
      @import url("./css/maven-theme.css");
      @import url("./css/site.css");
    </style>
    <link rel="stylesheet" href="./css/print.css" type="text/css" media="print" />
        <meta name="Date-Revision-yyyymmdd" content="20140107" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      </head>
  <body class="composite">
    <div id="banner">
                  <span id="bannerLeft">
                Llama
                </span>
                    <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="breadcrumbs">
            
                                <div class="xleft">
        Last Published: 2014-01-07
                  &nbsp;| Version: 1.0.0-cdh5.0.0-SNAPSHOT
                      </div>
            <div class="xright">            <a href="http://www.cloudera.com/" class="externalLink">Cloudera Inc.</a>
              
                      </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="leftColumn">
      <div id="navcolumn">
             
                                                <h5>Llama</h5>
                  <ul>
                  <li class="none">
            <strong>Overview</strong>
          </li>
                  <li class="none">
                  <a href="Llama.thrift">Thrift Definition</a>
            </li>
                  <li class="none">
                  <a href="ThriftInterfaceBehavior.html">Thrift Interface Behavior</a>
            </li>
                  <li class="none">
                  <a href="TestingWithLlama.html">Testing with Llama</a>
            </li>
                  <li class="none">
                  <a href="RunningLlama.html">Running Llama</a>
            </li>
                  <li class="none">
                  <a href="llama-site.html">LlamaAM Configuration</a>
            </li>
          </ul>
                                 <a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy">
          <img alt="Built by Maven" src="./images/logos/maven-feather.png"/>
        </a>
                       
                            </div>
    </div>
    <div id="bodyColumn">
      <div id="contentBox">
        <!-- Licensed under the Apache License, Version 2.0 (the "License"); --><!-- you may not use this file except in compliance with the License. --><!-- You may obtain a copy of the License at --><!--  --><!-- http://www.apache.org/licenses/LICENSE-2.0 --><!--  --><!-- Unless required by applicable law or agreed to in writing, software --><!-- distributed under the License is distributed on an "AS IS" BASIS, --><!-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. --><!-- See the License for the specific language governing permissions and --><!-- limitations under the License. --><div class="section"><h2>Llama - Low Latency Application MAster<a name="Llama_-_Low_Latency_Application_MAster"></a></h2><div class="section"><h3>Using Hadoop Resources for Low-Latency Processing from Impala<a name="Using_Hadoop_Resources_for_Low-Latency_Processing_from_Impala"></a></h3><p>Llama is a system that mediates resource management between Impala and Hadoop Yarn. Llama enables Impala to reserve, use and release resource allocations without requiring Impala to use Yarn-managed container processes.</p><ul><li><a href="#Problem">Problem</a></li><li><a href="#Solution_Space">Solution Space</a><ul><li><a href="#Service_Level_Isolation_Outside_Yarn">Service Level Isolation Outside Yarn</a></li><li><a href="#Model_JobsQueries_as_Yarn_Applications">Model Jobs/Queries as Yarn Applications</a></li><li><a href="#Model_a_Service_as_a_Yarn_Application">Model a Service as a Yarn Application</a></li><li><a href="#Multiplex_JobQueries_requests_over_a_Set_of_Yarn_Queues.">Multiplex Job/Queries requests over a Set of Yarn Queues.</a></li></ul></li><li><a href="#Background">Background</a></li><li><a href="#Main_Drivers">Main Drivers</a><ul><li><a href="#Use_Yarn_Managed_Resources_from_Impala">Use Yarn Managed Resources from Impala</a></li><li><a href="#Very_High_Throughput_of_Queries_with_Low_Latency">Very High Throughput of Queries with Low Latency</a></li><li><a href="#Handle_Resource_Allocations_from_Multiple_Scheduler_Queues">Handle Resource Allocations from Multiple Scheduler Queues</a></li><li><a href="#Leverage_the_Hadoop_Clusters_Free_Capacity_for_Opportunistic_Processing">Leverage the Hadoop Cluster&#x2019;s Free Capacity for Opportunistic Processing</a></li></ul></li><li><a href="#High_Level_Design">High Level Design</a></li><li><a href="#Implementation_Highlights">Implementation Highlights</a><ul><li><a href="#API">API</a></li><li><a href="#Allocating_Resources_in_Multiple_Scheduler_Queues">Allocating Resources in Multiple Scheduler Queues</a></li><li><a href="#Long_Running_Application_Masters_per_Scheduler_Queue">Long Running Application Masters per Scheduler Queue</a></li><li><a href="#Unmanaged_Containers">Unmanaged Containers</a></li><li><a href="#Gang_Scheduling">Gang Scheduling</a></li></ul></li></ul><div class="section"><h4>Problem<a name="Problem"></a></h4><p>Currently Hadoop Yarn expects to manage the lifecycle of the processes its applications run work in. Some frameworks/systems that could benefit from sharing resources with Yarn run work within long-running processes owned by the framework. These processes are not a good fit for Yarn containers because they need to run work on behalf of many users on many queues. We've been working on how to integrate Impala with Yarn, and we think our ideas might be applicable to other frameworks, Spark for example, as well.</p></div><div class="section"><h4>Solution Space<a name="Solution_Space"></a></h4><p>There are different ways of integrating long lived services with Yarn. Using a service X as example, the different approaches to integrate service X with yarn can be characterized as follows.</p><div class="section"><h5>Service Level Isolation Outside Yarn<a name="Service_Level_Isolation_Outside_Yarn"></a></h5><p>Carving out (i.e. via Linux cgroups) part of the cluster nodes for service X and part for Yarn. This is better than just sharing the same cluster and hoping for the best.</p><p>This approach is simple and effective, for example, assigning 30% to service X and 70% to Yarn. However, it lacks flexibility as load conditions change. Also, it is not possible to assign a common share to a user or a queue across service X and Yarn.</p></div><div class="section"><h5>Model Jobs/Queries as Yarn Applications<a name="Model_JobsQueries_as_Yarn_Applications"></a></h5><p>Each job submitted to service X results in an individual Yarn application.</p><p>Optimizations such as pooling and reusing Yarn application instances, and reusing containers within a Yarn application can help reducing latency significantly.</p><p>An important benefit of this approach is that resources utilization and distribution is completely managed by Yarn. However, this approach assumes each job submitted to service X is a set of independent processes from service X's job dispatcher.</p></div><div class="section"><h5>Model a Service as a Yarn Application<a name="Model_a_Service_as_a_Yarn_Application"></a></h5><p>Implementing service X as a Yarn application. With this approach, Yarn's scheduler is responsible for sharing resources among different users and applications. Because service X is a yarn application, resources utilization for service X and other Yarn applications is dynamic based on the load conditions. A drawback for this approach is a service X instance to one scheduler queue. While it is possible to run multiple service X Yarn applications, one per queue, this may not be practical because the different service X instances may have to coordinate among each other and may have to duplicate their resource headroom</p><p>utilization. In addition, as stated in YARN-896, there is significant work to be done for long live services running a Yarn applications.</p></div><div class="section"><h5>Multiplex Job/Queries requests over a Set of Yarn Queues.<a name="Multiplex_JobQueries_requests_over_a_Set_of_Yarn_Queues."></a></h5><p>Making service X use a set of unmanaged Yarn applications to request resources for the different jobs submitted to service X.</p><p>In this scenario, service X manages its processes on its own using resources allocated through Yarn, the resources distribution is completely managed by Yarn. If service X runs its jobs in process, common data structures can be easily shared among different jobs without duplication. A drawback of this approach is that service X must managed on its own the enforcement of resource utilization per job.</p><p>Our proposal for Impala, Llama, fits this approach.</p></div></div><div class="section"><h4>Background<a name="Background"></a></h4><p>Coexistence of low-latency and batch processing is not an easy problem to solve if the intention is to guarantee appropriate response times, maximize the utilization of hardware resources, ensure fairness and be flexible based on the current load mix.</p><p>Having a mechanism where low-latency and batch processing can get a guaranteed share of the cluster dynamically depending on the current load conditions will lead to better resources utilization and adaptability based on the current usage and demand.</p><p>As Impala and Hadoop share the same hardware, it seemed natural to make Impala acquire and use resources from Hadoop Yarn for executing queries. The benefits of this approach are significant:</p><ul><li>A single configuration on how cluster resources are shared is used for both low-latency and batch processing.</li><li>Users and organizations using the cluster get their share regardless of the type of processing they are doing.</li><li>Unified accounting and monitoring of cluster utilization.</li></ul><p>This kind of integration presents challenges as well. Impala must request resources from Hadoop before executing a query. Impala architecture is significantly different from Hadoop Yarn architecture. While each Hadoop jobs spawn several new processes throughout the cluster, in Impala, all queries run in-process within existing long-running processes.</p></div><div class="section"><h4>Main Drivers<a name="Main_Drivers"></a></h4><div class="section"><h5>Use Yarn Managed Resources from Impala<a name="Use_Yarn_Managed_Resources_from_Impala"></a></h5><p>In order to share resources of a Hadoop cluster and maximize their utilization base on the current load, Impala should negotiate cluster resource utilization with Yarn Resource Manager using the same mechanisms that other applications (i.e. Map-Reduce) use.</p><p>Using the scheduler in Yarn&#x2019;s Resource Manager as the ultimate source of truth for scheduling decisions enables leveraging the existing work done in Hadoop regarding scheduling.</p></div><div class="section"><h5>Very High Throughput of Queries with Low Latency<a name="Very_High_Throughput_of_Queries_with_Low_Latency"></a></h5><p>Impala is designed to run 1000s of concurrent queries, many of them running with few-seconds (and even sub-second) latencies.</p><p>We conducted some benchmarks to determine how fast Hadoop can start application masters. We ran them using unmanaged application masters (which start significantly faster than managed application masters).</p><p>We found that the first application master created from a client process takes around 900 ms to be ready to submit resource requests. Subsequent application masters created from the same client process take a mean of 20 ms. The application master submission throughput (discarding the first submission) tops at approximately 100 application masters per second.</p><p>While these numbers speak quite well about Hadoop Yarn, because of Impala&#x2019;s expected query throughput, we could not go with a design that uses one application master per query. Another option considered was having a pool of running application masters checking them in and out to execute queries (conceptually not different from the well known and used connection pooling). Because Impala runs all queries in-process this approach does not bring a performance advantage, and adds the complexity of managing pools of applications masters that must be checked in and out per query. In addition, a pooled application master could have a set of allocated containers from the previous query. For a new query those allocations may not be relevant due to a mismatch in locality or resource capabilities requirements of a new query.</p></div><div class="section"><h5>Handle Resource Allocations from Multiple Scheduler Queues<a name="Handle_Resource_Allocations_from_Multiple_Scheduler_Queues"></a></h5><p>An Impala query, similar to a Yarn application, is assigned to a particular scheduler queue and all the resources it consumes must be accounted from the corresponding scheduler queue.</p><p>Because Impala may run - concurrently - queries assigned to several different queues, Impala must be able to request resources from multiple scheduler queues. Each Impala daemon may run work on behalf of multiple queries, and thus multiple queues, at the same time.</p></div><div class="section"><h5>Leverage the Hadoop Cluster&#x2019;s Free Capacity for Opportunistic Processing<a name="Leverage_the_Hadoop_Clusters_Free_Capacity_for_Opportunistic_Processing"></a></h5><p>Typically, Impala will acquire from Hadoop&#x2019;s scheduler the necessary resources before executing queries. This is especially true when the Hadoop cluster is under load.</p><p>However, when the Hadoop cluster has sufficient idle capacity or the query is deemed lightweight enough, to speed up the query execution, Impala may bypass requesting resources from Hadoop and run the query in nodes of the cluster that have available capacity. This is referred to as opportunistic processing.</p><p>If the resources used for opportunistic processing are claimed by a Hadoop job, Impala will cancel the opportunistic processing for the query and will request the necessary resources through Yarn before attempting to execute the query again.</p></div></div><div class="section"><h4>High Level Design<a name="High_Level_Design"></a></h4><p>Llama consists of 2 components, an Application Master (Llama AM) and a Node Manager plugin (Llama NM-plugin).</p><p>The Llama AM is a standalone service. There is a single instance serving an Impala deployment. The Llama AM handles Impala resource requests (reserve and release) and delivers notifications regarding Hadoop Yarn resource status changes (allocations, rejections, preemptions, lost nodes) to Impala.</p><p>The Llama NM-plugin is a Yarn auxiliary service that runs in all Yarn NodeManager instances of the cluster. The Llama-NM plugin delivers notifications to the co-located Impala service regarding changes in the availability of resources in the NodeManager such as created, completed, preempted and lost containers as well as the currently available capacity.</p></div><div class="section"><h4>Implementation Highlights<a name="Implementation_Highlights"></a></h4><div class="section"><h5>API<a name="API"></a></h5><p>Llama provides a Thrift API to reserve and release allocations.</p></div><div class="section"><h5>Allocating Resources in Multiple Scheduler Queues<a name="Allocating_Resources_in_Multiple_Scheduler_Queues"></a></h5><p>A Yarn application is coordinated by a single Application Master (AM) and associated with a single scheduler queue. This association is fixed at application creation time and it cannot be changed for the lifetime of the Yarn application instance. Llama provides support for allocating resources to multiple queues by internally using a Yarn AM instance per queue.</p></div><div class="section"><h5>Long Running Application Masters per Scheduler Queue<a name="Long_Running_Application_Masters_per_Scheduler_Queue"></a></h5><p>Llama AM starts an AM instance per scheduler queue. Even when there are no outstanding reservations or allocations for a particular queue, Llama AM will keep the AM instance for the queue running.</p><p>All reservations/allocations for a particular queue are done via the same AM instance, Llama AM multiplexes all reservations on a per queue basis. Yarn does not have visibility into individual Impala queries, just into the total load requested/used by Impala per queue. Llama AM monitors outstanding requests, allocates capacity, and handles preemption at the queue level.</p></div><div class="section"><h5>Unmanaged Containers<a name="Unmanaged_Containers"></a></h5><p>When Llama receives an allocation from Yarn, besides notifying Impala about the allocation, it starts a 'dummy' Yarn container. &#x2018;dummy&#x2019; container processes sleep forever and do not consume &#x2018;significant&#x2019; resources. &#x2018;dummy&#x2019; containers are used to keep Yarn&#x2019;s existing container allocation management and reporting intact. Impala must explicitly release its allocations via Llama AM, which will ask the appropriate Node Manager to kill those dummy containers corresponding to the released resources.</p><p>Impala uses the resources out of band from Yarn ensuring the utilization is within the allocated capabilities.</p></div><div class="section"><h5>Gang Scheduling<a name="Gang_Scheduling"></a></h5><p>Impala runs a query as a collection of 'query fragments', loosely equivalent to Map-Reduce 'tasks'. Differently from Map-Reduce tasks, which write their output to local disk or HDFS, Impala keeps the query fragments&#x2019; intermediate output data in memory. Impala's execution model is heavily pipelined. Impala therefore requires that query fragments run concurrently, unlike the Map-Reduce execution model, which is checkpoint-based. Therefore, Impala must wait until allocations are available at all the nodes needed to run a query before the query starts.</p><p>In other words, Impala needs all its resources at once to process a query. Having a gang scheduling mechanism in place in the resource scheduler means better cluster resource utilization because the scheduler can plan/project when a set of resources will be used as part of a gang reservation.</p><p>Currently, support for gang scheduling in Hadoop is under discussion, YARN-624.</p><p>As a stopgap measure until gang scheduling is supported by Hadoop, Llama implements client side gang scheduling by waiting for all allocations of a gang reservation to be granted before notifying Impala. Llama implements logic to avoid deadlocks (timeout, release, wait, re-reserve) and uses probabilistic fairness for gang reservations.</p><p>Note that client side gang scheduling, like Llama implements, does not drive a better cluster resource utilization but the opposite as allocated resources are withheld by Llama without being utilized. In addition, client side deadlock detection does not scale well beyond a single gang-scheduling AM.</p></div></div></div></div>
      </div>
    </div>
    <div class="clear">
      <hr/>
    </div>
    <div id="footer">
      <div class="xright">&#169;            2014
              Cloudera Inc
            
                       - <a href="http://maven.apache.org/privacy-policy.html">Privacy Policy</a></div>
      <div class="clear">
        <hr/>
      </div>
    </div>
  </body>
</html>
