<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>Chapter&nbsp;1.&nbsp;Performance Tuning</title><link rel="stylesheet" type="text/css" href="css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="chapter" title="Chapter&nbsp;1.&nbsp;Performance Tuning"><div class="titlepage"><div><div><h2 class="title"><a name="performance"></a>Chapter&nbsp;1.&nbsp;Performance Tuning</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#perf.os">1.1. Operating System</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.os.ram">1.1.1. Memory</a></span></dt><dt><span class="section"><a href="#perf.os.64">1.1.2. 64-bit</a></span></dt><dt><span class="section"><a href="#perf.os.swap">1.1.3. Swapping</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.network">1.2. Network</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.network.1switch">1.2.1. Single Switch</a></span></dt><dt><span class="section"><a href="#perf.network.2switch">1.2.2. Multiple Switches</a></span></dt><dt><span class="section"><a href="#perf.network.multirack">1.2.3. Multiple Racks</a></span></dt></dl></dd><dt><span class="section"><a href="#jvm">1.3. Java</a></span></dt><dd><dl><dt><span class="section"><a href="#gc">1.3.1. The Garbage Collector and HBase</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.configurations">1.4. HBase Configurations</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.number.of.regions">1.4.1. Number of Regions</a></span></dt><dt><span class="section"><a href="#perf.compactions.and.splits">1.4.2. Managing Compactions</a></span></dt><dt><span class="section"><a href="#perf.handlers">1.4.3. <code class="varname">hbase.regionserver.handler.count</code></a></span></dt><dt><span class="section"><a href="#perf.hfile.block.cache.size">1.4.4. <code class="varname">hfile.block.cache.size</code></a></span></dt><dt><span class="section"><a href="#perf.rs.memstore.upperlimit">1.4.5. <code class="varname">hbase.regionserver.global.memstore.upperLimit</code></a></span></dt><dt><span class="section"><a href="#perf.rs.memstore.lowerlimit">1.4.6. <code class="varname">hbase.regionserver.global.memstore.lowerLimit</code></a></span></dt><dt><span class="section"><a href="#perf.hstore.blockingstorefiles">1.4.7. <code class="varname">hbase.hstore.blockingStoreFiles</code></a></span></dt><dt><span class="section"><a href="#perf.hregion.memstore.block.multiplier">1.4.8. <code class="varname">hbase.hregion.memstore.block.multiplier</code></a></span></dt></dl></dd><dt><span class="section"><a href="#perf.schema">1.5. Schema Design</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.number.of.cfs">1.5.1. Number of Column Families</a></span></dt><dt><span class="section"><a href="#perf.schema.keys">1.5.2. Key and Attribute Lengths</a></span></dt><dt><span class="section"><a href="#schema.regionsize">1.5.3. Table RegionSize</a></span></dt><dt><span class="section"><a href="#schema.bloom">1.5.4. Bloom Filters</a></span></dt><dt><span class="section"><a href="#schema.cf.blocksize">1.5.5. ColumnFamily BlockSize</a></span></dt><dt><span class="section"><a href="#cf.in.memory">1.5.6. In-Memory ColumnFamilies</a></span></dt><dt><span class="section"><a href="#perf.compression">1.5.7. Compression</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.writing">1.6. Writing to HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.batch.loading">1.6.1. Batch Loading</a></span></dt><dt><span class="section"><a href="#precreate.regions">1.6.2. 
    Table Creation: Pre-Creating Regions
    </a></span></dt><dt><span class="section"><a href="#def.log.flush">1.6.3. 
    Table Creation: Deferred Log Flush
    </a></span></dt><dt><span class="section"><a href="#perf.hbase.client.autoflush">1.6.4. HBase Client:  AutoFlush</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.putwal">1.6.5. HBase Client:  Turn off WAL on Puts</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.regiongroup">1.6.6. HBase Client: Group Puts by RegionServer</a></span></dt><dt><span class="section"><a href="#perf.hbase.write.mr.reducer">1.6.7. MapReduce:  Skip The Reducer</a></span></dt><dt><span class="section"><a href="#perf.one.region">1.6.8. Anti-Pattern:  One Hot Region</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.reading">1.7. Reading from HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.hbase.client.caching">1.7.1. Scan Caching</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.selection">1.7.2. Scan Attribute Selection</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.scannerclose">1.7.3. Close ResultScanners</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.blockcache">1.7.4. Block Cache</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.rowkeyonly">1.7.5. Optimal Loading of Row Keys</a></span></dt><dt><span class="section"><a href="#perf.hbase.read.dist">1.7.6. Concurrency:  Monitor Data Spread</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.deleting">1.8. Deleting from HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.deleting.queue">1.8.1. Using HBase Tables as Queues</a></span></dt><dt><span class="section"><a href="#perf.deleting.rpc">1.8.2. Delete RPC Behavior</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.hdfs">1.9. HDFS</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.hdfs.curr">1.9.1. Current Issues With Low-Latency Reads</a></span></dt><dt><span class="section"><a href="#perf.hdfs.comp">1.9.2. Performance Comparisons of HBase vs. HDFS</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.ec2">1.10. Amazon EC2</a></span></dt></dl></div><div class="section" title="1.1.&nbsp;Operating System"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.os"></a>1.1.&nbsp;Operating System</h2></div></div></div><div class="section" title="1.1.1.&nbsp;Memory"><div class="titlepage"><div><div><h3 class="title"><a name="perf.os.ram"></a>1.1.1.&nbsp;Memory</h3></div></div></div><p>RAM, RAM, RAM.  Don't starve HBase.</p></div><div class="section" title="1.1.2.&nbsp;64-bit"><div class="titlepage"><div><div><h3 class="title"><a name="perf.os.64"></a>1.1.2.&nbsp;64-bit</h3></div></div></div><p>Use a 64-bit platform (and 64-bit JVM).</p></div><div class="section" title="1.1.3.&nbsp;Swapping"><div class="titlepage"><div><div><h3 class="title"><a name="perf.os.swap"></a>1.1.3.&nbsp;Swapping</h3></div></div></div><p>Watch out for swapping.  Set swappiness to 0.</p></div></div><div class="section" title="1.2.&nbsp;Network"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.network"></a>1.2.&nbsp;Network</h2></div></div></div><p>
    Perhaps the most important factor in avoiding network issues degrading Hadoop and HBbase performance is the switching hardware
    that is used, decisions made early in the scope of the project can cause major problems when you double or triple the size of your cluster (or more). 
    </p><p>
    Important items to consider:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Switching capacity of the device</li><li class="listitem">Number of systems connected</li><li class="listitem">Uplink capacity</li></ul></div><p>
    </p><div class="section" title="1.2.1.&nbsp;Single Switch"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.1switch"></a>1.2.1.&nbsp;Single Switch</h3></div></div></div><p>The single most important factor in this configuration is that the switching capacity of the hardware is capable of 
      handling the traffic which can be generated by all systems connected to the switch. Some lower priced commodity hardware
      can have a slower switching capacity than could be utilized by a full switch. 
      </p></div><div class="section" title="1.2.2.&nbsp;Multiple Switches"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.2switch"></a>1.2.2.&nbsp;Multiple Switches</h3></div></div></div><p>Multiple switches are a potential pitfall in the architecture.   The most common configuration of lower priced hardware is a
      simple 1Gbps uplink from one switch to another. This often overlooked pinch point can easily become a bottleneck for cluster communication. 
      Especially with MapReduce jobs that are both reading and writing a lot of data the communication across this uplink could be saturated.
      </p><p>Mitigation of this issue is fairly simple and can be accomplished in multiple ways:
      </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Use appropriate hardware for the scale of the cluster which you're attempting to build.</li><li class="listitem">Use larger single switch configurations i.e. single 48 port as opposed to 2x 24 port</li><li class="listitem">Configure port trunking for uplinks to utilize multiple interfaces to increase cross switch bandwidth.</li></ul></div><p>
      </p></div><div class="section" title="1.2.3.&nbsp;Multiple Racks"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.multirack"></a>1.2.3.&nbsp;Multiple Racks</h3></div></div></div><p>Multiple rack configurations carry the same potential issues as multiple switches, and can suffer performance degradation from two main areas:
         </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Poor switch capacity performance</li><li class="listitem">Insufficient uplink to another rack</li></ul></div><p>
      If the the switches in your rack have appropriate switching capacity to handle all the hosts at full speed, the next most likely issue will be caused by homing 
      more of your cluster across racks.  The easiest way to avoid issues when spanning multiple racks is to use port trunking to create a bonded uplink to other racks.
      The downside of this method however, is in the overhead of ports that could potentially be used. An example of this is, creating an 8Gbps port channel from rack
      A to rack B, using 8 of your 24 ports to communicate between racks gives you a poor ROI, using too few however can mean you're not getting the most out of your cluster. 
      </p><p>Using 10Gbe links between racks will greatly increase performance, and assuming your switches support a 10Gbe uplink or allow for an expansion card will allow you to
      save your ports for machines as opposed to uplinks.
      </p></div></div><div class="section" title="1.3.&nbsp;Java"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="jvm"></a>1.3.&nbsp;Java</h2></div></div></div><div class="section" title="1.3.1.&nbsp;The Garbage Collector and HBase"><div class="titlepage"><div><div><h3 class="title"><a name="gc"></a>1.3.1.&nbsp;The Garbage Collector and HBase</h3></div></div></div><div class="section" title="1.3.1.1.&nbsp;Long GC pauses"><div class="titlepage"><div><div><h4 class="title"><a name="gcpause"></a>1.3.1.1.&nbsp;Long GC pauses</h4></div></div></div><p>In his presentation, <a class="link" href="http://www.slideshare.net/cloudera/hbase-hug-presentation" target="_top">Avoiding
        Full GCs with MemStore-Local Allocation Buffers</a>, Todd Lipcon
        describes two cases of stop-the-world garbage collections common in
        HBase, especially during loading; CMS failure modes and old generation
        heap fragmentation brought. To address the first, start the CMS
        earlier than default by adding
        <code class="code">-XX:CMSInitiatingOccupancyFraction</code> and setting it down
        from defaults. Start at 60 or 70 percent (The lower you bring down the
        threshold, the more GCing is done, the more CPU used). To address the
        second fragmentation issue, Todd added an experimental facility that
        must be explicitly enabled in HBase 0.90.x (Its defaulted to be on in
        0.92.x HBase). See <code class="code">hbase.hregion.memstore.mslab.enabled</code>
        to true in your <code class="classname">Configuration</code>. See the cited
        slides for background and detail<sup>[<a name="d3094e95" href="#ftn.d3094e95" class="footnote">1</a>]</sup>.</p><p>For more information about GC logs, see <a class="xref" href="#">???</a>.
        </p></div></div></div><div class="section" title="1.4.&nbsp;HBase Configurations"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.configurations"></a>1.4.&nbsp;HBase Configurations</h2></div></div></div><p>See <a class="xref" href="#">???</a>.</p><div class="section" title="1.4.1.&nbsp;Number of Regions"><div class="titlepage"><div><div><h3 class="title"><a name="perf.number.of.regions"></a>1.4.1.&nbsp;Number of Regions</h3></div></div></div><p>The number of regions for an HBase table is driven by the <a class="xref" href="#">???</a>. Also, see the architecture
          section on <a class="xref" href="#">???</a></p><p>A lower number of regions is preferred, generally in the range of 20 to low-hundreds
       per RegionServer.  Adjust the regionsize as appropriate to achieve this number. 
       </p><p>For the 0.90.x codebase, the upper-bound of regionsize is about 4Gb.
       For 0.92.x codebase, due to the HFile v2 change much larger regionsizes can be supported (e.g., 20Gb).
       </p><p>You may need to experiment with this setting based on your hardware configuration and application needs.
       </p></div><div class="section" title="1.4.2.&nbsp;Managing Compactions"><div class="titlepage"><div><div><h3 class="title"><a name="perf.compactions.and.splits"></a>1.4.2.&nbsp;Managing Compactions</h3></div></div></div><p>For larger systems, managing <a class="link" href="#">compactions and splits</a> may be
      something you want to consider.</p></div><div class="section" title="1.4.3.&nbsp;hbase.regionserver.handler.count"><div class="titlepage"><div><div><h3 class="title"><a name="perf.handlers"></a>1.4.3.&nbsp;<code class="varname">hbase.regionserver.handler.count</code></h3></div></div></div><p>See <a class="xref" href="#">???</a>. 
            This setting in essence sets how many requests are
            concurrently being processed inside the RegionServer at any
            one time.  If set too high, then throughput may suffer as
            the concurrent requests contend; if set too low, requests will
            be stuck waiting to get into the machine.  You can get a
            sense of whether you have too little or too many handlers by
            <a class="xref" href="#">???</a>
            on an individual RegionServer then tailing its logs (Queued requests
            consume memory).</p></div><div class="section" title="1.4.4.&nbsp;hfile.block.cache.size"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hfile.block.cache.size"></a>1.4.4.&nbsp;<code class="varname">hfile.block.cache.size</code></h3></div></div></div><p>See <a class="xref" href="#">???</a>. 
        A memory setting for the RegionServer process.
        </p></div><div class="section" title="1.4.5.&nbsp;hbase.regionserver.global.memstore.upperLimit"><div class="titlepage"><div><div><h3 class="title"><a name="perf.rs.memstore.upperlimit"></a>1.4.5.&nbsp;<code class="varname">hbase.regionserver.global.memstore.upperLimit</code></h3></div></div></div><p>See <a class="xref" href="#">???</a>.  
        This memory setting is often adjusted for the RegionServer process depending on needs.
        </p></div><div class="section" title="1.4.6.&nbsp;hbase.regionserver.global.memstore.lowerLimit"><div class="titlepage"><div><div><h3 class="title"><a name="perf.rs.memstore.lowerlimit"></a>1.4.6.&nbsp;<code class="varname">hbase.regionserver.global.memstore.lowerLimit</code></h3></div></div></div><p>See <a class="xref" href="#">???</a>.  
        This memory setting is often adjusted for the RegionServer process depending on needs.
        </p></div><div class="section" title="1.4.7.&nbsp;hbase.hstore.blockingStoreFiles"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hstore.blockingstorefiles"></a>1.4.7.&nbsp;<code class="varname">hbase.hstore.blockingStoreFiles</code></h3></div></div></div><p>See <a class="xref" href="#">???</a>.  
        If there is blocking in the RegionServer logs, increasing this can help.
        </p></div><div class="section" title="1.4.8.&nbsp;hbase.hregion.memstore.block.multiplier"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hregion.memstore.block.multiplier"></a>1.4.8.&nbsp;<code class="varname">hbase.hregion.memstore.block.multiplier</code></h3></div></div></div><p>See <a class="xref" href="#">???</a>.  
        If there is enough RAM, increasing this can help.  
        </p></div></div><div class="section" title="1.5.&nbsp;Schema Design"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.schema"></a>1.5.&nbsp;Schema Design</h2></div></div></div><div class="section" title="1.5.1.&nbsp;Number of Column Families"><div class="titlepage"><div><div><h3 class="title"><a name="perf.number.of.cfs"></a>1.5.1.&nbsp;Number of Column Families</h3></div></div></div><p>See <a class="xref" href="#">???</a>.</p></div><div class="section" title="1.5.2.&nbsp;Key and Attribute Lengths"><div class="titlepage"><div><div><h3 class="title"><a name="perf.schema.keys"></a>1.5.2.&nbsp;Key and Attribute Lengths</h3></div></div></div><p>See <a class="xref" href="#">???</a>.</p></div><div class="section" title="1.5.3.&nbsp;Table RegionSize"><div class="titlepage"><div><div><h3 class="title"><a name="schema.regionsize"></a>1.5.3.&nbsp;Table RegionSize</h3></div></div></div><p>The regionsize can be set on a per-table basis via <code class="code">setFileSize</code> on
    <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html" target="_top">HTableDescriptor</a> in the 
    event where certain tables require different regionsizes than the configured default regionsize.
    </p><p>See <a class="xref" href="#perf.number.of.regions" title="1.4.1.&nbsp;Number of Regions">Section&nbsp;1.4.1, &#8220;Number of Regions&#8221;</a> for more information.
    </p></div><div class="section" title="1.5.4.&nbsp;Bloom Filters"><div class="titlepage"><div><div><h3 class="title"><a name="schema.bloom"></a>1.5.4.&nbsp;Bloom Filters</h3></div></div></div><p>Bloom Filters can be enabled per-ColumnFamily.
        Use <code class="code">HColumnDescriptor.setBloomFilterType(NONE | ROW |
        ROWCOL)</code> to enable blooms per Column Family. Default =
        <code class="varname">NONE</code> for no bloom filters. If
        <code class="varname">ROW</code>, the hash of the row will be added to the bloom
        on each insert. If <code class="varname">ROWCOL</code>, the hash of the row +
        column family + column family qualifier will be added to the bloom on
        each key insert.</p><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a> and 
    <a class="xref" href="#">???</a> for more information.
    </p></div><div class="section" title="1.5.5.&nbsp;ColumnFamily BlockSize"><div class="titlepage"><div><div><h3 class="title"><a name="schema.cf.blocksize"></a>1.5.5.&nbsp;ColumnFamily BlockSize</h3></div></div></div><p>The blocksize can be configured for each ColumnFamily in a table, and this defaults to 64k.  Larger cell values require larger blocksizes. 
    There is an inverse relationship between blocksize and the resulting StoreFile indexes (i.e., if the blocksize is doubled then the resulting
    indexes should be roughly halved).
    </p><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a> 
    and <a class="xref" href="#">???</a>for more information.
    </p></div><div class="section" title="1.5.6.&nbsp;In-Memory ColumnFamilies"><div class="titlepage"><div><div><h3 class="title"><a name="cf.in.memory"></a>1.5.6.&nbsp;In-Memory ColumnFamilies</h3></div></div></div><p>ColumnFamilies can optionally be defined as in-memory.  Data is still persisted to disk, just like any other ColumnFamily.  
    In-memory blocks have the highest priority in the <a class="xref" href="#">???</a>, but it is not a guarantee that the entire table
    will be in memory.
    </p><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a> for more information.
    </p></div><div class="section" title="1.5.7.&nbsp;Compression"><div class="titlepage"><div><div><h3 class="title"><a name="perf.compression"></a>1.5.7.&nbsp;Compression</h3></div></div></div><p>Production systems should use compression with their ColumnFamily definitions.  See <a class="xref" href="#">???</a> for more information.
      </p></div></div><div class="section" title="1.6.&nbsp;Writing to HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.writing"></a>1.6.&nbsp;Writing to HBase</h2></div></div></div><div class="section" title="1.6.1.&nbsp;Batch Loading"><div class="titlepage"><div><div><h3 class="title"><a name="perf.batch.loading"></a>1.6.1.&nbsp;Batch Loading</h3></div></div></div><p>Use the bulk load tool if you can.  See
        <a class="link" href="http://hbase.apache.org/bulk-loads.html" target="_top">Bulk Loads</a>.
        Otherwise, pay attention to the below.
      </p></div><div class="section" title="1.6.2.&nbsp; Table Creation: Pre-Creating Regions"><div class="titlepage"><div><div><h3 class="title"><a name="precreate.regions"></a>1.6.2.&nbsp;
    Table Creation: Pre-Creating Regions
    </h3></div></div></div><p>
Tables in HBase are initially created with one region by default.  For bulk imports, this means that all clients will write to the same region until it is large enough to split and become distributed across the cluster.  A useful pattern to speed up the bulk import process is to pre-create empty regions.  Be somewhat conservative in this, because too-many regions can actually degrade performance.  An example of pre-creation using hex-keys is as follows (note:  this example may need to be tweaked to the individual applications keys):
</p><p>
</p><pre class="programlisting">public static boolean createTable(HBaseAdmin admin, HTableDescriptor table, byte[][] splits)
throws IOException {
  try {
    admin.createTable( table, splits );
    return true;
  } catch (TableExistsException e) {
    logger.info("table " + table.getNameAsString() + " already exists");
    // the table already exists...
    return false;  
  }
}

public static byte[][] getHexSplits(String startKey, String endKey, int numRegions) {
  byte[][] splits = new byte[numRegions-1][];
  BigInteger lowestKey = new BigInteger(startKey, 16);
  BigInteger highestKey = new BigInteger(endKey, 16);
  BigInteger range = highestKey.subtract(lowestKey);
  BigInteger regionIncrement = range.divide(BigInteger.valueOf(numRegions));
  lowestKey = lowestKey.add(regionIncrement);
  for(int i=0; i &lt; numRegions-1;i++) {
    BigInteger key = lowestKey.add(regionIncrement.multiply(BigInteger.valueOf(i)));
    byte[] b = String.format("%016x", key).getBytes();
    splits[i] = b;
  }
  return splits;
}</pre><p>
  </p></div><div class="section" title="1.6.3.&nbsp; Table Creation: Deferred Log Flush"><div class="titlepage"><div><div><h3 class="title"><a name="def.log.flush"></a>1.6.3.&nbsp;
    Table Creation: Deferred Log Flush
    </h3></div></div></div><p>
The default behavior for Puts using the Write Ahead Log (WAL) is that <code class="classname">HLog</code> edits will be written immediately.  If deferred log flush is used, 
WAL edits are kept in memory until the flush period.  The benefit is aggregated and asynchronous <code class="classname">HLog</code>- writes, but the potential downside is that if
 the RegionServer goes down the yet-to-be-flushed edits are lost.  This is safer, however, than not using WAL at all with Puts.
</p><p>
Deferred log flush can be configured on tables via <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html" target="_top">HTableDescriptor</a>.  The default value of <code class="varname">hbase.regionserver.optionallogflushinterval</code> is 1000ms.
</p></div><div class="section" title="1.6.4.&nbsp;HBase Client: AutoFlush"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.autoflush"></a>1.6.4.&nbsp;HBase Client:  AutoFlush</h3></div></div></div><p>When performing a lot of Puts, make sure that setAutoFlush is set
      to false on your <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html" target="_top">HTable</a>
      instance. Otherwise, the Puts will be sent one at a time to the
      RegionServer. Puts added via <code class="code"> htable.add(Put)</code> and <code class="code"> htable.add( &lt;List&gt; Put)</code>
      wind up in the same write buffer. If <code class="code">autoFlush = false</code>,
      these messages are not sent until the write-buffer is filled. To
      explicitly flush the messages, call <code class="methodname">flushCommits</code>.
      Calling <code class="methodname">close</code> on the <code class="classname">HTable</code>
      instance will invoke <code class="methodname">flushCommits</code>.</p></div><div class="section" title="1.6.5.&nbsp;HBase Client: Turn off WAL on Puts"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.putwal"></a>1.6.5.&nbsp;HBase Client:  Turn off WAL on Puts</h3></div></div></div><p>A frequently discussed option for increasing throughput on <code class="classname">Put</code>s is to call <code class="code">writeToWAL(false)</code>.  Turning this off means
          that the RegionServer will <span class="emphasis"><em>not</em></span> write the <code class="classname">Put</code> to the Write Ahead Log,
          only into the memstore, HOWEVER the consequence is that if there
          is a RegionServer failure <span class="emphasis"><em>there will be data loss</em></span>.
          If <code class="code">writeToWAL(false)</code> is used, do so with extreme caution.  You may find in actuality that
          it makes little difference if your load is well distributed across the cluster.
      </p><p>In general, it is best to use WAL for Puts, and where loading throughput
          is a concern to use <a class="link" href="#perf.batch.loading" title="1.6.1.&nbsp;Batch Loading">bulk loading</a> techniques instead.  
      </p></div><div class="section" title="1.6.6.&nbsp;HBase Client: Group Puts by RegionServer"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.regiongroup"></a>1.6.6.&nbsp;HBase Client: Group Puts by RegionServer</h3></div></div></div><p>In addition to using the writeBuffer, grouping <code class="classname">Put</code>s by RegionServer can reduce the number of client RPC calls per writeBuffer flush. 
      There is a utility <code class="classname">HTableUtil</code> currently on TRUNK that does this, but you can either copy that or implement your own verison for
      those still on 0.90.x or earlier.
      </p></div><div class="section" title="1.6.7.&nbsp;MapReduce: Skip The Reducer"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.write.mr.reducer"></a>1.6.7.&nbsp;MapReduce:  Skip The Reducer</h3></div></div></div><p>When writing a lot of data to an HBase table from a MR job (e.g., with <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.html" target="_top">TableOutputFormat</a>), and specifically where Puts are being emitted
      from the Mapper, skip the Reducer step.  When a Reducer step is used, all of the output (Puts) from the Mapper will get spooled to disk, then sorted/shuffled to other 
      Reducers that will most likely be off-node.  It's far more efficient to just write directly to HBase.   
      </p><p>For summary jobs where HBase is used as a source and a sink, then writes will be coming from the Reducer step (e.g., summarize values then write out result). 
      This is a different processing problem than from the the above case. 
      </p></div><div class="section" title="1.6.8.&nbsp;Anti-Pattern: One Hot Region"><div class="titlepage"><div><div><h3 class="title"><a name="perf.one.region"></a>1.6.8.&nbsp;Anti-Pattern:  One Hot Region</h3></div></div></div><p>If all your data is being written to one region at a time, then re-read the
    section on processing <a class="link" href="#">timeseries</a> data.</p><p>Also, see <a class="xref" href="#precreate.regions" title="1.6.2.&nbsp; Table Creation: Pre-Creating Regions">Section&nbsp;1.6.2, &#8220;
    Table Creation: Pre-Creating Regions
    &#8221;</a>, as well as <a class="xref" href="#perf.configurations" title="1.4.&nbsp;HBase Configurations">Section&nbsp;1.4, &#8220;HBase Configurations&#8221;</a> </p></div></div><div class="section" title="1.7.&nbsp;Reading from HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.reading"></a>1.7.&nbsp;Reading from HBase</h2></div></div></div><div class="section" title="1.7.1.&nbsp;Scan Caching"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.caching"></a>1.7.1.&nbsp;Scan Caching</h3></div></div></div><p>If HBase is used as an input source for a MapReduce job, for
      example, make sure that the input <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a>
      instance to the MapReduce job has <code class="methodname">setCaching</code> set to something greater
      than the default (which is 1). Using the default value means that the
      map-task will make call back to the region-server for every record
      processed. Setting this value to 500, for example, will transfer 500
      rows at a time to the client to be processed. There is a cost/benefit to
      have the cache value be large because it costs more in memory for both
      client and RegionServer, so bigger isn't always better.</p><div class="section" title="1.7.1.1.&nbsp;Scan Caching in MapReduce Jobs"><div class="titlepage"><div><div><h4 class="title"><a name="perf.hbase.client.caching.mr"></a>1.7.1.1.&nbsp;Scan Caching in MapReduce Jobs</h4></div></div></div><p>Scan settings in MapReduce jobs deserve special attention.  Timeouts can result (e.g., UnknownScannerException)
        in Map tasks if it takes longer to process a batch of records before the client goes back to the RegionServer for the
        next set of data.  This problem can occur because there is non-trivial processing occuring per row.  If you process
        rows quickly, set caching higher.  If you process rows more slowly (e.g., lots of transformations per row, writes), 
        then set caching lower.
        </p><p>Timeouts can also happen in a non-MapReduce use case (i.e., single threaded HBase client doing a Scan), but the
        processing that is often performed in MapReduce jobs tends to exacerbate this issue.
        </p></div></div><div class="section" title="1.7.2.&nbsp;Scan Attribute Selection"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.selection"></a>1.7.2.&nbsp;Scan Attribute Selection</h3></div></div></div><p>Whenever a Scan is used to process large numbers of rows (and especially when used
      as a MapReduce source), be aware of which attributes are selected.   If <code class="code">scan.addFamily</code> is called
      then <span class="emphasis"><em>all</em></span> of the attributes in the specified ColumnFamily will be returned to the client.
      If only a small number of the available attributes are to be processed, then only those attributes should be specified
      in the input scan because attribute over-selection is a non-trivial performance penalty over large datasets.
      </p></div><div class="section" title="1.7.3.&nbsp;Close ResultScanners"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.scannerclose"></a>1.7.3.&nbsp;Close ResultScanners</h3></div></div></div><p>This isn't so much about improving performance but rather
      <span class="emphasis"><em>avoiding</em></span> performance problems. If you forget to
      close <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/ResultScanner.html" target="_top">ResultScanners</a>
      you can cause problems on the RegionServers. Always have ResultScanner
      processing enclosed in try/catch blocks... </p><pre class="programlisting">
Scan scan = new Scan();
// set attrs...
ResultScanner rs = htable.getScanner(scan);
try {
  for (Result r = rs.next(); r != null; r = rs.next()) {
  // process result...
} finally {
  rs.close();  // always close the ResultScanner!
}
htable.close();</pre></div><div class="section" title="1.7.4.&nbsp;Block Cache"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.blockcache"></a>1.7.4.&nbsp;Block Cache</h3></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a>
      instances can be set to use the block cache in the RegionServer via the
      <code class="methodname">setCacheBlocks</code> method. For input Scans to MapReduce jobs, this should be
      <code class="varname">false</code>. For frequently accessed rows, it is advisable to use the block
      cache.</p></div><div class="section" title="1.7.5.&nbsp;Optimal Loading of Row Keys"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.rowkeyonly"></a>1.7.5.&nbsp;Optimal Loading of Row Keys</h3></div></div></div><p>When performing a table <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">scan</a>
            where only the row keys are needed (no families, qualifiers, values or timestamps), add a FilterList with a
            <code class="varname">MUST_PASS_ALL</code> operator to the scanner using <code class="methodname">setFilter</code>. The filter list
            should include both a <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.html" target="_top">FirstKeyOnlyFilter</a>
            and a <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/KeyOnlyFilter.html" target="_top">KeyOnlyFilter</a>.
            Using this filter combination will result in a worst case scenario of a RegionServer reading a single value from disk
            and minimal network traffic to the client for a single row.
      </p></div><div class="section" title="1.7.6.&nbsp;Concurrency: Monitor Data Spread"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.read.dist"></a>1.7.6.&nbsp;Concurrency:  Monitor Data Spread</h3></div></div></div><p>When performing a high number of concurrent reads, monitor the data spread of the target tables.  If the target table(s) have 
      too few regions then the reads could likely be served from too few nodes.  </p><p>See <a class="xref" href="#precreate.regions" title="1.6.2.&nbsp; Table Creation: Pre-Creating Regions">Section&nbsp;1.6.2, &#8220;
    Table Creation: Pre-Creating Regions
    &#8221;</a>, as well as <a class="xref" href="#perf.configurations" title="1.4.&nbsp;HBase Configurations">Section&nbsp;1.4, &#8220;HBase Configurations&#8221;</a> </p></div></div><div class="section" title="1.8.&nbsp;Deleting from HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.deleting"></a>1.8.&nbsp;Deleting from HBase</h2></div></div></div><div class="section" title="1.8.1.&nbsp;Using HBase Tables as Queues"><div class="titlepage"><div><div><h3 class="title"><a name="perf.deleting.queue"></a>1.8.1.&nbsp;Using HBase Tables as Queues</h3></div></div></div><p>HBase tables are sometimes used as queues.  In this case, special care must be taken to regularly perform major compactions on tables used in
       this manner.  As is documented in <a class="xref" href="#">???</a>, marking rows as deleted creates additional StoreFiles which then need to be processed
       on reads.  Tombstones only get cleaned up with major compactions.
       </p><p>See also <a class="xref" href="#">???</a> and <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29" target="_top">HBaseAdmin.majorCompact</a>.
       </p></div><div class="section" title="1.8.2.&nbsp;Delete RPC Behavior"><div class="titlepage"><div><div><h3 class="title"><a name="perf.deleting.rpc"></a>1.8.2.&nbsp;Delete RPC Behavior</h3></div></div></div><p>Be aware that <code class="code">htable.delete(Delete)</code> doesn't use the writeBuffer.  It will execute an RegionServer RPC with each invocation.
       For a large number of deletes, consider <code class="code">htable.delete(List)</code>.
       </p><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#delete%28org.apache.hadoop.hbase.client.Delete%29" target="_top">http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#delete%28org.apache.hadoop.hbase.client.Delete%29</a>
       </p></div></div><div class="section" title="1.9.&nbsp;HDFS"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.hdfs"></a>1.9.&nbsp;HDFS</h2></div></div></div><p>Because HBase runs on <a class="xref" href="#">???</a> it is important to understand how it works and how it affects
   HBase.
   </p><div class="section" title="1.9.1.&nbsp;Current Issues With Low-Latency Reads"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hdfs.curr"></a>1.9.1.&nbsp;Current Issues With Low-Latency Reads</h3></div></div></div><p>The original use-case for HDFS was batch processing.  As such, there low-latency reads were historically not a priority.
      With the increased adoption of HBase this is changing, and several improvements are already in development.
      See the 
      <a class="link" href="https://issues.apache.org/jira/browse/HDFS-1599" target="_top">Umbrella Jira Ticket for HDFS Improvements for HBase</a>.
      </p></div><div class="section" title="1.9.2.&nbsp;Performance Comparisons of HBase vs. HDFS"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hdfs.comp"></a>1.9.2.&nbsp;Performance Comparisons of HBase vs. HDFS</h3></div></div></div><p>A fairly common question on the dist-list is why HBase isn't as performant as HDFS files in a batch context (e.g., as 
     a MapReduce source or sink).  The short answer is that HBase is doing a lot more than HDFS (e.g., reading the KeyValues, 
     returning the most current row or specified timestamps, etc.), and as such HBase is 4-5 times slower than HDFS in this 
     processing context.  Not that there isn't room for improvement (and this gap will, over time, be reduced), but HDFS
      will always be faster in this use-case.
     </p></div></div><div class="section" title="1.10.&nbsp;Amazon EC2"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.ec2"></a>1.10.&nbsp;Amazon EC2</h2></div></div></div><p>Performance questions are common on Amazon EC2 environments because it is a shared environment.  You will
   not see the same throughput as a dedicated server.  In terms of running tests on EC2, run them several times for the same
   reason (i.e., it's a shared environment and you don't know what else is happening on the server).
   </p><p>If you are running on EC2 and post performance questions on the dist-list, please state this fact up-front that
    because EC2 issues are practically a separate class of performance issues.
   </p></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d3094e95" href="#d3094e95" class="para">1</a>] </sup>The latest jvms do better
        regards fragmentation so make sure you are running a recent release.
        Read down in the message,
        <a class="link" href="http://osdir.com/ml/hotspot-gc-use/2011-11/msg00002.html" target="_top">Identifying concurrent mode failures caused by fragmentation</a>.</p></div></div></div></body></html>