From b236e0ecea08591d25b15494f79ac1be91193a77 Mon Sep 17 00:00:00 2001
From: Jonathan Hsieh <jmhsieh@apache.org>
Date: Fri, 23 Mar 2012 23:54:31 +0000
Subject: [PATCH 008/154] HBASE-5128 [uber hbck] Online automated repair of table integrity and region consistency problems

git-svn-id: https://svn.apache.org/repos/asf/hbase/branches/0.92@1304667 13f79535-47bb-0310-9956-ffa450edef68

Reason: Supportability
Author: Jonathan Hsieh
Ref: CDH-4040
---
 .../apache/hadoop/hbase/ipc/HMasterInterface.java  |    1 -
 .../hadoop/hbase/master/AssignmentManager.java     |    5 +-
 .../org/apache/hadoop/hbase/master/HMaster.java    |    7 +-
 .../org/apache/hadoop/hbase/util/HBaseFsck.java    | 1838 ++++++++++++++++----
 .../apache/hadoop/hbase/util/HBaseFsckRepair.java  |  159 ++-
 .../hadoop/hbase/util/hbck/OfflineMetaRepair.java  |   13 +-
 .../util/hbck/TableIntegrityErrorHandler.java      |   92 +
 .../util/hbck/TableIntegrityErrorHandlerImpl.java  |   96 +
 .../apache/hadoop/hbase/HBaseTestingUtility.java   |   11 +-
 .../apache/hadoop/hbase/util/TestHBaseFsck.java    |  591 ++++++-
 .../hadoop/hbase/util/TestHBaseFsckComparator.java |    8 -
 .../hadoop/hbase/util/hbck/HbckTestingUtil.java    |   20 +-
 .../util/hbck/TestOfflineMetaRebuildBase.java      |    2 +-
 .../util/hbck/TestOfflineMetaRebuildHole.java      |    3 +-
 .../util/hbck/TestOfflineMetaRebuildOverlap.java   |    2 +-
 15 files changed, 2419 insertions(+), 429 deletions(-)
 create mode 100644 src/main/java/org/apache/hadoop/hbase/util/hbck/TableIntegrityErrorHandler.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/util/hbck/TableIntegrityErrorHandlerImpl.java

diff --git a/src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java b/src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
index 5c30f50..583ad20 100644
--- a/src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
+++ b/src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
@@ -217,7 +217,6 @@ public interface HMasterInterface extends VersionedProtocol {
   public void unassign(final byte [] regionName, final boolean force)
   throws IOException;
 
-  
   /**
    * Offline a region from the assignment manager's in-memory state.  The
    * region should be in a closed state and there will be no attempt to
diff --git a/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java b/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
index 63f538b..2e6a5df 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
@@ -1010,8 +1010,9 @@ public class AssignmentManager extends ZooKeeperListener {
           regionInfo = regionState.getRegion();
         } else {
           try {
-            regionInfo = MetaReader.getRegion(catalogTracker,
-                data.getRegionName()).getFirst();
+            byte[] name = data.getRegionName();
+            Pair<HRegionInfo, ServerName> p = MetaReader.getRegion(catalogTracker, name);
+            regionInfo = p.getFirst();
           } catch (IOException e) {
             LOG.info("Exception reading META doing HBCK repair operation", e);
             return;
diff --git a/src/main/java/org/apache/hadoop/hbase/master/HMaster.java b/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
index 231f05e..9842757 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
@@ -1603,18 +1603,17 @@ Server {
   public double getAverageLoad() {
     return this.assignmentManager.getAverageLoad();
   }
-  
+
   /**
    * Special method, only used by hbck.
    */
   @Override
-  public void offline(final byte[] regionName) 
-  throws IOException {
+  public void offline(final byte[] regionName) throws IOException {
     Pair<HRegionInfo, ServerName> pair =
       MetaReader.getRegion(this.catalogTracker, regionName);
     if (pair == null) throw new UnknownRegionException(Bytes.toStringBinary(regionName));
     HRegionInfo hri = pair.getFirst();
-    this.assignmentManager.regionOffline(hri);    
+    this.assignmentManager.regionOffline(hri);
   }
 
   /**
diff --git a/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java b/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java
index e55b906..c49181c 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java
@@ -1,6 +1,4 @@
 /**
- * Copyright 2010 The Apache Software Foundation
- *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -49,78 +47,163 @@ import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HRegionLocation;
-import org.apache.hadoop.hbase.HServerAddress;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.MasterNotRunningException;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.ZooKeeperConnectionException;
 import org.apache.hadoop.hbase.catalog.MetaReader;
+import org.apache.hadoop.hbase.client.Delete;
+import org.apache.hadoop.hbase.client.Get;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HConnection;
 import org.apache.hadoop.hbase.client.HConnectionManager;
 import org.apache.hadoop.hbase.client.HConnectionManager.HConnectable;
+import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.MetaScanner;
 import org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.io.hfile.CacheConfig;
+import org.apache.hadoop.hbase.io.hfile.HFile;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
 import org.apache.hadoop.hbase.master.MasterFileSystem;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.util.HBaseFsck.ErrorReporter.ERROR_CODE;
+import org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandler;
+import org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl;
 import org.apache.hadoop.hbase.zookeeper.RootRegionTracker;
 import org.apache.hadoop.hbase.zookeeper.ZKTable;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
-import org.apache.hadoop.io.MultipleIOException;
 import org.apache.zookeeper.KeeperException;
 
 import com.google.common.base.Joiner;
+import com.google.common.base.Preconditions;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Multimap;
 import com.google.common.collect.TreeMultimap;
 
 /**
- * Check consistency among the in-memory states of the master and the
- * region server(s) and the state of data in HDFS.
+ * HBaseFsck (hbck) is a tool for checking and repairing region consistency and
+ * table integrity problems in a corrupted HBase.
+ * <p>
+ * Region consistency checks verify that .META., region deployment on region
+ * servers and the state of data in HDFS (.regioninfo files) all are in
+ * accordance.
+ * <p>
+ * Table integrity checks verify that all possible row keys resolve to exactly
+ * one region of a table.  This means there are no individual degenerate
+ * or backwards regions; no holes between regions; and that there are no
+ * overlapping regions.
+ * <p>
+ * The general repair strategy works in two phases:
+ * <ol>
+ * <li> Repair Table Integrity on HDFS. (merge or fabricate regions)
+ * <li> Repair Region Consistency with .META. and assignments
+ * </ol>
+ * <p>
+ * For table integrity repairs, the tables' region directories are scanned
+ * for .regioninfo files.  Each table's integrity is then verified.  If there
+ * are any orphan regions (regions with no .regioninfo files) or holes, new
+ * regions are fabricated.  Backwards regions are sidelined as well as empty
+ * degenerate (endkey==startkey) regions.  If there are any overlapping regions,
+ * a new region is created and all data is merged into the new region.
+ * <p>
+ * Table integrity repairs deal solely with HDFS and could potentially be done
+ * offline -- the hbase region servers or master do not need to be running.
+ * This phase can eventually be used to completely reconstruct the META table in
+ * an offline fashion.
+ * <p>
+ * Region consistency requires three conditions -- 1) valid .regioninfo file
+ * present in an HDFS region dir,  2) valid row with .regioninfo data in META,
+ * and 3) a region is deployed only at the regionserver that was assigned to
+ * with proper state in the master.
+ * <p>
+ * Region consistency repairs require hbase to be online so that hbck can
+ * contact the HBase master and region servers.  The hbck#connect() method must
+ * first be called successfully.  Much of the region consistency information
+ * is transient and less risky to repair.
+ * <p>
+ * If hbck is run from the command line, there are a handful of arguments that
+ * can be used to limit the kinds of repairs hbck will do.  See the code in
+ * {@link #printUsageAndExit()} for more details.
  */
 public class HBaseFsck {
   public static final long DEFAULT_TIME_LAG = 60000; // default value of 1 minute
   public static final long DEFAULT_SLEEP_BEFORE_RERUN = 10000;
-
+  private static final int MAX_NUM_THREADS = 50; // #threads to contact regions
   private static final long THREADS_KEEP_ALIVE_SECONDS = 60;
+  private static boolean rsSupportsOffline = true;
+  private static final int DEFAULT_MAX_MERGE = 5;
 
+  /**********************
+   * Internal resources
+   **********************/
   private static final Log LOG = LogFactory.getLog(HBaseFsck.class.getName());
   private Configuration conf;
-
   private ClusterStatus status;
   private HConnection connection;
-
-  private TreeMap<String, HbckInfo> regionInfo = new TreeMap<String, HbckInfo>();
-  private TreeMap<String, TInfo> tablesInfo = new TreeMap<String, TInfo>();
-  private TreeSet<byte[]> disabledTables =
-    new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
-  ErrorReporter errors = new PrintingErrorReporter();
-
+  private HBaseAdmin admin;
+  private HTable meta;
+  private ThreadPoolExecutor executor; // threads to retrieve data from regionservers
+  private int numThreads = MAX_NUM_THREADS;
+  private long startMillis = System.currentTimeMillis();
+
+  /***********
+   * Options
+   ***********/
   private static boolean details = false; // do we display the full report
   private long timelag = DEFAULT_TIME_LAG; // tables whose modtime is older
-  private boolean fix = false; // do we want to try fixing the errors?
-  private boolean rerun = false; // if we tried to fix something rerun hbck
+  private boolean fixAssignments = false; // fix assignment errors?
+  private boolean fixMeta = false; // fix meta errors?
+  private boolean fixHdfsHoles = false; // fix fs holes?
+  private boolean fixHdfsOverlaps = false; // fix fs overlaps (risky)
+  private boolean fixHdfsOrphans = false; // fix fs holes (missing .regioninfo)
+
+  // limit fixes to listed tables, if empty atttempt to fix all
+  private List<byte[]> tablesToFix = new ArrayList<byte[]>();
+  private int maxMerge = DEFAULT_MAX_MERGE; // maximum number of overlapping regions to merge
+
+  private boolean rerun = false; // if we tried to fix something, rerun hbck
   private static boolean summary = false; // if we want to print less output
   private boolean checkMetaOnly = false;
-  
+
+  /*********
+   * State
+   *********/
+  private ErrorReporter errors = new PrintingErrorReporter();
+  int fixes = 0;
+
+  /**
+   * This map contains the state of all hbck items.  It maps from encoded region
+   * name to HbckInfo structure.  The information contained in HbckInfo is used
+   * to detect and correct consistency (hdfs/meta/deployment) problems.
+   */
+  private TreeMap<String, HbckInfo> regionInfoMap = new TreeMap<String, HbckInfo>();
+  private TreeSet<byte[]> disabledTables =
+    new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
   // Empty regioninfo qualifiers in .META.
   private Set<Result> emptyRegionInfoQualifiers = new HashSet<Result>();
-  private HBaseAdmin admin;
 
-  ThreadPoolExecutor executor; // threads to retrieve data from regionservers
+  /**
+   * This map from Tablename -> TableInfo contains the structures necessary to
+   * detect table consistency problems (holes, dupes, overlaps).  It is sorted
+   * to prevent dupes.
+   */
+  private TreeMap<String, TableInfo> tablesInfo = new TreeMap<String, TableInfo>();
+
+  /**
+   * When initially looking at HDFS, we attempt to find any orphaned data.
+   */
+  private List<HbckInfo> orphanHdfsDirs = new ArrayList<HbckInfo>();
 
   /**
    * Constructor
    *
    * @param conf Configuration object
    * @throws MasterNotRunningException if the master is not running
-   * @throws ZooKeeperConnectionException if unable to connect to zookeeper
+   * @throws ZooKeeperConnectionException if unable to connect to ZooKeeper
    */
   public HBaseFsck(Configuration conf) throws MasterNotRunningException,
       ZooKeeperConnectionException, IOException {
@@ -133,85 +216,33 @@ public class HBaseFsck {
     executor.allowCoreThreadTimeOut(true);
   }
 
-  public void connect() throws MasterNotRunningException,
-      ZooKeeperConnectionException {
+  /**
+   * To repair region consistency, one must call connect() in order to repair
+   * online state.
+   */
+  public void connect() throws IOException {
     admin = new HBaseAdmin(conf);
+    meta = new HTable(conf, HConstants.META_TABLE_NAME);
     status = admin.getMaster().getClusterStatus();
     connection = admin.getConnection();
   }
 
   /**
-   * Contacts the master and prints out cluster-wide information
-   * @throws IOException if a remote or network exception occurs
-   * @return 0 on success, non-zero on failure
-   * @throws KeeperException
-   * @throws InterruptedException
+   * Get deployed regions according to the region servers.
    */
-  public int doWork() throws IOException, KeeperException, InterruptedException {
-    // print hbase server version
-    errors.print("Version: " + status.getHBaseVersion());
-
-    // Make sure regionInfo is empty before starting
-    regionInfo.clear();
-    tablesInfo.clear();
-    emptyRegionInfoQualifiers.clear();
-    disabledTables.clear();
-    errors.clear();
-
-    // get a list of all regions from the master. This involves
-    // scanning the META table
-    if (!recordRootRegion()) {
-      // Will remove later if we can fix it
-      errors.reportError("Encountered fatal error. Exiting...");
-      return -1;
-    }
-    
-    getMetaEntries();
-
-    // Check if .META. is found only once and in the right place
-    if (!checkMetaEntries()) {
-      // Will remove later if we can fix it
-      errors.reportError("Encountered fatal error. Exiting...");
-      return -1;
-    }
-
-    // get a list of all tables that have not changed recently.
-    if (!checkMetaOnly) {
-      AtomicInteger numSkipped = new AtomicInteger(0);
-      HTableDescriptor[] allTables = getTables(numSkipped);
-      errors.print("Number of Tables: " +
-          (allTables == null ? 0 : allTables.length));
-      if (details) {
-        if (numSkipped.get() > 0) {
-          errors.detail("Number of Tables in flux: " + numSkipped.get());
-        }
-        if (allTables != null && allTables.length > 0) {
-          for (HTableDescriptor td : allTables) {
-          String tableName = td.getNameAsString();
-          errors.detail("  Table: " + tableName + "\t" +
-                             (td.isReadOnly() ? "ro" : "rw") + "\t" +
-                             (td.isRootRegion() ? "ROOT" :
-                              (td.isMetaRegion() ? "META" : "    ")) + "\t" +
-                             " families: " + td.getFamilies().size());
-          }
-        }
-      }
-    }
-    
+  private void loadDeployedRegions() throws IOException, InterruptedException {
     // From the master, get a list of all known live region servers
     Collection<ServerName> regionServers = status.getServers();
-    errors.print("Number of live region servers: " +
-                       regionServers.size());
+    errors.print("Number of live region servers: " + regionServers.size());
     if (details) {
       for (ServerName rsinfo: regionServers) {
-        errors.print("  " + rsinfo);
+        errors.print("  " + rsinfo.getServerName());
       }
     }
 
     // From the master, get a list of all dead region servers
     Collection<ServerName> deadRegionServers = status.getDeadServerNames();
-    errors.print("Number of dead region servers: " +
-                       deadRegionServers.size());
+    errors.print("Number of dead region servers: " + deadRegionServers.size());
     if (details) {
       for (ServerName name: deadRegionServers) {
         errors.print("  " + name);
@@ -232,100 +263,390 @@ public class HBaseFsck {
 
     // Determine what's deployed
     processRegionServers(regionServers);
+  }
 
-    // Determine what's on HDFS
-    checkHdfs();
+  /**
+   * Clear the current state of hbck.
+   */
+  private void clearState() {
+    // Make sure regionInfo is empty before starting
+    fixes = 0;
+    regionInfoMap.clear();
+    emptyRegionInfoQualifiers.clear();
+    disabledTables.clear();
+    errors.clear();
+    tablesInfo.clear();
+    orphanHdfsDirs.clear();
+  }
 
-    // Empty cells in .META.?
-    errors.print("Number of empty REGIONINFO_QUALIFIER rows in .META.: " +
-      emptyRegionInfoQualifiers.size());
-    if (details) {
-      for (Result r: emptyRegionInfoQualifiers) {
-        errors.print("  " + r);
+  /**
+   * This repair method analyzes hbase data in hdfs and repairs it to satisfy
+   * the table integrity rules.  HBase doesn't need to be online for this
+   * operation to work.
+   */
+  public void offlineHdfsIntegrityRepair() throws IOException, InterruptedException {
+    // Initial pass to fix orphans.
+    if (shouldFixHdfsOrphans() || shouldFixHdfsHoles() || shouldFixHdfsOverlaps()) {
+      LOG.info("Loading regioninfos HDFS");
+      // if nothing is happening this should always complete in two iterations.
+      int maxIterations = conf.getInt("hbase.hbck.integrityrepair.iterations.max", 3);
+      int curIter = 0;
+      do {
+        clearState(); // clears hbck state and reset fixes to 0 and.
+        // repair what's on HDFS
+        restoreHdfsIntegrity();
+        curIter++;// limit the number of iterations.
+      } while (fixes > 0 && curIter <= maxIterations);
+
+      // Repairs should be done in the first iteration and verification in the second.
+      // If there are more than 2 passes, something funny has happened.
+      if (curIter > 2) {
+        if (curIter == maxIterations) {
+          LOG.warn("Exiting integrity repairs after max " + curIter + " iterations. "
+              + "Tables integrity may not be fully repaired!");
+        } else {
+          LOG.info("Successfully exiting integrity repairs after " + curIter + " iterations");
+        }
       }
     }
+  }
+
+  /**
+   * This repair method requires the cluster to be online since it contacts
+   * region servers and the masters.  It makes each region's state in HDFS, in
+   * .META., and deployments consistent.
+   *
+   * @return If > 0 , number of errors detected, if < 0 there was an unrecoverable
+   * error.  If 0, we have a clean hbase.
+   */
+  public int onlineConsistencyRepair() throws IOException, KeeperException,
+    InterruptedException {
+    clearState();
+
+    LOG.info("Loading regionsinfo from the .META. table");
+    boolean success = loadMetaEntries();
+    if (!success) return -1;
+
+    // Check if .META. is found only once and in the right place
+    if (!checkMetaRegion()) {
+      // Will remove later if we can fix it
+      errors.reportError("Encountered fatal error. Exiting...");
+      return -2;
+    }
+
+    // get a list of all tables that have not changed recently.
+    if (!checkMetaOnly) {
+      reportTablesInFlux();
+    }
+
+    // get regions according to what is online on each RegionServer
+    loadDeployedRegions();
+
+    // load regiondirs and regioninfos from HDFS
+    loadHdfsRegionDirs();
+    loadHdfsRegionInfos();
+
+    // Empty cells in .META.?
+    reportEmptyMetaCells();
 
     // Get disabled tables from ZooKeeper
     loadDisabledTables();
 
-    // Check consistency
-    checkConsistency();
+    // Check and fix consistency
+    checkAndFixConsistency();
 
-    // Check integrity
+    // Check integrity (does not fix)
     checkIntegrity();
+    return errors.getErrorList().size();
+  }
 
-    // Print table summary
-    printTableSummary();
+  /**
+   * Contacts the master and prints out cluster-wide information
+   * @return 0 on success, non-zero on failure
+   */
+  public int onlineHbck() throws IOException, KeeperException, InterruptedException {
+    // print hbase server version
+    errors.print("Version: " + status.getHBaseVersion());
+    offlineHdfsIntegrityRepair();
+
+    // turn the balancer off
+    boolean oldBalancer = admin.balanceSwitch(false);
+
+    onlineConsistencyRepair();
 
+    admin.balanceSwitch(oldBalancer);
+
+    // Print table summary
+    printTableSummary(tablesInfo);
     return errors.summarize();
   }
 
+  /**
+   * Iterates through the list of all orphan/invalid regiondirs.
+   */
+  private void adoptHdfsOrphans(Collection<HbckInfo> orphanHdfsDirs) throws IOException {
+    for (HbckInfo hi : orphanHdfsDirs) {
+      LOG.info("Attempting to handle orphan hdfs dir: " + hi.getHdfsRegionDir());
+      adoptHdfsOrphan(hi);
+    }
+  }
+
+  /**
+   * Orphaned regions are regions without a .regioninfo file in them.  We "adopt"
+   * these orphans by creating a new region, and moving the column families,
+   * recovered edits, HLogs, into the new region dir.  We determine the region
+   * startkey and endkeys by looking at all of the hfiles inside the column
+   * families to identify the min and max keys. The resulting region will
+   * likely violate table integrity but will be dealt with by merging
+   * overlapping regions.
+   */
+  private void adoptHdfsOrphan(HbckInfo hi) throws IOException {
+    Path p = hi.getHdfsRegionDir();
+    FileSystem fs = p.getFileSystem(conf);
+    FileStatus[] dirs = fs.listStatus(p);
+
+    String tableName = Bytes.toString(hi.getTableName());
+    TableInfo tableInfo = tablesInfo.get(tableName);
+    Preconditions.checkNotNull("Table " + tableName + "' not present!", tableInfo);
+    HTableDescriptor template = tableInfo.getHTD();
+
+    // find min and max key values
+    Pair<byte[],byte[]> orphanRegionRange = null;
+    for (FileStatus cf : dirs) {
+      String cfName= cf.getPath().getName();
+      // TODO Figure out what the special dirs are
+      if (cfName.startsWith(".") || cfName.equals("splitlog")) continue;
+
+      FileStatus[] hfiles = fs.listStatus(cf.getPath());
+      for (FileStatus hfile : hfiles) {
+        byte[] start, end;
+        HFile.Reader hf = null;
+        try {
+          CacheConfig cacheConf = new CacheConfig(conf);
+          hf = HFile.createReader(fs, hfile.getPath(), cacheConf);
+          hf.loadFileInfo();
+          KeyValue startKv = KeyValue.createKeyValueFromKey(hf.getFirstKey());
+          start = startKv.getRow();
+          KeyValue endKv = KeyValue.createKeyValueFromKey(hf.getLastKey());
+          end = endKv.getRow();
+        } catch (IOException ioe) {
+          LOG.warn("Problem reading orphan file " + hfile + ", skipping");
+          continue;
+        } catch (NullPointerException ioe) {
+          LOG.warn("Orphan file " + hfile + " is possibly corrupted HFile, skipping");
+          continue;
+        } finally {
+          if (hf != null) {
+            hf.close();
+          }
+        }
+
+        // expand the range to include the range of all hfiles
+        if (orphanRegionRange == null) {
+          // first range
+          orphanRegionRange = new Pair<byte[], byte[]>(start, end);
+        } else {
+          // TODO add test
+
+          // expand range only if the hfile is wider.
+          if (Bytes.compareTo(orphanRegionRange.getFirst(), start) > 0) {
+            orphanRegionRange.setFirst(start);
+          }
+          if (Bytes.compareTo(orphanRegionRange.getSecond(), end) < 0 ) {
+            orphanRegionRange.setSecond(end);
+          }
+        }
+      }
+    }
+    if (orphanRegionRange == null) {
+      LOG.warn("No data in dir " + p + ", sidelining data");
+      fixes++;
+      sidelineRegionDir(fs, hi);
+      return;
+    }
+    LOG.info("Min max keys are : [" + Bytes.toString(orphanRegionRange.getFirst()) + ", " +
+        Bytes.toString(orphanRegionRange.getSecond()) + ")");
+
+    // create new region on hdfs.  move data into place.
+    HRegionInfo hri = new HRegionInfo(template.getName(), orphanRegionRange.getFirst(), orphanRegionRange.getSecond());
+    LOG.info("Creating new region : " + hri);
+    HRegion region = HBaseFsckRepair.createHDFSRegionDir(conf, hri, template);
+    Path target = region.getRegionDir();
+
+    // rename all the data to new region
+    mergeRegionDirs(target, hi);
+    fixes++;
+  }
+
+  /**
+   * This method determines if there are table integrity errors in HDFS.  If
+   * there are errors and the appropriate "fix" options are enabled, the method
+   * will first correct orphan regions making them into legit regiondirs, and
+   * then reload to merge potentially overlapping regions.
+   *
+   * @return number of table integrity errors found
+   */
+  private int restoreHdfsIntegrity() throws IOException, InterruptedException {
+    // Determine what's on HDFS
+    LOG.info("Loading HBase regioninfo from HDFS...");
+    loadHdfsRegionDirs(); // populating regioninfo table.
+
+    int errs = errors.getErrorList().size();
+    // First time just get suggestions.
+    tablesInfo = loadHdfsRegionInfos(); // update tableInfos based on region info in fs.
+    checkHdfsIntegrity(false, false);
+
+    if (errors.getErrorList().size() == errs) {
+      LOG.info("No integrity errors.  We are done with this phase. Glorious.");
+      return 0;
+    }
+
+    if (shouldFixHdfsOrphans() && orphanHdfsDirs.size() > 0) {
+      adoptHdfsOrphans(orphanHdfsDirs);
+      // TODO optimize by incrementally adding instead of reloading.
+    }
+
+    // Make sure there are no holes now.
+    if (shouldFixHdfsHoles()) {
+      clearState(); // this also resets # fixes.
+      loadHdfsRegionDirs();
+      tablesInfo = loadHdfsRegionInfos(); // update tableInfos based on region info in fs.
+      tablesInfo = checkHdfsIntegrity(shouldFixHdfsHoles(), false);
+    }
+
+    // Now we fix overlaps
+    if (shouldFixHdfsOverlaps()) {
+      // second pass we fix overlaps.
+      clearState(); // this also resets # fixes.
+      loadHdfsRegionDirs();
+      tablesInfo = loadHdfsRegionInfos(); // update tableInfos based on region info in fs.
+      tablesInfo = checkHdfsIntegrity(false, shouldFixHdfsOverlaps());
+    }
+
+    return errors.getErrorList().size();
+  }
+
+  /**
+   * TODO -- need to add tests for this.
+   */
+  private void reportEmptyMetaCells() {
+    errors.print("Number of empty REGIONINFO_QUALIFIER rows in .META.: " +
+      emptyRegionInfoQualifiers.size());
+    if (details) {
+      for (Result r: emptyRegionInfoQualifiers) {
+        errors.print("  " + r);
+      }
+    }
+  }
+
+  /**
+   * TODO -- need to add tests for this.
+   */
+  private void reportTablesInFlux() {
+    AtomicInteger numSkipped = new AtomicInteger(0);
+    HTableDescriptor[] allTables = getTables(numSkipped);
+    errors.print("Number of Tables: " + allTables.length);
+    if (details) {
+      if (numSkipped.get() > 0) {
+        errors.detail("Number of Tables in flux: " + numSkipped.get());
+      }
+      for (HTableDescriptor td : allTables) {
+        String tableName = td.getNameAsString();
+        errors.detail("  Table: " + tableName + "\t" +
+                           (td.isReadOnly() ? "ro" : "rw") + "\t" +
+                           (td.isRootRegion() ? "ROOT" :
+                            (td.isMetaRegion() ? "META" : "    ")) + "\t" +
+                           " families: " + td.getFamilies().size());
+      }
+    }
+  }
+
   public ErrorReporter getErrors() {
     return errors;
   }
 
   /**
-   * Populate a specific hbi from regioninfo on file system.
+   * Read the .regioninfo file from the file system.  If there is no
+   * .regioninfo, add it to the orphan hdfs region list.
    */
-  private void loadMetaEntry(HbckInfo hbi) throws IOException {
-    Path regionDir = hbi.foundRegionDir.getPath();
+  private void loadHdfsRegioninfo(HbckInfo hbi) throws IOException {
+    Path regionDir = hbi.getHdfsRegionDir();
+    if (regionDir == null) {
+      LOG.warn("No HDFS region dir found: " + hbi + " meta=" + hbi.metaEntry);
+      return;
+    }
     Path regioninfo = new Path(regionDir, HRegion.REGIONINFO_FILE);
-    FileSystem fs = FileSystem.get(conf);
+    FileSystem fs = regioninfo.getFileSystem(conf);
+
     FSDataInputStream in = fs.open(regioninfo);
-    byte[] tableName = Bytes.toBytes(hbi.hdfsTableName);
-    HRegionInfo hri = new HRegionInfo(tableName);
+    HRegionInfo hri = new HRegionInfo();
     hri.readFields(in);
     in.close();
     LOG.debug("HRegionInfo read: " + hri.toString());
-    hbi.metaEntry = new MetaEntry(hri, null,
-        hbi.foundRegionDir.getModificationTime());
+    hbi.hdfsEntry.hri = hri;
   }
 
-  public static class RegionInfoLoadException extends IOException {
+  /**
+   * Exception thrown when a integrity repair operation fails in an
+   * unresolvable way.
+   */
+  public static class RegionRepairException extends IOException {
     private static final long serialVersionUID = 1L;
     final IOException ioe;
-    public RegionInfoLoadException(String s, IOException ioe) {
+    public RegionRepairException(String s, IOException ioe) {
       super(s);
       this.ioe = ioe;
     }
   }
 
   /**
-   * Populate hbi's from regionInfos loaded from file system. 
+   * Populate hbi's from regionInfos loaded from file system.
    */
-  private void loadTableInfo() throws IOException {
-    List<IOException> ioes = new ArrayList<IOException>();
+  private TreeMap<String, TableInfo> loadHdfsRegionInfos() throws IOException {
+    tablesInfo.clear(); // regenerating the data
     // generate region split structure
-    for (HbckInfo hbi : regionInfo.values()) {
+    for (HbckInfo hbi : regionInfoMap.values()) {
+
       // only load entries that haven't been loaded yet.
-      if (hbi.metaEntry == null) {
+      if (hbi.getHdfsHRI() == null) {
         try {
-          loadMetaEntry(hbi);
+          loadHdfsRegioninfo(hbi);
         } catch (IOException ioe) {
-          String msg = "Unable to load region info for table " + hbi.hdfsTableName
-            + "!  It may be an invalid format or version file.  You may want to "
-            + "remove " + hbi.foundRegionDir.getPath()
-            + " region from hdfs and retry.";
-          errors.report(msg);
-          LOG.error(msg, ioe);
-          ioes.add(new RegionInfoLoadException(msg, ioe));
+          String msg = "Orphan region in HDFS: Unable to load .regioninfo from table "
+            + Bytes.toString(hbi.getTableName()) + " in hdfs dir "
+            + hbi.getHdfsRegionDir()
+            + "!  It may be an invalid format or version file.  Treating as "
+            + "an orphaned regiondir.";
+          errors.reportError(ERROR_CODE.ORPHAN_HDFS_REGION, msg);
+          debugLsr(hbi.getHdfsRegionDir());
+          orphanHdfsDirs.add(hbi);
           continue;
         }
       }
 
       // get table name from hdfs, populate various HBaseFsck tables.
-      String tableName = hbi.hdfsTableName;
-      TInfo modTInfo = tablesInfo.get(tableName);
+      String tableName = Bytes.toString(hbi.getTableName());
+      if (tableName == null) {
+        // There was an entry in META not in the HDFS?
+        LOG.warn("tableName was null for: " + hbi);
+        continue;
+      }
+
+      TableInfo modTInfo = tablesInfo.get(tableName);
       if (modTInfo == null) {
-        modTInfo = new TInfo(tableName);
+        // only executed once per table.
+        modTInfo = new TableInfo(tableName);
+        Path hbaseRoot = new Path(conf.get(HConstants.HBASE_DIR));
+        HTableDescriptor htd =
+          FSTableDescriptors.getTableDescriptor(hbaseRoot.getFileSystem(conf),
+              hbaseRoot, tableName);
+        modTInfo.htds.add(htd);
       }
       modTInfo.addRegionInfo(hbi);
       tablesInfo.put(tableName, modTInfo);
     }
 
-    if (ioes.size() != 0) {
-      throw MultipleIOException.createIOException(ioes);
-    }
+    return tablesInfo;
   }
 
   /**
@@ -360,10 +681,10 @@ public class HBaseFsck {
    * 
    * @return An array list of puts to do in bulk, null if tables have problems
    */
-  private ArrayList<Put> generatePuts() throws IOException {
+  private ArrayList<Put> generatePuts(TreeMap<String, TableInfo> tablesInfo) throws IOException {
     ArrayList<Put> puts = new ArrayList<Put>();
     boolean hasProblems = false;
-    for (Entry<String, TInfo> e : tablesInfo.entrySet()) {
+    for (Entry<String, TableInfo> e : tablesInfo.entrySet()) {
       String name = e.getKey();
 
       // skip "-ROOT-" and ".META."
@@ -372,7 +693,7 @@ public class HBaseFsck {
         continue;
       }
 
-      TInfo ti = e.getValue();
+      TableInfo ti = e.getValue();
       for (Entry<byte[], Collection<HbckInfo>> spl : ti.sc.getStarts().asMap()
           .entrySet()) {
         Collection<HbckInfo> his = spl.getValue();
@@ -387,7 +708,7 @@ public class HBaseFsck {
 
         // add the row directly to meta.
         HbckInfo hi = his.iterator().next();
-        HRegionInfo hri = hi.metaEntry;
+        HRegionInfo hri = hi.getHdfsHRI(); // hi.metaEntry;
         Put p = new Put(hri.getRegionName());
         p.add(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER,
             Writables.getBytes(hri));
@@ -400,57 +721,68 @@ public class HBaseFsck {
   /**
    * Suggest fixes for each table
    */
-  private void suggestFixes(TreeMap<String, TInfo> tablesInfo) {
-    for (TInfo tInfo : tablesInfo.values()) {
-      tInfo.checkRegionChain();
+  private void suggestFixes(TreeMap<String, TableInfo> tablesInfo) throws IOException {
+    for (TableInfo tInfo : tablesInfo.values()) {
+      TableIntegrityErrorHandler handler = tInfo.new IntegrityFixSuggester(tInfo, errors);
+      tInfo.checkRegionChain(handler);
     }
   }
 
-
   /**
    * Rebuilds meta from information in hdfs/fs.  Depends on configuration
    * settings passed into hbck constructor to point to a particular fs/dir.
    * 
+   * @param fix flag that determines if method should attempt to fix holes
    * @return true if successful, false if attempt failed.
    */
-  public boolean rebuildMeta() throws IOException, InterruptedException {
+  public boolean rebuildMeta(boolean fix) throws IOException,
+      InterruptedException {
+
     // TODO check to make sure hbase is offline. (or at least the table
     // currently being worked on is off line)
 
     // Determine what's on HDFS
     LOG.info("Loading HBase regioninfo from HDFS...");
-    checkHdfs(); // populating regioninfo table.
-    loadTableInfo(); // update tableInfos based on region info in fs.
+    loadHdfsRegionDirs(); // populating regioninfo table.
 
-    LOG.info("Checking HBase region split map from HDFS data...");
     int errs = errors.getErrorList().size();
-    for (TInfo tInfo : tablesInfo.values()) {
-      if (!tInfo.checkRegionChain()) {
-        // should dump info as well.
-        errors.report("Found inconsistency in table " + tInfo.getName());
-      }
-    }
+    tablesInfo = loadHdfsRegionInfos(); // update tableInfos based on region info in fs.
+    checkHdfsIntegrity(false, false);
 
     // make sure ok.
     if (errors.getErrorList().size() != errs) {
-      suggestFixes(tablesInfo);
-
-      // Not ok, bail out.
-      return false;
+      // While in error state, iterate until no more fixes possible
+      while(true) {
+        fixes = 0;
+        suggestFixes(tablesInfo);
+        errors.clear();
+        loadHdfsRegionInfos(); // update tableInfos based on region info in fs.
+        checkHdfsIntegrity(shouldFixHdfsHoles(), shouldFixHdfsOverlaps());
+
+        int errCount = errors.getErrorList().size();
+
+        if (fixes == 0) {
+          if (errCount > 0) {
+            return false; // failed to fix problems.
+          } else {
+            break; // no fixes and no problems? drop out and fix stuff!
+          }
+        }
+      }
     }
 
     // we can rebuild, move old root and meta out of the way and start
     LOG.info("HDFS regioninfo's seems good.  Sidelining old .META.");
     sidelineOldRootAndMeta();
-    
+
     LOG.info("Creating new .META.");
     HRegion meta = createNewRootAndMeta();
 
     // populate meta
-    List<Put> puts = generatePuts();
+    List<Put> puts = generatePuts(tablesInfo);
     if (puts == null) {
       LOG.fatal("Problem encountered when creating new .META. entries.  " +
-        "You may need to restore the previously sidlined -ROOT- and .META.");
+        "You may need to restore the previously sidelined -ROOT- and .META.");
       return false;
     }
     meta.put(puts.toArray(new Put[0]));
@@ -460,13 +792,113 @@ public class HBaseFsck {
     return true;
   }
 
-  void sidelineTable(FileSystem fs, byte[] table, Path hbaseDir, 
+  private TreeMap<String, TableInfo> checkHdfsIntegrity(boolean fixHoles,
+      boolean fixOverlaps) throws IOException {
+    LOG.info("Checking HBase region split map from HDFS data...");
+    for (TableInfo tInfo : tablesInfo.values()) {
+      TableIntegrityErrorHandler handler;
+      if (fixHoles || fixOverlaps) {
+        if (shouldFixTable(Bytes.toBytes(tInfo.getName()))) {
+          handler = tInfo.new HDFSIntegrityFixer(tInfo, errors, conf,
+              fixHoles, fixOverlaps);
+        } else {
+          LOG.info("Table " + tInfo.getName() + " is not in the include table " +
+            "list.  Just suggesting fixes.");
+          handler = tInfo.new IntegrityFixSuggester(tInfo, errors);
+        }
+      } else {
+        handler = tInfo.new IntegrityFixSuggester(tInfo, errors);
+      }
+      if (!tInfo.checkRegionChain(handler)) {
+        // should dump info as well.
+        errors.report("Found inconsistency in table " + tInfo.getName());
+      }
+    }
+    return tablesInfo;
+  }
+
+  private Path getSidelineDir() throws IOException {
+    Path hbaseDir = FSUtils.getRootDir(conf);
+    Path backupDir = new Path(hbaseDir.getParent(), hbaseDir.getName() + "-"
+        + startMillis);
+    return backupDir;
+  }
+
+  /**
+   * Sideline a region dir (instead of deleting it)
+   */
+  void sidelineRegionDir(FileSystem fs, HbckInfo hi)
+    throws IOException {
+    String tableName = Bytes.toString(hi.getTableName());
+    Path regionDir = hi.getHdfsRegionDir();
+
+    if (!fs.exists(regionDir)) {
+      LOG.warn("No previous " + regionDir + " exists.  Continuing.");
+      return;
+    }
+
+    Path sidelineTableDir= new Path(getSidelineDir(), tableName);
+    Path sidelineRegionDir = new Path(sidelineTableDir, regionDir.getName());
+    fs.mkdirs(sidelineRegionDir);
+    boolean success = false;
+    FileStatus[] cfs =  fs.listStatus(regionDir);
+    if (cfs == null) {
+      LOG.info("Region dir is empty: " + regionDir);
+    } else {
+      for (FileStatus cf : cfs) {
+        Path src = cf.getPath();
+        Path dst =  new Path(sidelineRegionDir, src.getName());
+        if (fs.isFile(src)) {
+          // simple file
+          success = fs.rename(src, dst);
+          if (!success) {
+            String msg = "Unable to rename file " + src +  " to " + dst;
+            LOG.error(msg);
+            throw new IOException(msg);
+          }
+          continue;
+        }
+
+        // is a directory.
+        fs.mkdirs(dst);
+
+        LOG.info("Sidelining files from " + src + " into containing region " + dst);
+        // FileSystem.rename is inconsistent with directories -- if the
+        // dst (foo/a) exists and is a dir, and the src (foo/b) is a dir,
+        // it moves the src into the dst dir resulting in (foo/a/b).  If
+        // the dst does not exist, and the src a dir, src becomes dst. (foo/b)
+        for (FileStatus hfile : fs.listStatus(src)) {
+          success = fs.rename(hfile.getPath(), dst);
+          if (!success) {
+            String msg = "Unable to rename file " + src +  " to " + dst;
+            LOG.error(msg);
+            throw new IOException(msg);
+          }
+        }
+        LOG.debug("Sideline directory contents:");
+        debugLsr(sidelineRegionDir);
+      }
+    }
+
+    LOG.info("Removing old region dir: " + regionDir);
+    success = fs.delete(regionDir, true);
+    if (!success) {
+      String msg = "Unable to delete dir " + regionDir;
+      LOG.error(msg);
+      throw new IOException(msg);
+    }
+  }
+
+  /**
+   * Side line an entire table.
+   */
+  void sidelineTable(FileSystem fs, byte[] table, Path hbaseDir,
       Path backupHbaseDir) throws IOException {
     String tableName = Bytes.toString(table);
     Path tableDir = new Path(hbaseDir, tableName);
     if (fs.exists(tableDir)) {
       Path backupTableDir= new Path(backupHbaseDir, tableName);
-      boolean success = fs.rename(tableDir, backupTableDir); 
+      boolean success = fs.rename(tableDir, backupTableDir);
       if (!success) {
         throw new IOException("Failed to move  " + tableName + " from " 
             +  tableDir.getName() + " to " + backupTableDir.getName());
@@ -475,18 +907,16 @@ public class HBaseFsck {
       LOG.info("No previous " + tableName +  " exists.  Continuing.");
     }
   }
-  
+
   /**
    * @return Path to backup of original directory
-   * @throws IOException
    */
   Path sidelineOldRootAndMeta() throws IOException {
     // put current -ROOT- and .META. aside.
     Path hbaseDir = new Path(conf.get(HConstants.HBASE_DIR));
     FileSystem fs = hbaseDir.getFileSystem(conf);
-    long now = System.currentTimeMillis();
     Path backupDir = new Path(hbaseDir.getParent(), hbaseDir.getName() + "-"
-        + now);
+        + startMillis);
     fs.mkdirs(backupDir);
 
     sidelineTable(fs, HConstants.ROOT_TABLE_NAME, hbaseDir, backupDir);
@@ -535,9 +965,6 @@ public class HBaseFsck {
 
   /**
    * Check if the specified region's table is disabled.
-   * @throws ZooKeeperConnectionException
-   * @throws IOException
-   * @throws KeeperException
    */
   private boolean isTableDisabled(HRegionInfo regionInfo) {
     return disabledTables.contains(regionInfo.getTableName());
@@ -545,9 +972,9 @@ public class HBaseFsck {
 
   /**
    * Scan HDFS for all regions, recording their information into
-   * regionInfo
+   * regionInfoMap
    */
-  public void checkHdfs() throws IOException, InterruptedException {
+  public void loadHdfsRegionDirs() throws IOException, InterruptedException {
     Path rootDir = new Path(conf.get(HConstants.HBASE_DIR));
     FileSystem fs = rootDir.getFileSystem(conf);
 
@@ -576,19 +1003,21 @@ public class HBaseFsck {
     }
 
     // level 1:  <HBASE_DIR>/*
-    WorkItemHdfsDir[] dirs = new WorkItemHdfsDir[tableDirs.size()];  
+    WorkItemHdfsDir[] dirs = new WorkItemHdfsDir[tableDirs.size()];
     int num = 0;
     for (FileStatus tableDir : tableDirs) {
-      dirs[num] = new WorkItemHdfsDir(this, fs, errors, tableDir); 
+      LOG.debug("Loading region dirs from " +tableDir.getPath());
+      dirs[num] = new WorkItemHdfsDir(this, fs, errors, tableDir);
       executor.execute(dirs[num]);
       num++;
     }
 
     // wait for all directories to be done
     for (int i = 0; i < num; i++) {
-      synchronized (dirs[i]) {
-        while (!dirs[i].isDone()) {
-          dirs[i].wait();
+      WorkItemHdfsDir dir = dirs[i];
+      synchronized (dir) {
+        while (!dir.isDone()) {
+          dir.wait();
         }
       }
     }
@@ -599,7 +1028,7 @@ public class HBaseFsck {
    * as if it were in a META table. This is so that we can check
    * deployment of ROOT.
    */
-  boolean recordRootRegion() throws IOException {
+  private boolean recordRootRegion() throws IOException {
     HRegionLocation rootLocation = connection.locateRegion(
       HConstants.ROOT_TABLE_NAME, HConstants.EMPTY_START_ROW);
 
@@ -619,7 +1048,7 @@ public class HBaseFsck {
     MetaEntry m =
       new MetaEntry(rootLocation.getRegionInfo(), sn, System.currentTimeMillis());
     HbckInfo hbInfo = new HbckInfo(m);
-    regionInfo.put(rootLocation.getRegionInfo().getEncodedName(), hbInfo);
+    regionInfoMap.put(rootLocation.getRegionInfo().getEncodedName(), hbInfo);
     return true;
   }
 
@@ -654,7 +1083,8 @@ public class HBaseFsck {
    * @throws IOException if a remote or network exception occurs
    */
   void processRegionServers(Collection<ServerName> regionServerList)
-  throws IOException, InterruptedException {
+    throws IOException, InterruptedException {
+
     WorkItemRegion[] work = new WorkItemRegion[regionServerList.size()];
     int num = 0;
 
@@ -677,27 +1107,145 @@ public class HBaseFsck {
 
   /**
    * Check consistency of all regions that have been found in previous phases.
-   * @throws KeeperException
-   * @throws InterruptedException
    */
-  void checkConsistency()
+  private void checkAndFixConsistency()
   throws IOException, KeeperException, InterruptedException {
-    for (java.util.Map.Entry<String, HbckInfo> e: regionInfo.entrySet()) {
-      doConsistencyCheck(e.getKey(), e.getValue());
+    for (java.util.Map.Entry<String, HbckInfo> e: regionInfoMap.entrySet()) {
+      checkRegionConsistency(e.getKey(), e.getValue());
+    }
+  }
+
+  /**
+   * Deletes region from meta table
+   */
+  private void deleteMetaRegion(HbckInfo hi) throws IOException {
+    Delete d = new Delete(hi.metaEntry.getRegionName());
+    meta.delete(d);
+    meta.flushCommits();
+    LOG.info("Deleted " + hi.metaEntry.getRegionNameAsString() + " from META" );
+  }
+
+  /**
+   * This backwards-compatibility wrapper for permanently offlining a region
+   * that should not be alive.  If the region server does not support the
+   * "offline" method, it will use the closest unassign method instead.  This
+   * will basically work until one attempts to disable or delete the affected
+   * table.  The problem has to do with in-memory only master state, so
+   * restarting the HMaster or failing over to another should fix this.
+   */
+  private void offline(byte[] regionName) throws IOException {
+    String regionString = Bytes.toStringBinary(regionName);
+    if (!rsSupportsOffline) {
+      LOG.warn("Using unassign region " + regionString
+          + " instead of using offline method, you should"
+          + " restart HMaster after these repairs");
+      admin.unassign(regionName, true);
+      return;
+    }
+
+    // first time we assume the rs's supports #offline.
+    try {
+      LOG.info("Offlining region " + regionString);
+      admin.getMaster().offline(regionName);
+    } catch (IOException ioe) {
+      String notFoundMsg = "java.lang.NoSuchMethodException: " +
+        "org.apache.hadoop.hbase.master.HMaster.offline([B)";
+      if (ioe.getMessage().contains(notFoundMsg)) {
+        LOG.warn("Using unassign region " + regionString
+            + " instead of using offline method, you should"
+            + " restart HMaster after these repairs");
+        rsSupportsOffline = false; // in the future just use unassign
+        admin.unassign(regionName, true);
+        return;
+      }
+      throw ioe;
+    }
+  }
+
+  private void undeployRegions(HbckInfo hi) throws IOException, InterruptedException {
+    for (OnlineEntry rse : hi.deployedEntries) {
+      LOG.debug("Undeploy region "  + rse.hri + " from " + rse.hsa);
+      try {
+        HBaseFsckRepair.closeRegionSilentlyAndWait(admin, rse.hsa, rse.hri);
+        offline(rse.hri.getRegionName());
+      } catch (IOException ioe) {
+        LOG.warn("Got exception when attempting to offline region "
+            + Bytes.toString(rse.hri.getRegionName()), ioe);
+      }
+    }
+  }
+
+  /**
+   * Attempts to undeploy a region from a region server based in information in
+   * META.  Any operations that modify the file system should make sure that
+   * its corresponding region is not deployed to prevent data races.
+   *
+   * A separate call is required to update the master in-memory region state
+   * kept in the AssignementManager.  Because disable uses this state instead of
+   * that found in META, we can't seem to cleanly disable/delete tables that
+   * have been hbck fixed.  When used on a version of HBase that does not have
+   * the offline ipc call exposed on the master (<0.90.5, <0.92.0) a master
+   * restart or failover may be required.
+   */
+  private void closeRegion(HbckInfo hi) throws IOException, InterruptedException {
+    if (hi.metaEntry == null && hi.hdfsEntry == null) {
+      undeployRegions(hi);
+      return;
+    }
+
+    // get assignment info and hregioninfo from meta.
+    Get get = new Get(hi.getRegionName());
+    get.addColumn(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);
+    get.addColumn(HConstants.CATALOG_FAMILY, HConstants.SERVER_QUALIFIER);
+    get.addColumn(HConstants.CATALOG_FAMILY, HConstants.STARTCODE_QUALIFIER);
+    Result r = meta.get(get);
+    byte[] value = r.getValue(HConstants.CATALOG_FAMILY, HConstants.SERVER_QUALIFIER);
+    byte[] startcodeBytes = r.getValue(HConstants.CATALOG_FAMILY, HConstants.STARTCODE_QUALIFIER);
+    if (value == null || startcodeBytes == null) {
+      errors.reportError("Unable to close region "
+          + hi.getRegionNameAsString() +  " because meta does not "
+          + "have handle to reach it.");
+      return;
+    }
+    long startcode = Bytes.toLong(startcodeBytes);
+
+    ServerName hsa = new ServerName(Bytes.toString(value), startcode);
+    byte[] hriVal = r.getValue(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);
+    HRegionInfo hri= Writables.getHRegionInfoOrNull(hriVal);
+    if (hri == null) {
+      LOG.warn("Unable to close region " + hi.getRegionNameAsString()
+          + " because META had invalid or missing "
+          + HConstants.CATALOG_FAMILY_STR + ":"
+          + Bytes.toString(HConstants.REGIONINFO_QUALIFIER)
+          + " qualifier value.");
+      return;
+    }
+
+    // close the region -- close files and remove assignment
+    HBaseFsckRepair.closeRegionSilentlyAndWait(admin, hsa, hri);
+  }
+
+  private void tryAssignmentRepair(HbckInfo hbi, String msg) throws IOException,
+    KeeperException, InterruptedException {
+    // If we are trying to fix the errors
+    if (shouldFixAssignments()) {
+      errors.print(msg);
+      undeployRegions(hbi);
+      setShouldRerun();
+      HBaseFsckRepair.fixUnassigned(admin, hbi.getHdfsHRI());
+      HBaseFsckRepair.waitUntilAssigned(admin, hbi.getHdfsHRI());
     }
   }
 
   /**
    * Check a single region for consistency and correct deployment.
-   * @throws KeeperException
-   * @throws InterruptedException
    */
-  void doConsistencyCheck(final String key, final HbckInfo hbi)
+  private void checkRegionConsistency(final String key, final HbckInfo hbi)
   throws IOException, KeeperException, InterruptedException {
     String descriptiveName = hbi.toString();
 
     boolean inMeta = hbi.metaEntry != null;
-    boolean inHdfs = hbi.foundRegionDir != null;
+    boolean inHdfs = hbi.getHdfsRegionDir()!= null;
     boolean hasMetaAssignment = inMeta && hbi.metaEntry.regionServer != null;
     boolean isDeployed = !hbi.deployedOn.isEmpty();
     boolean isMultiplyDeployed = hbi.deployedOn.size() > 1;
@@ -707,18 +1255,21 @@ public class HBaseFsck {
     boolean splitParent =
       (hbi.metaEntry == null)? false: hbi.metaEntry.isSplit() && hbi.metaEntry.isOffline();
     boolean shouldBeDeployed = inMeta && !isTableDisabled(hbi.metaEntry);
-    boolean recentlyModified = hbi.foundRegionDir != null &&
-      hbi.foundRegionDir.getModificationTime() + timelag > System.currentTimeMillis();
+    boolean recentlyModified = hbi.getHdfsRegionDir() != null &&
+      hbi.getModTime() + timelag > System.currentTimeMillis();
 
     // ========== First the healthy cases =============
-    if (hbi.onlyEdits) {
+    if (hbi.containsOnlyHdfsEdits()) {
       return;
     }
     if (inMeta && inHdfs && isDeployed && deploymentMatchesMeta && shouldBeDeployed) {
       return;
-    } else if (inMeta && !isDeployed && splitParent) {
+    } else if (inMeta && inHdfs && !isDeployed && splitParent) {
+      LOG.warn("Region " + descriptiveName + " is a split parent in META and in HDFS");
       return;
-    } else if (inMeta && !shouldBeDeployed && !isDeployed) {
+    } else if (inMeta && inHdfs && !shouldBeDeployed && !isDeployed) {
+      LOG.info("Region " + descriptiveName + " is in META, and in a disabled " +
+        "tabled that is not deployed");
       return;
     } else if (recentlyModified) {
       LOG.warn("Region " + descriptiveName + " was recently modified -- skipping");
@@ -732,46 +1283,87 @@ public class HBaseFsck {
       errors.reportError(ERROR_CODE.NOT_IN_META_HDFS, "Region "
           + descriptiveName + ", key=" + key + ", not on HDFS or in META but " +
           "deployed on " + Joiner.on(", ").join(hbi.deployedOn));
+      if (shouldFixAssignments()) {
+        undeployRegions(hbi);
+      }
+
     } else if (!inMeta && inHdfs && !isDeployed) {
       errors.reportError(ERROR_CODE.NOT_IN_META_OR_DEPLOYED, "Region "
           + descriptiveName + " on HDFS, but not listed in META " +
           "or deployed on any region server");
+      // restore region consistency of an adopted orphan
+      if (shouldFixMeta()) {
+        if (!hbi.isHdfsRegioninfoPresent()) {
+          LOG.error("Region " + hbi.getHdfsHRI() + " could have been repaired"
+              +  " in table integrity repair phase if -fixHdfsOrphans was" +
+              " used.");
+          return;
+        }
+
+        LOG.info("Patching .META. with .regioninfo: " + hbi.getHdfsHRI());
+        HBaseFsckRepair.fixMetaHoleOnline(conf, hbi.getHdfsHRI());
+
+        tryAssignmentRepair(hbi, "Trying to reassign region...");
+      }
+
     } else if (!inMeta && inHdfs && isDeployed) {
       errors.reportError(ERROR_CODE.NOT_IN_META, "Region " + descriptiveName
           + " not in META, but deployed on " + Joiner.on(", ").join(hbi.deployedOn));
+      debugLsr(hbi.getHdfsRegionDir());
+      if (shouldFixMeta()) {
+        if (!hbi.isHdfsRegioninfoPresent()) {
+          LOG.error("This should have been repaired in table integrity repair phase");
+          return;
+        }
+
+        LOG.info("Patching .META. with with .regioninfo: " + hbi.getHdfsHRI());
+        HBaseFsckRepair.fixMetaHoleOnline(conf, hbi.getHdfsHRI());
+
+        tryAssignmentRepair(hbi, "Trying to fix unassigned region...");
+      }
 
     // ========== Cases where the region is in META =============
     } else if (inMeta && !inHdfs && !isDeployed) {
       errors.reportError(ERROR_CODE.NOT_IN_HDFS_OR_DEPLOYED, "Region "
           + descriptiveName + " found in META, but not in HDFS "
           + "or deployed on any region server.");
+      if (shouldFixMeta()) {
+        deleteMetaRegion(hbi);
+      }
     } else if (inMeta && !inHdfs && isDeployed) {
       errors.reportError(ERROR_CODE.NOT_IN_HDFS, "Region " + descriptiveName
           + " found in META, but not in HDFS, " +
           "and deployed on " + Joiner.on(", ").join(hbi.deployedOn));
+      // We treat HDFS as ground truth.  Any information in meta is transient
+      // and equivalent data can be regenerated.  So, lets unassign and remove
+      // these problems from META.
+      if (shouldFixAssignments()) {
+        errors.print("Trying to fix unassigned region...");
+        closeRegion(hbi);// Close region will cause RS to abort.
+      }
+      if (shouldFixMeta()) {
+        // wait for it to complete
+        deleteMetaRegion(hbi);
+      }
     } else if (inMeta && inHdfs && !isDeployed && shouldBeDeployed) {
       errors.reportError(ERROR_CODE.NOT_DEPLOYED, "Region " + descriptiveName
           + " not deployed on any region server.");
-      // If we are trying to fix the errors
-      if (shouldFix()) {
-        errors.print("Trying to fix unassigned region...");
-        setShouldRerun();
-        HBaseFsckRepair.fixUnassigned(this.admin, hbi.metaEntry);
-      }
+      tryAssignmentRepair(hbi, "Trying to fix unassigned region...");
     } else if (inMeta && inHdfs && isDeployed && !shouldBeDeployed) {
-      errors.reportError(ERROR_CODE.SHOULD_NOT_BE_DEPLOYED, "Region "
-          + descriptiveName + " should not be deployed according " +
+      errors.reportError(ERROR_CODE.SHOULD_NOT_BE_DEPLOYED, "UNHANDLED CASE:" +
+          " Region " + descriptiveName + " should not be deployed according " +
           "to META, but is deployed on " + Joiner.on(", ").join(hbi.deployedOn));
+      // TODO test and handle this case.
     } else if (inMeta && inHdfs && isMultiplyDeployed) {
       errors.reportError(ERROR_CODE.MULTI_DEPLOYED, "Region " + descriptiveName
           + " is listed in META on region server " + hbi.metaEntry.regionServer
           + " but is multiply assigned to region servers " +
           Joiner.on(", ").join(hbi.deployedOn));
       // If we are trying to fix the errors
-      if (shouldFix()) {
+      if (shouldFixAssignments()) {
         errors.print("Trying to fix assignment error...");
         setShouldRerun();
-        HBaseFsckRepair.fixDupeAssignment(this.admin, hbi.metaEntry, hbi.deployedOn);
+        HBaseFsckRepair.fixMultiAssignment(admin, hbi.metaEntry, hbi.deployedOn);
       }
     } else if (inMeta && inHdfs && isDeployed && !deploymentMatchesMeta) {
       errors.reportError(ERROR_CODE.SERVER_DOES_NOT_MATCH_META, "Region "
@@ -779,10 +1371,11 @@ public class HBaseFsck {
           hbi.metaEntry.regionServer + " but found on region server " +
           hbi.deployedOn.get(0));
       // If we are trying to fix the errors
-      if (shouldFix()) {
+      if (shouldFixAssignments()) {
         errors.print("Trying to fix assignment error...");
         setShouldRerun();
-        HBaseFsckRepair.fixDupeAssignment(this.admin, hbi.metaEntry, hbi.deployedOn);
+        HBaseFsckRepair.fixMultiAssignment(admin, hbi.metaEntry, hbi.deployedOn);
+        HBaseFsckRepair.waitUntilAssigned(admin, hbi.getHdfsHRI());
       }
     } else {
       errors.reportError(ERROR_CODE.UNKNOWN, "Region " + descriptiveName +
@@ -800,13 +1393,37 @@ public class HBaseFsck {
    * Checks tables integrity. Goes over all regions and scans the tables.
    * Collects all the pieces for each table and checks if there are missing,
    * repeated or overlapping ones.
+   * @throws IOException
    */
-  void checkIntegrity() {
-    for (HbckInfo hbi : regionInfo.values()) {
+  TreeMap<String, TableInfo> checkIntegrity() throws IOException {
+    tablesInfo = new TreeMap<String,TableInfo> ();
+    List<HbckInfo> noHDFSRegionInfos = new ArrayList<HbckInfo>();
+    LOG.debug("There are " + regionInfoMap.size() + " region info entries");
+    for (HbckInfo hbi : regionInfoMap.values()) {
       // Check only valid, working regions
-      if (hbi.metaEntry == null) continue;
-      if (hbi.metaEntry.regionServer == null) continue;
-      if (hbi.onlyEdits) continue;
+      if (hbi.metaEntry == null) {
+        // this assumes that consistency check has run loadMetaEntry
+        noHDFSRegionInfos.add(hbi);
+        Path p = hbi.getHdfsRegionDir();
+        if (p == null) {
+          errors.report("No regioninfo in Meta or HDFS. " + hbi);
+        }
+
+        // TODO test.
+        continue;
+      }
+      if (hbi.metaEntry.regionServer == null) {
+        errors.detail("Skipping region because no region server: " + hbi);
+        continue;
+      }
+      if (hbi.metaEntry.isOffline()) {
+        errors.detail("Skipping region because it is offline: " + hbi);
+        continue;
+      }
+      if (hbi.containsOnlyHdfsEdits()) {
+        errors.detail("Skipping region because it only contains edits" + hbi);
+        continue;
+      }
 
       // Missing regionDir or over-deployment is checked elsewhere. Include
       // these cases in modTInfo, so we can evaluate those regions as part of
@@ -817,9 +1434,9 @@ public class HBaseFsck {
 
       // We should be safe here
       String tableName = hbi.metaEntry.getTableNameAsString();
-      TInfo modTInfo = tablesInfo.get(tableName);
+      TableInfo modTInfo = tablesInfo.get(tableName);
       if (modTInfo == null) {
-        modTInfo = new TInfo(tableName);
+        modTInfo = new TableInfo(tableName);
       }
       for (ServerName server : hbi.deployedOn) {
         modTInfo.addServer(server);
@@ -830,32 +1447,111 @@ public class HBaseFsck {
       tablesInfo.put(tableName, modTInfo);
     }
 
-    for (TInfo tInfo : tablesInfo.values()) {
-      if (!tInfo.checkRegionChain()) {
+    for (TableInfo tInfo : tablesInfo.values()) {
+      TableIntegrityErrorHandler handler = tInfo.new IntegrityFixSuggester(tInfo, errors);
+      if (!tInfo.checkRegionChain(handler)) {
         errors.report("Found inconsistency in table " + tInfo.getName());
       }
     }
+    return tablesInfo;
+  }
+
+  /**
+   * Merge hdfs data by moving from contained HbckInfo into targetRegionDir.
+   * @return number of file move fixes done to merge regions.
+   */
+  public int mergeRegionDirs(Path targetRegionDir, HbckInfo contained) throws IOException {
+    int fileMoves = 0;
+
+    LOG.debug("Contained region dir after close and pause");
+    debugLsr(contained.getHdfsRegionDir());
+
+    // rename the contained into the container.
+    FileSystem fs = targetRegionDir.getFileSystem(conf);
+    FileStatus[] dirs = fs.listStatus(contained.getHdfsRegionDir());
+
+    if (dirs == null) {
+      if (!fs.exists(contained.getHdfsRegionDir())) {
+        LOG.warn("HDFS region dir " + contained.getHdfsRegionDir() + " already sidelined.");
+      } else {
+        sidelineRegionDir(fs, contained);
+      }
+      return fileMoves;
+    }
+
+    for (FileStatus cf : dirs) {
+      Path src = cf.getPath();
+      Path dst =  new Path(targetRegionDir, src.getName());
+
+      if (src.getName().equals(HRegion.REGIONINFO_FILE)) {
+        // do not copy the old .regioninfo file.
+        continue;
+      }
+
+      if (src.getName().equals(HConstants.HREGION_OLDLOGDIR_NAME)) {
+        // do not copy the .oldlogs files
+        continue;
+      }
+
+      LOG.info("Moving files from " + src + " into containing region " + dst);
+      // FileSystem.rename is inconsistent with directories -- if the
+      // dst (foo/a) exists and is a dir, and the src (foo/b) is a dir,
+      // it moves the src into the dst dir resulting in (foo/a/b).  If
+      // the dst does not exist, and the src a dir, src becomes dst. (foo/b)
+      for (FileStatus hfile : fs.listStatus(src)) {
+        boolean success = fs.rename(hfile.getPath(), dst);
+        if (success) {
+          fileMoves++;
+        }
+      }
+      LOG.debug("Sideline directory contents:");
+      debugLsr(targetRegionDir);
+    }
+
+    // if all success.
+    sidelineRegionDir(fs, contained);
+    LOG.info("Sidelined region dir "+ contained.getHdfsRegionDir() + " into " +
+        getSidelineDir());
+    debugLsr(contained.getHdfsRegionDir());
+
+    return fileMoves;
   }
 
   /**
    * Maintain information about a particular table.
    */
-  private class TInfo {
+  public class TableInfo {
     String tableName;
     TreeSet <ServerName> deployedOn;
 
+    // backwards regions
     final List<HbckInfo> backwards = new ArrayList<HbckInfo>();
+
+    // region split calculator
     final RegionSplitCalculator<HbckInfo> sc = new RegionSplitCalculator<HbckInfo>(cmp);
 
+    // Histogram of different HTableDescriptors found.  Ideally there is only one!
+    final Set<HTableDescriptor> htds = new HashSet<HTableDescriptor>();
+
     // key = start split, values = set of splits in problem group
-    final Multimap<byte[], HbckInfo> overlapGroups = 
+    final Multimap<byte[], HbckInfo> overlapGroups =
       TreeMultimap.create(RegionSplitCalculator.BYTES_COMPARATOR, cmp);
 
-    TInfo(String name) {
+    TableInfo(String name) {
       this.tableName = name;
       deployedOn = new TreeSet <ServerName>();
     }
 
+    /**
+     * @return descriptor common to all regions.  null if are none or multiple!
+     */
+    private HTableDescriptor getHTD() {
+      if (htds.size() == 1) {
+        return (HTableDescriptor)htds.toArray()[0];
+      }
+      return null;
+    }
+
     public void addRegionInfo(HbckInfo hir) {
       if (Bytes.equals(hir.getEndKey(), HConstants.EMPTY_END_ROW)) {
         // end key is absolute end key, just add it.
@@ -891,12 +1587,226 @@ public class HBaseFsck {
       return sc.getStarts().size() + backwards.size();
     }
 
+    private class IntegrityFixSuggester extends TableIntegrityErrorHandlerImpl {
+      ErrorReporter errors;
+
+      IntegrityFixSuggester(TableInfo ti, ErrorReporter errors) {
+        this.errors = errors;
+        setTableInfo(ti);
+      }
+
+      @Override
+      public void handleRegionStartKeyNotEmpty(HbckInfo hi) throws IOException{
+        errors.reportError(ERROR_CODE.FIRST_REGION_STARTKEY_NOT_EMPTY,
+            "First region should start with an empty key.  You need to "
+            + " create a new region and regioninfo in HDFS to plug the hole.",
+            getTableInfo(), hi);
+      }
+
+      @Override
+      public void handleDegenerateRegion(HbckInfo hi) throws IOException{
+        errors.reportError(ERROR_CODE.DEGENERATE_REGION,
+            "Region has the same start and end key.", getTableInfo(), hi);
+      }
+
+      @Override
+      public void handleDuplicateStartKeys(HbckInfo r1, HbckInfo r2) throws IOException{
+        byte[] key = r1.getStartKey();
+        // dup start key
+        errors.reportError(ERROR_CODE.DUPE_STARTKEYS,
+            "Multiple regions have the same startkey: "
+            + Bytes.toStringBinary(key), getTableInfo(), r1);
+        errors.reportError(ERROR_CODE.DUPE_STARTKEYS,
+            "Multiple regions have the same startkey: "
+            + Bytes.toStringBinary(key), getTableInfo(), r2);
+      }
+
+      @Override
+      public void handleOverlapInRegionChain(HbckInfo hi1, HbckInfo hi2) throws IOException{
+        errors.reportError(ERROR_CODE.OVERLAP_IN_REGION_CHAIN,
+            "There is an overlap in the region chain.",
+            getTableInfo(), hi1, hi2);
+      }
+
+      @Override
+      public void handleHoleInRegionChain(byte[] holeStart, byte[] holeStop) throws IOException{
+        errors.reportError(
+            ERROR_CODE.HOLE_IN_REGION_CHAIN,
+            "There is a hole in the region chain between "
+                + Bytes.toStringBinary(holeStart) + " and "
+                + Bytes.toStringBinary(holeStop)
+                + ".  You need to create a new .regioninfo and region "
+                + "dir in hdfs to plug the hole.");
+      }
+    };
+
+    /**
+     * This handler fixes integrity errors from hdfs information.  There are
+     * basically three classes of integrity problems 1) holes, 2) overlaps, and
+     * 3) invalid regions.
+     *
+     * This class overrides methods that fix holes and the overlap group case.
+     * Individual cases of particular overlaps are handled by the general
+     * overlap group merge repair case.
+     *
+     * If hbase is online, this forces regions offline before doing merge
+     * operations.
+     */
+    private class HDFSIntegrityFixer extends IntegrityFixSuggester {
+      Configuration conf;
+
+      boolean fixOverlaps = true;
+
+      HDFSIntegrityFixer(TableInfo ti, ErrorReporter errors, Configuration conf,
+          boolean fixHoles, boolean fixOverlaps) {
+        super(ti, errors);
+        this.conf = conf;
+        this.fixOverlaps = fixOverlaps;
+        // TODO properly use fixHoles
+      }
+
+      /**
+       * This is a special case hole -- when the first region of a table is
+       * missing from META, HBase doesn't acknowledge the existance of the
+       * table.
+       */
+      public void handleRegionStartKeyNotEmpty(HbckInfo next) throws IOException {
+        errors.reportError(ERROR_CODE.FIRST_REGION_STARTKEY_NOT_EMPTY,
+            "First region should start with an empty key.  Creating a new " +
+            "region and regioninfo in HDFS to plug the hole.",
+            getTableInfo(), next);
+        HTableDescriptor htd = getTableInfo().getHTD();
+        // from special EMPTY_START_ROW to next region's startKey
+        HRegionInfo newRegion = new HRegionInfo(htd.getName(),
+            HConstants.EMPTY_START_ROW, next.getStartKey());
+
+        // TODO test
+        HRegion region = HBaseFsckRepair.createHDFSRegionDir(conf, newRegion, htd);
+        LOG.info("Table region start key was not empty.  Created new empty region: "
+            + newRegion + " " +region);
+        fixes++;
+      }
+
+      /**
+       * There is a hole in the hdfs regions that violates the table integrity
+       * rules.  Create a new empty region that patches the hole.
+       */
+      public void handleHoleInRegionChain(byte[] holeStartKey, byte[] holeStopKey) throws IOException {
+        errors.reportError(
+            ERROR_CODE.HOLE_IN_REGION_CHAIN,
+            "There is a hole in the region chain between "
+                + Bytes.toStringBinary(holeStartKey) + " and "
+                + Bytes.toStringBinary(holeStopKey)
+                + ".  Creating a new regioninfo and region "
+                + "dir in hdfs to plug the hole.");
+        HTableDescriptor htd = getTableInfo().getHTD();
+        HRegionInfo newRegion = new HRegionInfo(htd.getName(), holeStartKey, holeStopKey);
+        HRegion region = HBaseFsckRepair.createHDFSRegionDir(conf, newRegion, htd);
+        LOG.info("Plugged hold by creating new empty region: "+ newRegion + " " +region);
+        fixes++;
+      }
+
+      /**
+       * This takes set of overlapping regions and merges them into a single
+       * region.  This covers cases like degenerate regions, shared start key,
+       * general overlaps, duplicate ranges, and partial overlapping regions.
+       *
+       * Cases:
+       * - Clean regions that overlap
+       * - Only .oldlogs regions (can't find start/stop range, or figure out)
+       */
+      @Override
+      public void handleOverlapGroup(Collection<HbckInfo> overlap)
+          throws IOException {
+        Preconditions.checkNotNull(overlap);
+        Preconditions.checkArgument(overlap.size() >0);
+
+        if (!this.fixOverlaps) {
+          LOG.warn("Not attempting to repair overlaps.");
+          return;
+        }
+
+        if (overlap.size() > maxMerge) {
+          LOG.warn("Overlap group has " + overlap.size() + " overlapping " +
+              "regions which is greater than " + maxMerge + ", the max " +
+              "number of regions to merge.");
+          return;
+        }
+
+        LOG.info("== Merging regions into one region: "
+            + Joiner.on(",").join(overlap));
+        // get the min / max range and close all concerned regions
+        Pair<byte[], byte[]> range = null;
+        for (HbckInfo hi : overlap) {
+          if (range == null) {
+            range = new Pair<byte[], byte[]>(hi.getStartKey(), hi.getEndKey());
+          } else {
+            if (RegionSplitCalculator.BYTES_COMPARATOR
+                .compare(hi.getStartKey(), range.getFirst()) < 0) {
+              range.setFirst(hi.getStartKey());
+            }
+            if (RegionSplitCalculator.BYTES_COMPARATOR
+                .compare(hi.getEndKey(), range.getSecond()) > 0) {
+              range.setSecond(hi.getEndKey());
+            }
+          }
+          // need to close files so delete can happen.
+          LOG.debug("Closing region before moving data around: " +  hi);
+          LOG.debug("Contained region dir before close");
+          debugLsr(hi.getHdfsRegionDir());
+          try {
+            closeRegion(hi);
+          } catch (IOException ioe) {
+            // TODO exercise this
+            LOG.warn("Was unable to close region " + hi.getRegionNameAsString()
+                + ".  Just continuing... ");
+          } catch (InterruptedException e) {
+            // TODO exercise this
+            LOG.warn("Was unable to close region " + hi.getRegionNameAsString()
+                + ".  Just continuing... ");
+          }
+
+          try {
+            LOG.info("Offlining region: " + hi);
+            offline(hi.getRegionName());
+          } catch (IOException ioe) {
+            LOG.warn("Unable to offline region from master: " + hi, ioe);
+          }
+        }
+
+        // create new empty container region.
+        HTableDescriptor htd = getTableInfo().getHTD();
+        // from start key to end Key
+        HRegionInfo newRegion = new HRegionInfo(htd.getName(), range.getFirst(),
+            range.getSecond());
+        HRegion region = HBaseFsckRepair.createHDFSRegionDir(conf, newRegion, htd);
+        LOG.info("Created new empty container region: " +
+            newRegion + " to contain regions: " + Joiner.on(",").join(overlap));
+        debugLsr(region.getRegionDir());
+
+        // all target regions are closed, should be able to safely cleanup.
+        boolean didFix= false;
+        Path target = region.getRegionDir();
+        for (HbckInfo contained : overlap) {
+          LOG.info("Merging " + contained  + " into " + target );
+          int merges = mergeRegionDirs(target, contained);
+          if (merges > 0) {
+            didFix = true;
+          }
+        }
+        if (didFix) {
+          fixes++;
+        }
+      }
+    };
+
     /**
      * Check the region chain (from META) of this table.  We are looking for
      * holes, overlaps, and cycles.
      * @return false if there are errors
+     * @throws IOException
      */
-    public boolean checkRegionChain() {
+    public boolean checkRegionChain(TableIntegrityErrorHandler handler) throws IOException {
       int originalErrorsCount = errors.getErrorList().size();
       Multimap<byte[], HbckInfo> regions = sc.calcCoverage();
       SortedSet<byte[]> splits = sc.getSplits();
@@ -907,12 +1817,7 @@ public class HBaseFsck {
         Collection<HbckInfo> ranges = regions.get(key);
         if (prevKey == null && !Bytes.equals(key, HConstants.EMPTY_BYTE_ARRAY)) {
           for (HbckInfo rng : ranges) {
-            // TODO offline fix region hole.
-
-            errors.reportError(ERROR_CODE.FIRST_REGION_STARTKEY_NOT_EMPTY,
-                "First region should start with an empty key.  You need to "
-                + " create a new region and regioninfo in HDFS to plug the hole.",
-                this, rng);
+            handler.handleRegionStartKeyNotEmpty(rng);
           }
         }
 
@@ -922,8 +1827,7 @@ public class HBaseFsck {
           byte[] endKey = rng.getEndKey();
           endKey = (endKey.length == 0) ? null : endKey;
           if (Bytes.equals(rng.getStartKey(),endKey)) {
-            errors.reportError(ERROR_CODE.DEGENERATE_REGION,
-              "Region has the same start and end key.", this, rng);
+            handler.handleDegenerateRegion(rng);
           }
         }
 
@@ -950,18 +1854,10 @@ public class HBaseFsck {
             subRange.remove(r1);
             for (HbckInfo r2 : subRange) {
               if (Bytes.compareTo(r1.getStartKey(), r2.getStartKey())==0) {
-                // dup start key
-                errors.reportError(ERROR_CODE.DUPE_STARTKEYS,
-                    "Multiple regions have the same startkey: "
-                    + Bytes.toStringBinary(key), this, r1);
-                errors.reportError(ERROR_CODE.DUPE_STARTKEYS,
-                    "Multiple regions have the same startkey: "
-                    + Bytes.toStringBinary(key), this, r2);
+                handler.handleDuplicateStartKeys(r1,r2);
               } else {
                 // overlap
-                errors.reportError(ERROR_CODE.OVERLAP_IN_REGION_CHAIN,
-                    "There is an overlap in the region chain.",
-                    this, r1);
+                handler.handleOverlapInRegionChain(r1, r2);
               }
             }
           }
@@ -976,17 +1872,16 @@ public class HBaseFsck {
           // if higher key is null we reached the top.
           if (holeStopKey != null) {
             // hole
-            errors.reportError(ERROR_CODE.HOLE_IN_REGION_CHAIN,
-                "There is a hole in the region chain between "
-                + Bytes.toStringBinary(key) + " and "
-                + Bytes.toStringBinary(holeStopKey)
-                + ".  You need to create a new regioninfo and region "
-                + "dir in hdfs to plug the hole.");
+            handler.handleHoleInRegionChain(key, holeStopKey);
           }
         }
         prevKey = key;
       }
 
+      for (Collection<HbckInfo> overlap : overlapGroups.asMap().values()) {
+        handler.handleOverlapGroup(overlap);
+      }
+
       if (details) {
         // do full region split map dump
         System.out.println("---- Table '"  +  this.tableName 
@@ -1034,8 +1929,10 @@ public class HBaseFsck {
     }
   }
 
-  public Multimap<byte[], HbckInfo> getOverlapGroups(String table) {
-    return tablesInfo.get(table).overlapGroups;
+  public Multimap<byte[], HbckInfo> getOverlapGroups(
+      String table) {
+    TableInfo ti = tablesInfo.get(table);
+    return ti.overlapGroups;
   }
 
   /**
@@ -1051,7 +1948,7 @@ public class HBaseFsck {
     List<String> tableNames = new ArrayList<String>();
     long now = System.currentTimeMillis();
 
-    for (HbckInfo hbi : regionInfo.values()) {
+    for (HbckInfo hbi : regionInfoMap.values()) {
       MetaEntry info = hbi.metaEntry;
 
       // if the start key is zero, then we have found the first region of a table.
@@ -1085,10 +1982,10 @@ public class HBaseFsck {
    * and returned.
    */
   private synchronized HbckInfo getOrCreateInfo(String name) {
-    HbckInfo hbi = regionInfo.get(name);
+    HbckInfo hbi = regionInfoMap.get(name);
     if (hbi == null) {
       hbi = new HbckInfo(null);
-      regionInfo.put(name, hbi);
+      regionInfoMap.put(name, hbi);
     }
     return hbi;
   }
@@ -1102,10 +1999,10 @@ public class HBaseFsck {
    * @throws KeeperException
    * @throws InterruptedException
     */
-  boolean checkMetaEntries()
-  throws IOException, KeeperException, InterruptedException {
+  boolean checkMetaRegion()
+    throws IOException, KeeperException, InterruptedException {
     List <HbckInfo> metaRegions = Lists.newArrayList();
-    for (HbckInfo value : regionInfo.values()) {
+    for (HbckInfo value : regionInfoMap.values()) {
       if (value.metaEntry.isMetaRegion()) {
         metaRegions.add(value);
       }
@@ -1116,22 +2013,23 @@ public class HBaseFsck {
       HRegionLocation rootLocation = connection.locateRegion(
         HConstants.ROOT_TABLE_NAME, HConstants.EMPTY_START_ROW);
       HbckInfo root =
-          regionInfo.get(rootLocation.getRegionInfo().getEncodedName());
+          regionInfoMap.get(rootLocation.getRegionInfo().getEncodedName());
 
       // If there is no region holding .META.
       if (metaRegions.size() == 0) {
         errors.reportError(ERROR_CODE.NO_META_REGION, ".META. is not found on any region.");
-        if (shouldFix()) {
+        if (shouldFixAssignments()) {
           errors.print("Trying to fix a problem with .META...");
           setShouldRerun();
           // try to fix it (treat it as unassigned region)
-          HBaseFsckRepair.fixUnassigned(this.admin, root.metaEntry);
+          HBaseFsckRepair.fixUnassigned(admin, root.metaEntry);
+          HBaseFsckRepair.waitUntilAssigned(admin, root.getHdfsHRI());
         }
       }
       // If there are more than one regions pretending to hold the .META.
       else if (metaRegions.size() > 1) {
         errors.reportError(ERROR_CODE.MULTI_META_REGION, ".META. is found on more than one region.");
-        if (shouldFix()) {
+        if (shouldFixAssignments()) {
           errors.print("Trying to fix a problem with .META...");
           setShouldRerun();
           // try fix it (treat is a dupe assignment)
@@ -1139,7 +2037,7 @@ public class HBaseFsck {
           for (HbckInfo mRegion : metaRegions) {
             deployedOn.add(mRegion.metaEntry.regionServer);
           }
-          HBaseFsckRepair.fixDupeAssignment(this.admin, root.metaEntry, deployedOn);
+          HBaseFsckRepair.fixMultiAssignment(admin, root.metaEntry, deployedOn);
         }
       }
       // rerun hbck with hopefully fixed META
@@ -1153,7 +2051,16 @@ public class HBaseFsck {
    * Scan .META. and -ROOT-, adding all regions found to the regionInfo map.
    * @throws IOException if an error is encountered
    */
-  void getMetaEntries() throws IOException {
+  boolean loadMetaEntries() throws IOException {
+
+    // get a list of all regions from the master. This involves
+    // scanning the META table
+    if (!recordRootRegion()) {
+      // Will remove later if we can fix it
+      errors.reportError("Fatal error: unable to get root region location. Exiting...");
+      return false;
+    }
+
     MetaScannerVisitor visitor = new MetaScannerVisitor() {
       int countRecord = 1;
 
@@ -1180,7 +2087,7 @@ public class HBaseFsck {
           }
           MetaEntry m = new MetaEntry(pair.getFirst(), sn, ts);
           HbckInfo hbInfo = new HbckInfo(m);
-          HbckInfo previous = regionInfo.put(pair.getFirst().getEncodedName(), hbInfo);
+          HbckInfo previous = regionInfoMap.put(pair.getFirst().getEncodedName(), hbInfo);
           if (previous != null) {
             throw new IOException("Two entries in META are same " + previous);
           }
@@ -1208,13 +2115,13 @@ public class HBaseFsck {
     }
     
     errors.print("");
+    return true;
   }
 
   /**
-   * Stores the entries scanned from META
+   * Stores the regioninfo entries scanned from META
    */
   static class MetaEntry extends HRegionInfo {
-    private static final Log LOG = LogFactory.getLog(HRegionInfo.class);
     ServerName regionServer;   // server hosting this region
     long modTime;          // timestamp of most recent modification metadata
 
@@ -1223,44 +2130,168 @@ public class HBaseFsck {
       this.regionServer = regionServer;
       this.modTime = modTime;
     }
+
+    public boolean equals(Object o) {
+      boolean superEq = super.equals(o);
+      if (!superEq) {
+        return superEq;
+      }
+
+      MetaEntry me = (MetaEntry) o;
+      if (!regionServer.equals(me.regionServer)) {
+        return false;
+      }
+      return (modTime == me.modTime);
+    }
+  }
+
+  /**
+   * Stores the regioninfo entries from HDFS
+   */
+  static class HdfsEntry {
+    HRegionInfo hri;
+    Path hdfsRegionDir = null;
+    long hdfsRegionDirModTime  = 0;
+    boolean hdfsRegioninfoFilePresent = false;
+    boolean hdfsOnlyEdits = false;
   }
 
   /**
-   * Maintain information about a particular region.
+   * Stores the regioninfo retrieved from Online region servers.
+   */
+  static class OnlineEntry {
+    HRegionInfo hri;
+    ServerName hsa;
+
+    public String toString() {
+      return hsa.toString() + ";" + hri.getRegionNameAsString();
+    }
+  }
+
+  /**
+   * Maintain information about a particular region.  It gathers information
+   * from three places -- HDFS, META, and region servers.
    */
   public static class HbckInfo implements KeyRange {
-    boolean onlyEdits = false;
-    MetaEntry metaEntry = null;
-    FileStatus foundRegionDir = null;
-    List<ServerName> deployedOn = Lists.newArrayList();
-    String hdfsTableName = null; // This is set in the workitem loader.
+    private MetaEntry metaEntry = null; // info in META
+    private HdfsEntry hdfsEntry = null; // info in HDFS
+    private List<OnlineEntry> deployedEntries = Lists.newArrayList(); // on Region Server
+    private List<ServerName> deployedOn = Lists.newArrayList(); // info on RS's
 
     HbckInfo(MetaEntry metaEntry) {
       this.metaEntry = metaEntry;
     }
 
-    public synchronized void addServer(ServerName server) {
+    public synchronized void addServer(HRegionInfo hri, ServerName server) {
+      OnlineEntry rse = new OnlineEntry() ;
+      rse.hri = hri;
+      rse.hsa = server;
+      this.deployedEntries.add(rse);
       this.deployedOn.add(server);
     }
 
     public synchronized String toString() {
-      if (metaEntry != null) {
-        return metaEntry.getRegionNameAsString();
-      } else if (foundRegionDir != null) {
-        return foundRegionDir.getPath().toString();
-      } else {
-        return "UNKNOWN_REGION on " + Joiner.on(", ").join(deployedOn);
-      }
+      StringBuilder sb = new StringBuilder();
+      sb.append("{ meta => ");
+      sb.append((metaEntry != null)? metaEntry.getRegionNameAsString() : "null");
+      sb.append( ", hdfs => " + getHdfsRegionDir());
+      sb.append( ", deployed => " + Joiner.on(", ").join(deployedEntries));
+      sb.append(" }");
+      return sb.toString();
     }
 
     @Override
     public byte[] getStartKey() {
-      return this.metaEntry.getStartKey();
+      if (this.metaEntry != null) {
+        return this.metaEntry.getStartKey();
+      } else if (this.hdfsEntry != null) {
+        return this.hdfsEntry.hri.getStartKey();
+      } else {
+        LOG.error("Entry " + this + " has no meta or hdfs region start key.");
+        return null;
+      }
     }
 
     @Override
     public byte[] getEndKey() {
-      return this.metaEntry.getEndKey();
+      if (this.metaEntry != null) {
+        return this.metaEntry.getEndKey();
+      } else if (this.hdfsEntry != null) {
+        return this.hdfsEntry.hri.getEndKey();
+      } else {
+        LOG.error("Entry " + this + " has no meta or hdfs region start key.");
+        return null;
+      }
+    }
+
+    public byte[] getTableName() {
+      if (this.metaEntry != null) {
+        return this.metaEntry.getTableName();
+      } else if (this.hdfsEntry != null) {
+        // we are only guaranteed to have a path and not an HRI for hdfsEntry,
+        // so we get the name from the Path
+        Path tableDir = this.hdfsEntry.hdfsRegionDir.getParent();
+        return Bytes.toBytes(tableDir.getName());
+      } else {
+        // Currently no code exercises this path, but we could add one for
+        // getting table name from OnlineEntry
+        return null;
+      }
+    }
+
+    public String getRegionNameAsString() {
+      if (metaEntry != null) {
+        return metaEntry.getRegionNameAsString();
+      } else if (hdfsEntry != null) {
+        return hdfsEntry.hri.getRegionNameAsString();
+      } else {
+        return null;
+      }
+    }
+
+    public byte[] getRegionName() {
+      if (metaEntry != null) {
+        return metaEntry.getRegionName();
+      } else if (hdfsEntry != null) {
+        return hdfsEntry.hri.getRegionName();
+      } else {
+        return null;
+      }
+    }
+
+    Path getHdfsRegionDir() {
+      if (hdfsEntry == null) {
+        return null;
+      }
+      return hdfsEntry.hdfsRegionDir;
+    }
+
+    boolean containsOnlyHdfsEdits() {
+      if (hdfsEntry == null) {
+        return false;
+      }
+      return hdfsEntry.hdfsOnlyEdits;
+    }
+
+    boolean isHdfsRegioninfoPresent() {
+      if (hdfsEntry == null) {
+        return false;
+      }
+      return hdfsEntry.hdfsRegioninfoFilePresent;
+    }
+
+    long getModTime() {
+      if (hdfsEntry == null) {
+        return 0;
+      }
+      return hdfsEntry.hdfsRegionDirModTime;
+    }
+
+    HRegionInfo getHdfsHRI() {
+      if (hdfsEntry == null) {
+        return null;
+      }
+      return hdfsEntry.hri;
     }
   }
 
@@ -1273,21 +2304,21 @@ public class HBaseFsck {
       }
 
       int tableCompare = RegionSplitCalculator.BYTES_COMPARATOR.compare(
-          l.metaEntry.getTableName(), r.metaEntry.getTableName());
+          l.getTableName(), r.getTableName());
       if (tableCompare != 0) {
         return tableCompare;
       }
 
       int startComparison = RegionSplitCalculator.BYTES_COMPARATOR.compare(
-          l.metaEntry.getStartKey(), r.metaEntry.getStartKey());
+          l.getStartKey(), r.getStartKey());
       if (startComparison != 0) {
         return startComparison;
       }
 
       // Special case for absolute endkey
-      byte[] endKey = r.metaEntry.getEndKey();
+      byte[] endKey = r.getEndKey();
       endKey = (endKey.length == 0) ? null : endKey;
-      byte[] endKey2 = l.metaEntry.getEndKey();
+      byte[] endKey2 = l.getEndKey();
       endKey2 = (endKey2.length == 0) ? null : endKey2;
       int endComparison = RegionSplitCalculator.BYTES_COMPARATOR.compare(
           endKey2,  endKey);
@@ -1296,17 +2327,29 @@ public class HBaseFsck {
         return endComparison;
       }
 
-      // use modTime as tiebreaker.
-      return (int) (l.metaEntry.modTime - r.metaEntry.modTime);
+      // use regionId as tiebreaker.
+      // Null is considered after all possible values so make it bigger.
+      if (l.hdfsEntry == null && r.hdfsEntry == null) {
+        return 0;
+      }
+      if (l.hdfsEntry == null && r.hdfsEntry != null) {
+        return 1;
+      }
+      // l.hdfsEntry must not be null
+      if (r.hdfsEntry == null) {
+        return -1;
+      }
+      // both l.hdfsEntry and r.hdfsEntry must not be null.
+      return (int) (l.hdfsEntry.hri.getRegionId()- r.hdfsEntry.hri.getRegionId());
     }
   };
 
   /**
    * Prints summary of all tables found on the system.
    */
-  private void printTableSummary() {
+  private void printTableSummary(TreeMap<String, TableInfo> tablesInfo) {
     System.out.println("Summary:");
-    for (TInfo tInfo : tablesInfo.values()) {
+    for (TableInfo tInfo : tablesInfo.values()) {
       if (errors.tableHasErrors(tInfo)) {
         System.out.println("Table " + tInfo.getName() + " is inconsistent.");
       } else {
@@ -1327,28 +2370,29 @@ public class HBaseFsck {
       NOT_IN_META_OR_DEPLOYED, NOT_IN_HDFS_OR_DEPLOYED, NOT_IN_HDFS, SERVER_DOES_NOT_MATCH_META, NOT_DEPLOYED,
       MULTI_DEPLOYED, SHOULD_NOT_BE_DEPLOYED, MULTI_META_REGION, RS_CONNECT_FAILURE,
       FIRST_REGION_STARTKEY_NOT_EMPTY, DUPE_STARTKEYS,
-      HOLE_IN_REGION_CHAIN, OVERLAP_IN_REGION_CHAIN, REGION_CYCLE, DEGENERATE_REGION
+      HOLE_IN_REGION_CHAIN, OVERLAP_IN_REGION_CHAIN, REGION_CYCLE, DEGENERATE_REGION,
+      ORPHAN_HDFS_REGION
     }
     public void clear();
     public void report(String message);
     public void reportError(String message);
     public void reportError(ERROR_CODE errorCode, String message);
-    public void reportError(ERROR_CODE errorCode, String message, TInfo table, HbckInfo info);
-    public void reportError(ERROR_CODE errorCode, String message, TInfo table, HbckInfo info1, HbckInfo info2);
+    public void reportError(ERROR_CODE errorCode, String message, TableInfo table, HbckInfo info);
+    public void reportError(ERROR_CODE errorCode, String message, TableInfo table, HbckInfo info1, HbckInfo info2);
     public int summarize();
     public void detail(String details);
     public ArrayList<ERROR_CODE> getErrorList();
     public void progress();
     public void print(String message);
     public void resetErrors();
-    public boolean tableHasErrors(TInfo table);
+    public boolean tableHasErrors(TableInfo table);
   }
 
   private static class PrintingErrorReporter implements ErrorReporter {
     public int errorCount = 0;
     private int showProgress;
 
-    Set<TInfo> errorTables = new HashSet<TInfo>();
+    Set<TableInfo> errorTables = new HashSet<TableInfo>();
 
     // for use by unit tests to verify which errors were discovered
     private ArrayList<ERROR_CODE> errorList = new ArrayList<ERROR_CODE>();
@@ -1368,18 +2412,18 @@ public class HBaseFsck {
       showProgress = 0;
     }
 
-    public synchronized void reportError(ERROR_CODE errorCode, String message, TInfo table,
+    public synchronized void reportError(ERROR_CODE errorCode, String message, TableInfo table,
                                          HbckInfo info) {
       errorTables.add(table);
-      String reference = "(region " + info.metaEntry.getRegionNameAsString() + ")";
+      String reference = "(region " + info.getRegionNameAsString() + ")";
       reportError(errorCode, reference + " " + message);
     }
 
-    public synchronized void reportError(ERROR_CODE errorCode, String message, TInfo table,
+    public synchronized void reportError(ERROR_CODE errorCode, String message, TableInfo table,
                                          HbckInfo info1, HbckInfo info2) {
       errorTables.add(table);
-      String reference = "(regions " + info1.metaEntry.getRegionNameAsString()
-          + " and " + info2.metaEntry.getRegionNameAsString() + ")";
+      String reference = "(regions " + info1.getRegionNameAsString()
+          + " and " + info2.getRegionNameAsString() + ")";
       reportError(errorCode, reference + " " + message);
     }
 
@@ -1422,7 +2466,7 @@ public class HBaseFsck {
     }
 
     @Override
-    public boolean tableHasErrors(TInfo table) {
+    public boolean tableHasErrors(TableInfo table) {
       return errorTables.contains(table);
     }
 
@@ -1476,7 +2520,8 @@ public class HBaseFsck {
     public synchronized void run() {
       errors.progress();
       try {
-        HRegionInterface server = connection.getHRegionConnection(new HServerAddress(rsinfo.getHostname(), rsinfo.getPort()));
+        HRegionInterface server =
+            connection.getHRegionConnection(rsinfo.getHostname(), rsinfo.getPort());
 
         // list all online regions from this region server
         List<HRegionInfo> regions = server.getOnlineRegions();
@@ -1498,7 +2543,7 @@ public class HBaseFsck {
         // check to see if the existence of this region matches the region in META
         for (HRegionInfo r:regions) {
           HbckInfo hbi = hbck.getOrCreateInfo(r.getEncodedName());
-          hbi.addServer(rsinfo);
+          hbi.addServer(r, rsinfo);
         }
       } catch (IOException e) {          // unable to connect to the region server. 
         errors.reportError(ERROR_CODE.RS_CONNECT_FAILURE, "RegionServer: " + rsinfo.getServerName() +
@@ -1521,7 +2566,8 @@ public class HBaseFsck {
   }
 
   /**
-   * Contact hdfs and get all information about specified table directory.
+   * Contact hdfs and get all information about specified table directory into
+   * regioninfo list.
    */
   static class WorkItemHdfsDir implements Runnable {
     private HBaseFsck hbck;
@@ -1555,36 +2601,44 @@ public class HBaseFsck {
         FileStatus[] regionDirs = fs.listStatus(tableDir.getPath());
         for (FileStatus regionDir : regionDirs) {
           String encodedName = regionDir.getPath().getName();
-
           // ignore directories that aren't hexadecimal
           if (!encodedName.toLowerCase().matches("[0-9a-f]+")) continue;
-  
+
+          LOG.debug("Loading region info from hdfs:"+ regionDir.getPath());
           HbckInfo hbi = hbck.getOrCreateInfo(encodedName);
-          hbi.hdfsTableName = tableName;
+          HdfsEntry he = new HdfsEntry();
           synchronized (hbi) {
-            if (hbi.foundRegionDir != null) {
+            if (hbi.getHdfsRegionDir() != null) {
               errors.print("Directory " + encodedName + " duplicate??" +
-                           hbi.foundRegionDir);
+                           hbi.getHdfsRegionDir());
             }
-            hbi.foundRegionDir = regionDir;
-        
+
+            he.hdfsRegionDir = regionDir.getPath();
+            he.hdfsRegionDirModTime = regionDir.getModificationTime();
+            Path regioninfoFile = new Path(he.hdfsRegionDir, HRegion.REGIONINFO_FILE);
+            he.hdfsRegioninfoFilePresent = fs.exists(regioninfoFile);
+            // we add to orphan list when we attempt to read .regioninfo
+
             // Set a flag if this region contains only edits
             // This is special case if a region is left after split
-            hbi.onlyEdits = true;
+            he.hdfsOnlyEdits = true;
             FileStatus[] subDirs = fs.listStatus(regionDir.getPath());
             Path ePath = HLog.getRegionDirRecoveredEditsDir(regionDir.getPath());
             for (FileStatus subDir : subDirs) {
               String sdName = subDir.getPath().getName();
               if (!sdName.startsWith(".") && !sdName.equals(ePath.getName())) {
-                hbi.onlyEdits = false;
+                he.hdfsOnlyEdits = false;
                 break;
               }
             }
+            hbi.hdfsEntry = he;
           }
         }
-      } catch (IOException e) {          // unable to connect to the region server. 
-        errors.reportError(ERROR_CODE.RS_CONNECT_FAILURE, "Table Directory: " + tableDir.getPath().getName() +
-                      " Unable to fetch region information. " + e);
+      } catch (IOException e) {
+        // unable to connect to the region server.
+        errors.reportError(ERROR_CODE.RS_CONNECT_FAILURE, "Table Directory: "
+            + tableDir.getPath().getName()
+            + " Unable to fetch region information. " + e);
       } finally {
         done = true;
         notifyAll();
@@ -1596,7 +2650,7 @@ public class HBaseFsck {
    * Display the full report from fsck. This displays all live and dead region
    * servers, and all known regions.
    */
-  public void displayFullReport() {
+  public void setDisplayFullReport() {
     details = true;
   }
 
@@ -1634,12 +2688,74 @@ public class HBaseFsck {
    * Fix inconsistencies found by fsck. This should try to fix errors (if any)
    * found by fsck utility.
    */
-  public void setFixErrors(boolean shouldFix) {
-    fix = shouldFix;
+  public void setFixAssignments(boolean shouldFix) {
+    fixAssignments = shouldFix;
+  }
+
+  boolean shouldFixAssignments() {
+    return fixAssignments;
+  }
+
+  public void setFixMeta(boolean shouldFix) {
+    fixMeta = shouldFix;
+  }
+
+  boolean shouldFixMeta() {
+    return fixMeta;
+  }
+
+  public void setFixHdfsHoles(boolean shouldFix) {
+    fixHdfsHoles = shouldFix;
+  }
+
+  boolean shouldFixHdfsHoles() {
+    return fixHdfsHoles;
+  }
+
+  public void setFixHdfsOverlaps(boolean shouldFix) {
+    fixHdfsOverlaps = shouldFix;
+  }
+
+  boolean shouldFixHdfsOverlaps() {
+    return fixHdfsOverlaps;
+  }
+
+  public void setFixHdfsOrphans(boolean shouldFix) {
+    fixHdfsOrphans = shouldFix;
+  }
+
+  boolean shouldFixHdfsOrphans() {
+    return fixHdfsOrphans;
+  }
+
+  /**
+   * @param mm maximum number of regions to merge into a single region.
+   */
+  public void setMaxMerge(int mm) {
+    this.maxMerge = mm;
+  }
+
+  public int getMaxMerge() {
+    return maxMerge;
+  }
+
+  /**
+   * Only fix tables specified by the list
+   */
+  boolean shouldFixTable(byte[] table) {
+    if (tablesToFix.size() == 0) {
+      return true;
+    }
+
+    // doing this naively since byte[] equals may not do what we want.
+    for (byte[] t : tablesToFix) {
+      if (Bytes.equals(t, table)) return true;
+    }
+    return false;
   }
 
-  boolean shouldFix() {
-    return fix;
+  void includeTable(byte[] table) {
+    tablesToFix.add(table);
   }
 
   /**
@@ -1652,17 +2768,29 @@ public class HBaseFsck {
   }
 
   protected static void printUsageAndExit() {
-    System.err.println("Usage: fsck [opts] ");
+    System.err.println("Usage: fsck [opts] {only tables}");
     System.err.println(" where [opts] are:");
     System.err.println("   -details Display full report of all regions.");
     System.err.println("   -timelag {timeInSeconds}  Process only regions that " +
                        " have not experienced any metadata updates in the last " +
                        " {{timeInSeconds} seconds.");
-    System.err.println("   -fix Try to fix some of the errors.");
     System.err.println("   -sleepBeforeRerun {timeInSeconds} Sleep this many seconds" +
-                       " before checking if the fix worked if run with -fix");
+        " before checking if the fix worked if run with -fix");
     System.err.println("   -summary Print only summary of the tables and status.");
     System.err.println("   -metaonly Only check the state of ROOT and META tables.");
+
+    System.err.println("  Repair options: (expert features, use with caution!)");
+    System.err.println("   -fix              Try to fix region assignments.  This is for backwards compatiblity");
+    System.err.println("   -fixAssignments   Try to fix region assignments.  Replaces the old -fix");
+    System.err.println("   -fixMeta          Try to fix meta problems.  This assumes HDFS region info is good.");
+    System.err.println("   -fixHdfsHoles     Try to fix region holes in hdfs.");
+    System.err.println("   -fixHdfsOrphans   Try to fix region dirs with no .regioninfo file in hdfs");
+    System.err.println("   -fixHdfsOverlaps  Try to fix region overlaps in hdfs.");
+    System.err.println("   -maxMerge <n>     When fixing region overlaps, allow at most <n> regions to merge. (n=" + DEFAULT_MAX_MERGE +" by default)");
+    System.err.println("");
+    System.err.println("   -repair           Shortcut for -fixAssignments -fixMeta -fixHdfsHoles -fixHdfsOrphans -fixHdfsOverlaps");
+    System.err.println("   -repairHoles      Shortcut for -fixAssignments -fixMeta -fixHdfsHoles -fixHdfsOrphans");
+
     Runtime.getRuntime().exit(-2);
   }
 
@@ -1683,7 +2811,7 @@ public class HBaseFsck {
     for (int i = 0; i < args.length; i++) {
       String cmd = args[i];
       if (cmd.equals("-details")) {
-        fsck.displayFullReport();
+        fsck.setDisplayFullReport();
       } else if (cmd.equals("-timelag")) {
         if (i == args.length - 1) {
           System.err.println("HBaseFsck: -timelag needs a value.");
@@ -1710,21 +2838,60 @@ public class HBaseFsck {
         }
         i++;
       } else if (cmd.equals("-fix")) {
-        fsck.setFixErrors(true);
+        System.err.println("This option is deprecated, please use " +
+          "-fixAssignments instead.");
+        fsck.setFixAssignments(true);
+      } else if (cmd.equals("-fixAssignments")) {
+        fsck.setFixAssignments(true);
+      } else if (cmd.equals("-fixMeta")) {
+        fsck.setFixMeta(true);
+      } else if (cmd.equals("-fixHdfsHoles")) {
+        fsck.setFixHdfsHoles(true);
+      } else if (cmd.equals("-fixHdfsOrphans")) {
+        fsck.setFixHdfsOrphans(true);
+      } else if (cmd.equals("-fixHdfsOverlaps")) {
+        fsck.setFixHdfsOverlaps(true);
+      } else if (cmd.equals("-repair")) {
+        // this attempts to merge overlapping hdfs regions, needs testing
+        // under load
+        fsck.setFixHdfsHoles(true);
+        fsck.setFixHdfsOrphans(true);
+        fsck.setFixMeta(true);
+        fsck.setFixAssignments(true);
+        fsck.setFixHdfsOverlaps(true);
+      } else if (cmd.equals("-repairHoles")) {
+        // this will make all missing hdfs regions available but may lose data
+        fsck.setFixHdfsHoles(true);
+        fsck.setFixHdfsOrphans(false);
+        fsck.setFixMeta(true);
+        fsck.setFixAssignments(true);
+        fsck.setFixHdfsOverlaps(false);
+      } else if (cmd.equals("-maxMerge")) {
+        if (i == args.length - 1) {
+          System.err.println("-maxMerge needs a numeric value argument.");
+          printUsageAndExit();
+        }
+        try {
+          int maxMerge = Integer.parseInt(args[i+1]);
+          fsck.setMaxMerge(maxMerge);
+        } catch (NumberFormatException e) {
+          System.err.println("-maxMerge needs a numeric value argument.");
+          printUsageAndExit();
+        }
+        i++;
       } else if (cmd.equals("-summary")) {
         fsck.setSummary();
       } else if (cmd.equals("-metaonly")) {
         fsck.setCheckMetaOnly();
       } else {
-        String str = "Unknown command line option : " + cmd;
-        LOG.info(str);
-        System.out.println(str);
-        printUsageAndExit();
+        byte[] table = Bytes.toBytes(cmd);
+        fsck.includeTable(table);
+        System.out.println("Allow fixes for table: " + cmd);
       }
     }
     // do the real work of fsck
     fsck.connect();
-    int code = fsck.doWork();
+    int code = fsck.onlineHbck();
     // If we have changed the HBase state it is better to run fsck again
     // to see if we haven't broken something else in the process.
     // We run it only once more because otherwise we can easily fall into
@@ -1737,11 +2904,48 @@ public class HBaseFsck {
         Runtime.getRuntime().exit(code);
       }
       // Just report
-      fsck.setFixErrors(false);
+      fsck.setFixAssignments(false);
+      fsck.setFixMeta(false);
+      fsck.setFixHdfsHoles(false);
+      fsck.setFixHdfsOverlaps(false);
       fsck.errors.resetErrors();
-      code = fsck.doWork();
+      code = fsck.onlineHbck();
     }
 
     Runtime.getRuntime().exit(code);
   }
+
+  /**
+   * ls -r for debugging purposes
+   */
+  void debugLsr(Path p) throws IOException {
+    debugLsr(conf, p);
+  }
+
+  /**
+   * ls -r for debugging purposes
+   */
+  public static void debugLsr(Configuration conf, Path p) throws IOException {
+    if (!LOG.isDebugEnabled()) {
+      return;
+    }
+    FileSystem fs = p.getFileSystem(conf);
+
+    if (!fs.exists(p)) {
+      // nothing
+      return;
+    }
+    System.out.println(p);
+
+    if (fs.isFile(p)) {
+      return;
+    }
+
+    if (fs.getFileStatus(p).isDir()) {
+      FileStatus[] fss= fs.listStatus(p);
+      for (FileStatus status : fss) {
+        debugLsr(conf, status.getPath());
+      }
+    }
+  }
 }
diff --git a/src/main/java/org/apache/hadoop/hbase/util/HBaseFsckRepair.java b/src/main/java/org/apache/hadoop/hbase/util/HBaseFsckRepair.java
index 77f60a0..1cbb812 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/HBaseFsckRepair.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/HBaseFsckRepair.java
@@ -21,40 +21,54 @@ package org.apache.hadoop.hbase.util;
 
 import java.io.IOException;
 import java.util.List;
+import java.util.Map;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.NotServingRegionException;
+import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.ZooKeeperConnectionException;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HConnection;
 import org.apache.hadoop.hbase.client.HConnectionManager;
+import org.apache.hadoop.hbase.client.HConnectionManager.HConnectable;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
+import org.apache.hadoop.hbase.master.AssignmentManager.RegionState;
+import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.wal.HLog;
+import org.apache.hadoop.hbase.zookeeper.ZKAssign;
 import org.apache.zookeeper.KeeperException;
 
+/**
+ * This class contains helper methods that repair parts of hbase's filesystem
+ * contents.
+ */
 public class HBaseFsckRepair {
+  public static final Log LOG = LogFactory.getLog(HBaseFsckRepair.class);
 
   /**
-   * Fix dupe assignment by doing silent closes on each RS hosting the region
+   * Fix multiple assignment by doing silent closes on each RS hosting the region
    * and then force ZK unassigned node to OFFLINE to trigger assignment by
    * master.
-   * @param admin
-   * @param region
-   * @param servers
-   * @throws IOException
-   * @throws KeeperException
-   * @throws InterruptedException
+   *
+   * @param admin HBase admin used to undeploy
+   * @param region Region to undeploy
+   * @param servers list of Servers to undeploy from
    */
-  public static void fixDupeAssignment(HBaseAdmin admin, HRegionInfo region,
+  public static void fixMultiAssignment(HBaseAdmin admin, HRegionInfo region,
       List<ServerName> servers)
   throws IOException, KeeperException, InterruptedException {
-
     HRegionInfo actualRegion = new HRegionInfo(region);
 
     // Close region on the servers silently
     for(ServerName server : servers) {
-      closeRegionSilentlyAndWait(admin.getConfiguration(), server, actualRegion);
+      closeRegionSilentlyAndWait(admin, server, actualRegion);
     }
 
     // Force ZK node to OFFLINE so master assigns
@@ -63,58 +77,133 @@ public class HBaseFsckRepair {
 
   /**
    * Fix unassigned by creating/transition the unassigned ZK node for this
-   * region to OFFLINE state with a special flag to tell the master that this
-   * is a forced operation by HBCK.
-   * @param admin
+   * region to OFFLINE state with a special flag to tell the master that this is
+   * a forced operation by HBCK.
+   *
+   * This assumes that info is in META.
+   *
+   * @param conf
    * @param region
    * @throws IOException
    * @throws KeeperException
    */
   public static void fixUnassigned(HBaseAdmin admin, HRegionInfo region)
-  throws IOException, KeeperException {
+      throws IOException, KeeperException {
     HRegionInfo actualRegion = new HRegionInfo(region);
 
     // Force ZK node to OFFLINE so master assigns
     forceOfflineInZK(admin, actualRegion);
   }
 
+  /**
+   * In 0.90, this forces an HRI offline by setting the RegionTransitionData
+   * in ZK to have HBCK_CODE_NAME as the server.  This is a special case in
+   * the AssignmentManager that attempts an assign call by the master.
+   *
+   * @see org.apache.hadoop.hbase.master.AssignementManager#handleHBCK
+   *
+   * This doesn't seem to work properly in the updated version of 0.92+'s hbck
+   * so we use assign to force the region into transition.  This has the
+   * side-effect of requiring a HRegionInfo that considers regionId (timestamp)
+   * in comparators that is addressed by HBASE-5563.
+   */
   private static void forceOfflineInZK(HBaseAdmin admin, final HRegionInfo region)
   throws ZooKeeperConnectionException, KeeperException, IOException {
     admin.assign(region.getRegionName());
   }
 
-  private static void closeRegionSilentlyAndWait(Configuration conf,
-      ServerName server, HRegionInfo region) throws IOException,
-      InterruptedException {
-    HConnection connection = HConnectionManager.getConnection(conf);
-    boolean success = false;
+  /*
+   * Should we check all assignments or just not in RIT?
+   */
+  public static void waitUntilAssigned(HBaseAdmin admin,
+      HRegionInfo region) throws IOException, InterruptedException {
+    HConnection connection = admin.getConnection();
+
     try {
-      HRegionInterface rs =
-        connection.getHRegionConnection(server.getHostname(), server.getPort());
-      rs.closeRegion(region, false);
-      long timeout = conf.getLong("hbase.hbck.close.timeout", 120000);
+      long timeout = admin.getConfiguration().getLong("hbase.hbck.assign.timeout", 120000);
       long expiration = timeout + System.currentTimeMillis();
       while (System.currentTimeMillis() < expiration) {
         try {
-          HRegionInfo rsRegion = rs.getRegionInfo(region.getRegionName());
-          if (rsRegion == null)
-            throw new NotServingRegionException();
-        } catch (Exception e) {
-          success = true;
-          return;
+          Map<String, RegionState> rits=
+            admin.getClusterStatus().getRegionsInTransition();
+
+          if (rits.keySet() != null && !rits.keySet().contains(region.getEncodedName())) {
+            // yay! no longer RIT
+            return;
+          }
+          // still in rit
+          LOG.info("Region still in transition, waiting for "
+              + "it to become assigned: " + region);
+        } catch (IOException e) {
+          LOG.warn("Exception when waiting for region to become assigned,"
+              + " retrying", e);
         }
         Thread.sleep(1000);
       }
-      throw new IOException("Region " + region + " failed to close within"
-          + " timeout " + timeout);
+      throw new IOException("Region " + region + " failed to move out of " +
+          "transition within timeout " + timeout + "ms");
     } finally {
       try {
         connection.close();
       } catch (IOException ioe) {
-        if (success) {
-          throw ioe;
-        }
+        throw ioe;
       }
     }
   }
+
+  /**
+   * Contacts a region server and waits up to hbase.hbck.close.timeout ms
+   * (default 120s) to close the region.  This bypasses the active hmaster.
+   */
+  public static void closeRegionSilentlyAndWait(HBaseAdmin admin,
+      ServerName server, HRegionInfo region) throws IOException, InterruptedException {
+    HConnection connection = admin.getConnection();
+    HRegionInterface rs = connection.getHRegionConnection(server.getHostname(),
+        server.getPort());
+    rs.closeRegion(region, false);
+    long timeout = admin.getConfiguration()
+      .getLong("hbase.hbck.close.timeout", 120000);
+    long expiration = timeout + System.currentTimeMillis();
+    while (System.currentTimeMillis() < expiration) {
+      try {
+        HRegionInfo rsRegion = rs.getRegionInfo(region.getRegionName());
+        if (rsRegion == null)
+          return;
+      } catch (IOException ioe) {
+        return;
+      }
+      Thread.sleep(1000);
+    }
+    throw new IOException("Region " + region + " failed to close within"
+        + " timeout " + timeout);
+  }
+
+  /**
+   * Puts the specified HRegionInfo into META.
+   */
+  public static void fixMetaHoleOnline(Configuration conf,
+      HRegionInfo hri) throws IOException {
+    Put p = new Put(hri.getRegionName());
+    p.add(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER,
+        Writables.getBytes(hri));
+    HTable meta = new HTable(conf, HConstants.META_TABLE_NAME);
+    meta.put(p);
+    meta.close();
+  }
+
+  /**
+   * Creates, flushes, and closes a new region.
+   */
+  public static HRegion createHDFSRegionDir(Configuration conf,
+      HRegionInfo hri, HTableDescriptor htd) throws IOException {
+    // Create HRegion
+    Path root = FSUtils.getRootDir(conf);
+    HRegion region = HRegion.createHRegion(hri, root, conf, htd);
+    HLog hlog = region.getLog();
+
+    // Close the new region to flush to disk. Close log file too.
+    region.close();
+    hlog.closeAndDelete();
+    return region;
+  }
 }
diff --git a/src/main/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRepair.java b/src/main/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRepair.java
index 29e8bb2..d73821a 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRepair.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRepair.java
@@ -40,7 +40,6 @@ import org.apache.hadoop.io.MultipleIOException;
  */
 public class OfflineMetaRepair {
   private static final Log LOG = LogFactory.getLog(HBaseFsck.class.getName());
-  HBaseFsck fsck;
 
   protected static void printUsageAndExit() {
     System.err.println("Usage: OfflineMetaRepair [opts] ");
@@ -48,6 +47,8 @@ public class OfflineMetaRepair {
     System.err
         .println("   -details          Display full report of all regions.");
     System.err.println("   -base <hdfs://>   Base Hbase Data directory");
+    System.err.println("   -fix              Auto fix as many problems as possible");
+    System.err.println("   -fixHoles         Auto fix as region holes");
     Runtime.getRuntime().exit(-2);
   }
 
@@ -63,18 +64,24 @@ public class OfflineMetaRepair {
     Configuration conf = HBaseConfiguration.create();
     conf.set("fs.defaultFS", conf.get(HConstants.HBASE_DIR));
     HBaseFsck fsck = new HBaseFsck(conf);
+    boolean fixHoles = false;
 
     // Process command-line args.
     for (int i = 0; i < args.length; i++) {
       String cmd = args[i];
       if (cmd.equals("-details")) {
-        fsck.displayFullReport();
+        fsck.setDisplayFullReport();
       } else if (cmd.equals("-base")) {
         // update hbase root dir to user-specified base
         i++;
         String path = args[i];
         conf.set(HConstants.HBASE_DIR, path);
         conf.set("fs.defaultFS", conf.get(HConstants.HBASE_DIR));
+      } else if (cmd.equals("-fixHoles")) {
+        fixHoles = true;
+      } else if (cmd.equals("-fix")) {
+        // make all fix options true
+        fixHoles = true;
       } else {
         String str = "Unknown command line option : " + cmd;
         LOG.info(str);
@@ -87,7 +94,7 @@ public class OfflineMetaRepair {
     // threads cleanly, so we do a System.exit.
     boolean success = false;
     try {
-      success = fsck.rebuildMeta();
+      success = fsck.rebuildMeta(fixHoles);
     } catch (MultipleIOException mioes) {
       for (IOException ioe : mioes.getExceptions()) {
         LOG.error("Bailed out due to:", ioe);
diff --git a/src/main/java/org/apache/hadoop/hbase/util/hbck/TableIntegrityErrorHandler.java b/src/main/java/org/apache/hadoop/hbase/util/hbck/TableIntegrityErrorHandler.java
new file mode 100644
index 0000000..2d49d01
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/util/hbck/TableIntegrityErrorHandler.java
@@ -0,0 +1,92 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.util.hbck;
+
+import java.io.IOException;
+import java.util.Collection;
+
+import org.apache.hadoop.hbase.util.HBaseFsck.HbckInfo;
+import org.apache.hadoop.hbase.util.HBaseFsck.TableInfo;
+
+/**
+ * This interface provides callbacks for handling particular table integrity
+ * invariant violations.  This could probably be boiled down to handling holes
+ * and handling overlaps but currently preserves the older more specific error
+ * condition codes.
+ */
+public interface TableIntegrityErrorHandler {
+
+  TableInfo getTableInfo();
+
+  /**
+   * Set the TableInfo used by all HRegionInfos fabricated by other callbacks
+   */
+  void setTableInfo(TableInfo ti);
+
+  /**
+   * Callback for handling case where a Table has a first region that does not
+   * have an empty start key.
+   *
+   * @param hi An HbckInfo of the second region in a table.  This should have
+   *    a non-empty startkey, and can be used to fabricate a first region that
+   *    has an empty start key.
+   */
+  void handleRegionStartKeyNotEmpty(HbckInfo hi) throws IOException;
+
+  /**
+   * Callback for handling a region that has the same start and end key.
+   *
+   * @param hi An HbckInfo for a degenerate key.
+   */
+  void handleDegenerateRegion(HbckInfo hi) throws IOException;
+
+  /**
+   * Callback for handling two regions that have the same start key.  This is
+   * a specific case of a region overlap.
+   * @param hi1 one of the overlapping HbckInfo 
+   * @param hi2 the other overlapping HbckInfo
+   */
+  void handleDuplicateStartKeys(HbckInfo hi1, HbckInfo hi2) throws IOException;
+
+  /**
+   * Callback for handling two reigons that overlap in some arbitrary way.
+   * This is a specific case of region overlap, and called for each possible
+   * pair. If two regions have the same start key, the handleDuplicateStartKeys
+   * method is called.
+   * @param hi1 one of the overlapping HbckInfo
+   * @param hi2 the other overlapping HbckInfo
+   */
+  void handleOverlapInRegionChain(HbckInfo hi1, HbckInfo hi2)
+      throws IOException;
+
+  /**
+   * Callback for handling a region hole between two keys.
+   * @param holeStartKey key at the beginning of the region hole
+   * @param holeEndKey key at the end of the region hole
+   
+   */
+  void handleHoleInRegionChain(byte[] holeStartKey, byte[] holeEndKey)
+      throws IOException;
+
+  /**
+   * Callback for handling an group of regions that overlap.
+   * @param overlap Collection of overlapping regions.
+   */
+  void handleOverlapGroup(Collection<HbckInfo> overlap) throws IOException;
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/util/hbck/TableIntegrityErrorHandlerImpl.java b/src/main/java/org/apache/hadoop/hbase/util/hbck/TableIntegrityErrorHandlerImpl.java
new file mode 100644
index 0000000..2c34da8
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/util/hbck/TableIntegrityErrorHandlerImpl.java
@@ -0,0 +1,96 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.util.hbck;
+
+import java.io.IOException;
+import java.util.Collection;
+
+import org.apache.hadoop.hbase.util.HBaseFsck.HbckInfo;
+import org.apache.hadoop.hbase.util.HBaseFsck.TableInfo;
+
+/**
+ * Simple implementation of TableIntegrityErrorHandler. Can be used as a base
+ * class.
+ */
+abstract public class TableIntegrityErrorHandlerImpl implements
+    TableIntegrityErrorHandler {
+  TableInfo ti;
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public TableInfo getTableInfo() {
+    return ti;
+  }
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public void setTableInfo(TableInfo ti2) {
+    this.ti = ti2;
+  }
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public void handleRegionStartKeyNotEmpty(HbckInfo hi) throws IOException {
+  }
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public void handleDegenerateRegion(HbckInfo hi) throws IOException {
+  }
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public void handleDuplicateStartKeys(HbckInfo hi1, HbckInfo hi2)
+      throws IOException {
+  }
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public void handleOverlapInRegionChain(HbckInfo hi1, HbckInfo hi2)
+      throws IOException {
+  }
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public void handleHoleInRegionChain(byte[] holeStart, byte[] holeEnd)
+      throws IOException {
+  }
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public void handleOverlapGroup(Collection<HbckInfo> overlap)
+      throws IOException {
+  }
+
+}
diff --git a/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java b/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
index 1f19ddf..c05c2d9 100644
--- a/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
+++ b/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
@@ -1111,11 +1111,16 @@ public class HBaseTestingUtility {
     List<byte[]> rows = new ArrayList<byte[]>();
     ResultScanner s = t.getScanner(new Scan());
     for (Result result : s) {
-      HRegionInfo info = Writables.getHRegionInfo(
-          result.getValue(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER));
+      byte[] val = result.getValue(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);
+      if (val == null) {
+        LOG.error("No region info for row " + Bytes.toString(result.getRow()));
+        // TODO figure out what to do for this new hosed case.
+        continue;
+      }
+      HRegionInfo info = Writables.getHRegionInfo(val);
       if (Bytes.compareTo(info.getTableName(), tableName) == 0) {
         LOG.info("getMetaTableRows: row -> " +
-            Bytes.toStringBinary(result.getRow()));
+            Bytes.toStringBinary(result.getRow()) + info);
         rows.add(result.getRow());
       }
     }
diff --git a/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java b/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java
index 7e04eab..c4ca32c 100644
--- a/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java
+++ b/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java
@@ -23,8 +23,12 @@ import static org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.assertErrors;
 import static org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.assertNoErrors;
 import static org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.doFsck;
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.fail;
 
 import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
@@ -32,22 +36,26 @@ import java.util.Map.Entry;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.ClusterStatus;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.HServerAddress;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.client.HConnection;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.ResultScanner;
 import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.ipc.HRegionInterface;
+import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.util.HBaseFsck.ErrorReporter.ERROR_CODE;
 import org.apache.zookeeper.KeeperException;
 import org.junit.AfterClass;
@@ -58,16 +66,20 @@ import org.junit.Test;
  * This tests HBaseFsck's ability to detect reasons for inconsistent tables.
  */
 public class TestHBaseFsck {
-  final Log LOG = LogFactory.getLog(getClass());
+  final static Log LOG = LogFactory.getLog(TestHBaseFsck.class);
   private final static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
   private final static Configuration conf = TEST_UTIL.getConfiguration();
   private final static byte[] FAM = Bytes.toBytes("fam");
 
   // for the instance, reset every test run
   private HTable tbl;
-  private final static byte[][] splits= new byte[][] { Bytes.toBytes("A"), 
+  private final static byte[][] SPLITS = new byte[][] { Bytes.toBytes("A"),
     Bytes.toBytes("B"), Bytes.toBytes("C") };
-  
+  // one row per region.
+  private final static byte[][] ROWKEYS= new byte[][] {
+    Bytes.toBytes("00"), Bytes.toBytes("50"), Bytes.toBytes("A0"), Bytes.toBytes("A5"),
+    Bytes.toBytes("B0"), Bytes.toBytes("B5"), Bytes.toBytes("C0"), Bytes.toBytes("C5") };
+
   @BeforeClass
   public static void setUpBeforeClass() throws Exception {
     TEST_UTIL.getConfiguration().setBoolean("hbase.master.distributed.log.splitting", false);
@@ -121,8 +133,8 @@ public class TestHBaseFsck {
     assertErrors(doFsck(conf, true), new ERROR_CODE[]{
         ERROR_CODE.SERVER_DOES_NOT_MATCH_META});
 
-    // fixing assignements require opening regions is not synchronous.  To make
-    // the test pass consistentyl so for now we bake in some sleep to let it
+    // fixing assignments require opening regions is not synchronous.  To make
+    // the test pass consistently so for now we bake in some sleep to let it
     // finish.  1s seems sufficient.
     Thread.sleep(1000);
 
@@ -133,6 +145,9 @@ public class TestHBaseFsck {
     new HTable(conf, Bytes.toBytes(table)).getScanner(new Scan());
   }
 
+  /**
+   * Create a new region in META.
+   */
   private HRegionInfo createRegion(Configuration conf, final HTableDescriptor
       htd, byte[] startKey, byte[] endKey)
       throws IOException {
@@ -145,47 +160,102 @@ public class TestHBaseFsck {
     return hri;
   }
 
-  public void dumpMeta(HTableDescriptor htd) throws IOException {
-    List<byte[]> metaRows = TEST_UTIL.getMetaTableRows(htd.getName());
+  /**
+   * Debugging method to dump the contents of meta.
+   */
+  private void dumpMeta(byte[] tableName) throws IOException {
+    List<byte[]> metaRows = TEST_UTIL.getMetaTableRows(tableName);
     for (byte[] row : metaRows) {
       LOG.info(Bytes.toString(row));
     }
   }
 
-  private void deleteRegion(Configuration conf, final HTableDescriptor htd, 
-      byte[] startKey, byte[] endKey) throws IOException {
-
-    LOG.info("Before delete:");
-    dumpMeta(htd);
+  /**
+   * This method is used to undeploy a region -- close it and attempt to
+   * remove its state from the Master.
+   */
+  private void undeployRegion(HBaseAdmin admin, ServerName sn,
+      HRegionInfo hri) throws IOException, InterruptedException {
+    try {
+      HBaseFsckRepair.closeRegionSilentlyAndWait(admin, sn, hri);
+      admin.getMaster().offline(hri.getRegionName());
+    } catch (IOException ioe) {
+      LOG.warn("Got exception when attempting to offline region "
+          + Bytes.toString(hri.getRegionName()), ioe);
+    }
+  }
+  /**
+   * Delete a region from assignments, meta, or completely from hdfs.
+   * @param unassign if true unassign region if assigned
+   * @param metaRow  if true remove region's row from META
+   * @param hdfs if true remove region's dir in HDFS
+   */
+  private void deleteRegion(Configuration conf, final HTableDescriptor htd,
+      byte[] startKey, byte[] endKey, boolean unassign, boolean metaRow,
+      boolean hdfs) throws IOException, InterruptedException {
+    deleteRegion(conf, htd, startKey, endKey, unassign, metaRow, hdfs, false);
+  }
 
-    Map<HRegionInfo, HServerAddress> hris = tbl.getRegionsInfo();
-    for (Entry<HRegionInfo, HServerAddress> e: hris.entrySet()) {
+  /**
+   * Delete a region from assignments, meta, or completely from hdfs.
+   * @param unassign if true unassign region if assigned
+   * @param metaRow  if true remove region's row from META
+   * @param hdfs if true remove region's dir in HDFS
+   * @param regionInfoOnly if true remove a region dir's .regioninfo file
+   */
+  private void deleteRegion(Configuration conf, final HTableDescriptor htd,
+      byte[] startKey, byte[] endKey, boolean unassign, boolean metaRow,
+      boolean hdfs, boolean regionInfoOnly) throws IOException, InterruptedException {
+    LOG.info("** Before delete:");
+    dumpMeta(htd.getName());
+
+    Map<HRegionInfo, ServerName> hris = tbl.getRegionLocations();
+    for (Entry<HRegionInfo, ServerName> e: hris.entrySet()) {
       HRegionInfo hri = e.getKey();
-      HServerAddress hsa = e.getValue();
-      if (Bytes.compareTo(hri.getStartKey(), startKey) == 0 
+      ServerName hsa = e.getValue();
+      if (Bytes.compareTo(hri.getStartKey(), startKey) == 0
           && Bytes.compareTo(hri.getEndKey(), endKey) == 0) {
 
         LOG.info("RegionName: " +hri.getRegionNameAsString());
         byte[] deleteRow = hri.getRegionName();
-        TEST_UTIL.getHBaseAdmin().unassign(deleteRow, true);
 
-        LOG.info("deleting hdfs data: " + hri.toString() + hsa.toString());
-        Path rootDir = new Path(conf.get(HConstants.HBASE_DIR));
-        FileSystem fs = rootDir.getFileSystem(conf);
-        Path p = new Path(rootDir + "/" + htd.getNameAsString(), hri.getEncodedName());
-        fs.delete(p, true);
+        if (unassign) {
+          LOG.info("Undeploying region " + hri + " from server " + hsa);
+          undeployRegion(new HBaseAdmin(conf), hsa, hri);
+        }
+
+        if (regionInfoOnly) {
+          LOG.info("deleting hdfs .regioninfo data: " + hri.toString() + hsa.toString());
+          Path rootDir = new Path(conf.get(HConstants.HBASE_DIR));
+          FileSystem fs = rootDir.getFileSystem(conf);
+          Path p = new Path(rootDir + "/" + htd.getNameAsString(), hri.getEncodedName());
+          Path hriPath = new Path(p, HRegion.REGIONINFO_FILE);
+          fs.delete(hriPath, true);
+        }
+
+        if (hdfs) {
+          LOG.info("deleting hdfs data: " + hri.toString() + hsa.toString());
+          Path rootDir = new Path(conf.get(HConstants.HBASE_DIR));
+          FileSystem fs = rootDir.getFileSystem(conf);
+          Path p = new Path(rootDir + "/" + htd.getNameAsString(), hri.getEncodedName());
+          HBaseFsck.debugLsr(conf, p);
+          boolean success = fs.delete(p, true);
+          LOG.info("Deleted " + p + " sucessfully? " + success);
+          HBaseFsck.debugLsr(conf, p);
+        }
 
-        HTable meta = new HTable(conf, HConstants.META_TABLE_NAME);
-        Delete delete = new Delete(deleteRow);
-        meta.delete(delete);
+        if (metaRow) {
+          HTable meta = new HTable(conf, HConstants.META_TABLE_NAME);
+          Delete delete = new Delete(deleteRow);
+          meta.delete(delete);
+        }
       }
       LOG.info(hri.toString() + hsa.toString());
     }
 
     TEST_UTIL.getMetaTableRows(htd.getName());
-    LOG.info("After delete:");
-    dumpMeta(htd);
-
+    LOG.info("*** After delete:");
+    dumpMeta(htd.getName());
   }
 
   /**
@@ -199,11 +269,32 @@ public class TestHBaseFsck {
     HTableDescriptor desc = new HTableDescriptor(tablename);
     HColumnDescriptor hcd = new HColumnDescriptor(Bytes.toString(FAM));
     desc.addFamily(hcd); // If a table has no CF's it doesn't get checked
-    TEST_UTIL.getHBaseAdmin().createTable(desc, splits);
+    TEST_UTIL.getHBaseAdmin().createTable(desc, SPLITS);
     tbl = new HTable(TEST_UTIL.getConfiguration(), tablename);
+
+    List<Put> puts = new ArrayList<Put>();
+    for (byte[] row : ROWKEYS) {
+      Put p = new Put(row);
+      p.add(FAM, Bytes.toBytes("val"), row);
+      puts.add(p);
+    }
+    tbl.put(puts);
+    tbl.flushCommits();
     return tbl;
   }
 
+  /**
+   * Counts the number of row to verify data loss or non-dataloss.
+   */
+  int countRows() throws IOException {
+     Scan s = new Scan();
+     ResultScanner rs = tbl.getScanner(s);
+     int i = 0;
+     while(rs.next() !=null) {
+       i++;
+     }
+     return i;
+  }
 
   /**
    * delete table in preparation for next test
@@ -212,14 +303,21 @@ public class TestHBaseFsck {
    * @throws IOException
    */
   void deleteTable(String tablename) throws IOException {
-    HBaseAdmin admin = TEST_UTIL.getHBaseAdmin();
+    HBaseAdmin admin = new HBaseAdmin(conf);
+    admin.getConnection().clearRegionCache();
     byte[] tbytes = Bytes.toBytes(tablename);
-    admin.disableTable(tbytes);
+    admin.disableTableAsync(tbytes);
+    while (!admin.isTableDisabled(tbytes)) {
+      try {
+        Thread.sleep(250);
+      } catch (InterruptedException e) {
+        e.printStackTrace();
+        fail("Interrupted when trying to disable table " + tablename);
+      }
+    }
     admin.deleteTable(tbytes);
   }
 
-
-  
   /**
    * This creates a clean table and confirms that the table is clean.
    */
@@ -232,18 +330,21 @@ public class TestHBaseFsck {
       assertNoErrors(hbck);
 
       setupTable(table);
-      
+      assertEquals(ROWKEYS.length, countRows());
+
       // We created 1 table, should be fine
       hbck = doFsck(conf, false);
       assertNoErrors(hbck);
       assertEquals(0, hbck.getOverlapGroups(table).size());
+      assertEquals(ROWKEYS.length, countRows());
     } finally {
       deleteTable(table);
     }
   }
 
   /**
-   * This creates a bad table with regions that have a duplicate start key
+   * This create and fixes a bad table with regions that have a duplicate
+   * start key
    */
   @Test
   public void testDupeStartKey() throws Exception {
@@ -251,6 +352,7 @@ public class TestHBaseFsck {
     try {
       setupTable(table);
       assertNoErrors(doFsck(conf, false));
+      assertEquals(ROWKEYS.length, countRows());
 
       // Now let's mess it up, by adding a region with a duplicate startkey
       HRegionInfo hriDupe = createRegion(conf, tbl.getTableDescriptor(),
@@ -263,13 +365,112 @@ public class TestHBaseFsck {
       assertErrors(hbck, new ERROR_CODE[] { ERROR_CODE.DUPE_STARTKEYS,
             ERROR_CODE.DUPE_STARTKEYS});
       assertEquals(2, hbck.getOverlapGroups(table).size());
+      assertEquals(ROWKEYS.length, countRows()); // seems like the "bigger" region won.
+
+      // fix the degenerate region.
+      doFsck(conf,true);
+
+      // check that the degenerate region is gone and no data loss
+      HBaseFsck hbck2 = doFsck(conf,false);
+      assertNoErrors(hbck2);
+      assertEquals(0, hbck2.getOverlapGroups(table).size());
+      assertEquals(ROWKEYS.length, countRows());
     } finally {
       deleteTable(table);
     }
   }
-  
+
+  /**
+   * Get region info from local cluster.
+   */
+  Map<ServerName, List<String>> getDeployedHRIs(HBaseAdmin admin)
+    throws IOException {
+    ClusterStatus status = admin.getMaster().getClusterStatus();
+    Collection<ServerName> regionServers = status.getServers();
+    Map<ServerName, List<String>> mm =
+        new HashMap<ServerName, List<String>>();
+    HConnection connection = admin.getConnection();
+    for (ServerName hsi : regionServers) {
+      HRegionInterface server =
+        connection.getHRegionConnection(hsi.getHostname(), hsi.getPort());
+
+      // list all online regions from this region server
+      List<HRegionInfo> regions = server.getOnlineRegions();
+      List<String> regionNames = new ArrayList<String>();
+      for (HRegionInfo hri : regions) {
+        regionNames.add(hri.getRegionNameAsString());
+      }
+      mm.put(hsi, regionNames);
+    }
+    return mm;
+  }
+
+  /**
+   * Returns the HSI a region info is on.
+   */
+  ServerName findDeployedHSI(Map<ServerName, List<String>> mm, HRegionInfo hri) {
+    for (Map.Entry<ServerName,List <String>> e : mm.entrySet()) {
+      if (e.getValue().contains(hri.getRegionNameAsString())) {
+        return e.getKey();
+      }
+    }
+    return null;
+  }
+
   /**
-   * This creates a bad table with regions that has startkey == endkey
+   * This create and fixes a bad table with regions that have a duplicate
+   * start key
+   */
+  @Test
+  public void testDupeRegion() throws Exception {
+    String table = "tableDupeRegion";
+    try {
+      setupTable(table);
+      assertNoErrors(doFsck(conf, false));
+      assertEquals(ROWKEYS.length, countRows());
+
+      // Now let's mess it up, by adding a region with a duplicate startkey
+      HRegionInfo hriDupe = createRegion(conf, tbl.getTableDescriptor(),
+          Bytes.toBytes("A"), Bytes.toBytes("B"));
+
+      TEST_UTIL.getHBaseCluster().getMaster().assignRegion(hriDupe);
+      TEST_UTIL.getHBaseCluster().getMaster().getAssignmentManager()
+          .waitForAssignment(hriDupe);
+
+      // Yikes! The assignment manager can't tell between diff between two
+      // different regions with the same start/endkeys since it doesn't
+      // differentiate on ts/regionId!  We actually need to recheck
+      // deployments!
+      HBaseAdmin admin = TEST_UTIL.getHBaseAdmin();
+      ServerName hsi;
+      while ( (hsi = findDeployedHSI(getDeployedHRIs(admin), hriDupe)) == null) {
+        Thread.sleep(250);
+      }
+
+      LOG.debug("Finished assignment of dupe region");
+
+      // TODO why is dupe region different from dupe start keys?
+      HBaseFsck hbck = doFsck(conf, false);
+      assertErrors(hbck, new ERROR_CODE[] { ERROR_CODE.DUPE_STARTKEYS,
+            ERROR_CODE.DUPE_STARTKEYS});
+      assertEquals(2, hbck.getOverlapGroups(table).size());
+      assertEquals(ROWKEYS.length, countRows()); // seems like the "bigger" region won.
+
+      // fix the degenerate region.
+      doFsck(conf,true);
+
+      // check that the degenerate region is gone and no data loss
+      HBaseFsck hbck2 = doFsck(conf,false);
+      assertNoErrors(hbck2);
+      assertEquals(0, hbck2.getOverlapGroups(table).size());
+      assertEquals(ROWKEYS.length, countRows());
+    } finally {
+      deleteTable(table);
+    }
+  }
+
+  /**
+   * This creates and fixes a bad table with regions that has startkey == endkey
    */
   @Test
   public void testDegenerateRegions() throws Exception {
@@ -277,6 +478,7 @@ public class TestHBaseFsck {
     try {
       setupTable(table);
       assertNoErrors(doFsck(conf,false));
+      assertEquals(ROWKEYS.length, countRows());
 
       // Now let's mess it up, by adding a region with a duplicate startkey
       HRegionInfo hriDupe = createRegion(conf, tbl.getTableDescriptor(),
@@ -289,19 +491,111 @@ public class TestHBaseFsck {
       assertErrors(hbck, new ERROR_CODE[] { ERROR_CODE.DEGENERATE_REGION,
           ERROR_CODE.DUPE_STARTKEYS, ERROR_CODE.DUPE_STARTKEYS});
       assertEquals(2, hbck.getOverlapGroups(table).size());
+      assertEquals(ROWKEYS.length, countRows());
+
+      // fix the degenerate region.
+      doFsck(conf,true);
+
+      // check that the degenerate region is gone and no data loss
+      HBaseFsck hbck2 = doFsck(conf,false);
+      assertNoErrors(hbck2);
+      assertEquals(0, hbck2.getOverlapGroups(table).size());
+      assertEquals(ROWKEYS.length, countRows());
     } finally {
       deleteTable(table);
     }
   }
 
+
   /**
-   * This creates a bad table where a start key contained in another region.
+   * This creates and fixes a bad table where a region is completely contained
+   * by another region.
+   */
+  @Test
+  public void testContainedRegionOverlap() throws Exception {
+    String table = "tableContainedRegionOverlap";
+    try {
+      setupTable(table);
+      assertEquals(ROWKEYS.length, countRows());
+
+      // Mess it up by creating an overlap in the metadata
+      HRegionInfo hriOverlap = createRegion(conf, tbl.getTableDescriptor(),
+          Bytes.toBytes("A2"), Bytes.toBytes("B"));
+      TEST_UTIL.getHBaseCluster().getMaster().assignRegion(hriOverlap);
+      TEST_UTIL.getHBaseCluster().getMaster().getAssignmentManager()
+          .waitForAssignment(hriOverlap);
+
+      HBaseFsck hbck = doFsck(conf, false);
+      assertErrors(hbck, new ERROR_CODE[] {
+          ERROR_CODE.OVERLAP_IN_REGION_CHAIN });
+      assertEquals(2, hbck.getOverlapGroups(table).size());
+      assertEquals(ROWKEYS.length, countRows());
+
+      // fix the problem.
+      doFsck(conf, true);
+
+      // verify that overlaps are fixed
+      HBaseFsck hbck2 = doFsck(conf,false);
+      assertNoErrors(hbck2);
+      assertEquals(0, hbck2.getOverlapGroups(table).size());
+      assertEquals(ROWKEYS.length, countRows());
+    } finally {
+       deleteTable(table);
+    }
+  }
+
+  /**
+   * This creates and fixes a bad table where a region is completely contained
+   * by another region, and there is a hole (sort of like a bad split)
+   */
+  @Test
+  public void testOverlapAndOrphan() throws Exception {
+    String table = "tableOverlapAndOrphan";
+    try {
+      setupTable(table);
+      assertEquals(ROWKEYS.length, countRows());
+
+      // Mess it up by creating an overlap in the metadata
+      TEST_UTIL.getHBaseAdmin().disableTable(table);
+      deleteRegion(conf, tbl.getTableDescriptor(), Bytes.toBytes("A"),
+          Bytes.toBytes("B"), true, true, false, true);
+      TEST_UTIL.getHBaseAdmin().enableTable(table);
+
+      HRegionInfo hriOverlap = createRegion(conf, tbl.getTableDescriptor(),
+          Bytes.toBytes("A2"), Bytes.toBytes("B"));
+      TEST_UTIL.getHBaseCluster().getMaster().assignRegion(hriOverlap);
+      TEST_UTIL.getHBaseCluster().getMaster().getAssignmentManager()
+          .waitForAssignment(hriOverlap);
+
+      HBaseFsck hbck = doFsck(conf, false);
+      assertErrors(hbck, new ERROR_CODE[] {
+          ERROR_CODE.ORPHAN_HDFS_REGION, ERROR_CODE.NOT_IN_META_OR_DEPLOYED,
+          ERROR_CODE.HOLE_IN_REGION_CHAIN});
+
+      // fix the problem.
+      doFsck(conf, true);
+
+      // verify that overlaps are fixed
+      HBaseFsck hbck2 = doFsck(conf,false);
+      assertNoErrors(hbck2);
+      assertEquals(0, hbck2.getOverlapGroups(table).size());
+      assertEquals(ROWKEYS.length, countRows());
+    } finally {
+       deleteTable(table);
+    }
+  }
+
+  /**
+   * This creates and fixes a bad table where a region overlaps two regions --
+   * a start key contained in another region and its end key is contained in
+   * yet another region.
    */
   @Test
   public void testCoveredStartKey() throws Exception {
     String table = "tableCoveredStartKey";
     try {
       setupTable(table);
+      assertEquals(ROWKEYS.length, countRows());
 
       // Mess it up by creating an overlap in the metadata
       HRegionInfo hriOverlap = createRegion(conf, tbl.getTableDescriptor(),
@@ -315,37 +609,236 @@ public class TestHBaseFsck {
           ERROR_CODE.OVERLAP_IN_REGION_CHAIN,
           ERROR_CODE.OVERLAP_IN_REGION_CHAIN });
       assertEquals(3, hbck.getOverlapGroups(table).size());
+      assertEquals(ROWKEYS.length, countRows());
+
+      // fix the problem.
+      doFsck(conf, true);
+
+      // verify that overlaps are fixed
+      HBaseFsck hbck2 = doFsck(conf, false);
+      assertErrors(hbck2, new ERROR_CODE[0]);
+      assertEquals(0, hbck2.getOverlapGroups(table).size());
+      assertEquals(ROWKEYS.length, countRows());
     } finally {
       deleteTable(table);
     }
   }
 
   /**
-   * This creates a bad table with a hole in meta.
+   * This creates and fixes a bad table with a missing region -- hole in meta
+   * and data missing in the fs.
    */
   @Test
-  public void testMetaHole() throws Exception {
-    String table = "tableMetaHole";
+  public void testRegionHole() throws Exception {
+    String table = "tableRegionHole";
     try {
       setupTable(table);
+      assertEquals(ROWKEYS.length, countRows());
+
+      // Mess it up by leaving a hole in the assignment, meta, and hdfs data
+      TEST_UTIL.getHBaseAdmin().disableTable(table);
+      deleteRegion(conf, tbl.getTableDescriptor(), Bytes.toBytes("B"),
+          Bytes.toBytes("C"), true, true, true);
+      TEST_UTIL.getHBaseAdmin().enableTable(table);
+
+      HBaseFsck hbck = doFsck(conf, false);
+      assertErrors(hbck, new ERROR_CODE[] {
+          ERROR_CODE.HOLE_IN_REGION_CHAIN});
+      // holes are separate from overlap groups
+      assertEquals(0, hbck.getOverlapGroups(table).size());
+
+      // fix hole
+      doFsck(conf, true);
+
+      // check that hole fixed
+      assertNoErrors(doFsck(conf,false));
+      assertEquals(ROWKEYS.length - 2 , countRows()); // lost a region so lost a row
+    } finally {
+      deleteTable(table);
+    }
+  }
+
+  /**
+   * This creates and fixes a bad table with a missing region -- hole in meta
+   * and data present but .regioinfino missing (an orphan hdfs region)in the fs.
+   */
+  @Test
+  public void testHDFSRegioninfoMissing() throws Exception {
+    String table = "tableHDFSRegioininfoMissing";
+    try {
+      setupTable(table);
+      assertEquals(ROWKEYS.length, countRows());
 
       // Mess it up by leaving a hole in the meta data
-      HRegionInfo hriHole = createRegion(conf, tbl.getTableDescriptor(),
-          Bytes.toBytes("D"), Bytes.toBytes(""));
-      TEST_UTIL.getHBaseCluster().getMaster().assignRegion(hriHole);
-      TEST_UTIL.getHBaseCluster().getMaster().getAssignmentManager()
-          .waitForAssignment(hriHole);
+      TEST_UTIL.getHBaseAdmin().disableTable(table);
+      deleteRegion(conf, tbl.getTableDescriptor(), Bytes.toBytes("B"),
+          Bytes.toBytes("C"), true, true, false, true);
+      TEST_UTIL.getHBaseAdmin().enableTable(table);
+
+      HBaseFsck hbck = doFsck(conf, false);
+      assertErrors(hbck, new ERROR_CODE[] {
+          ERROR_CODE.ORPHAN_HDFS_REGION,
+          ERROR_CODE.NOT_IN_META_OR_DEPLOYED,
+          ERROR_CODE.HOLE_IN_REGION_CHAIN});
+      // holes are separate from overlap groups
+      assertEquals(0, hbck.getOverlapGroups(table).size());
+
+      // fix hole
+      doFsck(conf, true);
+
+      // check that hole fixed
+      assertNoErrors(doFsck(conf, false));
+      assertEquals(ROWKEYS.length, countRows());
+    } finally {
+      deleteTable(table);
+    }
+  }
+
+  /**
+   * This creates and fixes a bad table with a region that is missing meta and
+   * not assigned to a region server.
+   */
+  @Test
+  public void testNotInMetaOrDeployedHole() throws Exception {
+    String table = "tableNotInMetaOrDeployedHole";
+    try {
+      setupTable(table);
+      assertEquals(ROWKEYS.length, countRows());
+
+      // Mess it up by leaving a hole in the meta data
+      TEST_UTIL.getHBaseAdmin().disableTable(table);
+      deleteRegion(conf, tbl.getTableDescriptor(), Bytes.toBytes("B"),
+          Bytes.toBytes("C"), true, true, false); // don't rm from fs
+      TEST_UTIL.getHBaseAdmin().enableTable(table);
+
+      HBaseFsck hbck = doFsck(conf, false);
+      assertErrors(hbck, new ERROR_CODE[] {
+          ERROR_CODE.NOT_IN_META_OR_DEPLOYED, ERROR_CODE.HOLE_IN_REGION_CHAIN});
+      // holes are separate from overlap groups
+      assertEquals(0, hbck.getOverlapGroups(table).size());
+
+      // fix hole
+      assertErrors(doFsck(conf, true) , new ERROR_CODE[] {
+          ERROR_CODE.NOT_IN_META_OR_DEPLOYED, ERROR_CODE.HOLE_IN_REGION_CHAIN});
+
+      // check that hole fixed
+      assertNoErrors(doFsck(conf,false));
+      assertEquals(ROWKEYS.length, countRows());
+    } finally {
+      deleteTable(table);
+    }
+  }
+
+  /**
+   * This creates fixes a bad table with a hole in meta.
+   */
+  @Test
+  public void testNotInMetaHole() throws Exception {
+    String table = "tableNotInMetaHole";
+    try {
+      setupTable(table);
+      assertEquals(ROWKEYS.length, countRows());
 
+      // Mess it up by leaving a hole in the meta data
       TEST_UTIL.getHBaseAdmin().disableTable(table);
-      deleteRegion(conf, tbl.getTableDescriptor(), Bytes.toBytes("C"), Bytes.toBytes(""));
+      deleteRegion(conf, tbl.getTableDescriptor(), Bytes.toBytes("B"),
+          Bytes.toBytes("C"), false, true, false); // don't rm from fs
       TEST_UTIL.getHBaseAdmin().enableTable(table);
 
       HBaseFsck hbck = doFsck(conf, false);
-      assertErrors(hbck, new ERROR_CODE[] { ERROR_CODE.HOLE_IN_REGION_CHAIN });
+      assertErrors(hbck, new ERROR_CODE[] {
+          ERROR_CODE.NOT_IN_META_OR_DEPLOYED, ERROR_CODE.HOLE_IN_REGION_CHAIN});
       // holes are separate from overlap groups
       assertEquals(0, hbck.getOverlapGroups(table).size());
+
+      // fix hole
+      assertErrors(doFsck(conf, true) , new ERROR_CODE[] {
+          ERROR_CODE.NOT_IN_META_OR_DEPLOYED, ERROR_CODE.HOLE_IN_REGION_CHAIN});
+
+      // check that hole fixed
+      assertNoErrors(doFsck(conf,false));
+      assertEquals(ROWKEYS.length, countRows());
     } finally {
       deleteTable(table);
     }
   }
+
+  /**
+   * This creates and fixes a bad table with a region that is in meta but has
+   * no deployment or data hdfs
+   */
+  @Test
+  public void testNotInHdfs() throws Exception {
+    String table = "tableNotInHdfs";
+    try {
+      setupTable(table);
+      assertEquals(ROWKEYS.length, countRows());
+
+      // make sure data in regions, if in hlog only there is no data loss
+      TEST_UTIL.getHBaseAdmin().flush(table);
+
+      // Mess it up by leaving a hole in the hdfs data
+      deleteRegion(conf, tbl.getTableDescriptor(), Bytes.toBytes("B"),
+          Bytes.toBytes("C"), false, false, true); // don't rm meta
+
+      HBaseFsck hbck = doFsck(conf, false);
+      assertErrors(hbck, new ERROR_CODE[] {ERROR_CODE.NOT_IN_HDFS});
+      // holes are separate from overlap groups
+      assertEquals(0, hbck.getOverlapGroups(table).size());
+
+      // fix hole
+      doFsck(conf, true);
+
+      // check that hole fixed
+      assertNoErrors(doFsck(conf,false));
+      assertEquals(ROWKEYS.length - 2, countRows());
+    } finally {
+      deleteTable(table);
+    }
+  }
+
+  /**
+   * This creates entries in META with no hdfs data.  This should cleanly
+   * remove the table.
+   */
+  @Test
+  public void testNoHdfsTable() throws Exception {
+    String table = "NoHdfsTable";
+    setupTable(table);
+    assertEquals(ROWKEYS.length, countRows());
+
+    // make sure data in regions, if in hlog only there is no data loss
+    TEST_UTIL.getHBaseAdmin().flush(table);
+
+    // Mess it up by leaving a giant hole in meta
+    deleteRegion(conf, tbl.getTableDescriptor(), Bytes.toBytes(""),
+        Bytes.toBytes("A"), false, false, true); // don't rm meta
+    deleteRegion(conf, tbl.getTableDescriptor(), Bytes.toBytes("A"),
+        Bytes.toBytes("B"), false, false, true); // don't rm meta
+    deleteRegion(conf, tbl.getTableDescriptor(), Bytes.toBytes("B"),
+        Bytes.toBytes("C"), false, false, true); // don't rm meta
+    deleteRegion(conf, tbl.getTableDescriptor(), Bytes.toBytes("C"),
+        Bytes.toBytes(""), false, false, true); // don't rm meta
+
+    HBaseFsck hbck = doFsck(conf, false);
+    assertErrors(hbck, new ERROR_CODE[] {ERROR_CODE.NOT_IN_HDFS,
+        ERROR_CODE.NOT_IN_HDFS, ERROR_CODE.NOT_IN_HDFS,
+        ERROR_CODE.NOT_IN_HDFS,});
+    // holes are separate from overlap groups
+    assertEquals(0, hbck.getOverlapGroups(table).size());
+
+    // fix hole
+    doFsck(conf, true); // in 0.92+, meta entries auto create regiondirs
+
+    // check that hole fixed
+    assertNoErrors(doFsck(conf,false));
+
+    try {
+      assertEquals(0, countRows());
+    } catch (IOException ioe) {
+      // we've actually deleted the table already. :)
+      return;
+    }
+    fail("Should have failed with IOException");
+  }
 }
diff --git a/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckComparator.java b/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckComparator.java
index 9166968..beff29f 100644
--- a/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckComparator.java
+++ b/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckComparator.java
@@ -92,12 +92,4 @@ public class TestHBaseFsckComparator {
     assertTrue(HBaseFsck.cmp.compare(hi1, hi2) < 0);
     assertTrue(HBaseFsck.cmp.compare(hi2, hi1) > 0);
   }
-
-  @Test
-  public void testTiebreaker() {
-    HbckInfo hi1 = genHbckInfo(table, keyA, keyC, 0);
-    HbckInfo hi2 = genHbckInfo(table, keyA, keyC, 1);
-    assertTrue(HBaseFsck.cmp.compare(hi1, hi2) < 0);
-    assertTrue(HBaseFsck.cmp.compare(hi2, hi1) > 0);
-  }
 }
diff --git a/src/test/java/org/apache/hadoop/hbase/util/hbck/HbckTestingUtil.java b/src/test/java/org/apache/hadoop/hbase/util/hbck/HbckTestingUtil.java
index dbb97f8..75d24da 100644
--- a/src/test/java/org/apache/hadoop/hbase/util/hbck/HbckTestingUtil.java
+++ b/src/test/java/org/apache/hadoop/hbase/util/hbck/HbckTestingUtil.java
@@ -19,6 +19,7 @@ package org.apache.hadoop.hbase.util.hbck;
 
 import static org.junit.Assert.assertEquals;
 
+import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
 
@@ -28,18 +29,29 @@ import org.apache.hadoop.hbase.util.HBaseFsck.ErrorReporter.ERROR_CODE;
 
 public class HbckTestingUtil {
   public static HBaseFsck doFsck(Configuration conf, boolean fix) throws Exception {
+    return doFsck(conf, fix, fix, fix, fix,fix);
+  }
+
+  public static HBaseFsck doFsck(Configuration conf, boolean fixAssignments,
+      boolean fixMeta, boolean fixHdfsHoles, boolean fixHdfsOverlaps,
+      boolean fixHdfsOrphans) throws Exception {
     HBaseFsck fsck = new HBaseFsck(conf);
     fsck.connect();
-    fsck.displayFullReport(); // i.e. -details
+    fsck.setDisplayFullReport(); // i.e. -details
     fsck.setTimeLag(0);
-    fsck.setFixErrors(fix);
-    fsck.doWork();
+    fsck.setFixAssignments(fixAssignments);
+    fsck.setFixMeta(fixMeta);
+    fsck.setFixHdfsHoles(fixHdfsHoles);
+    fsck.setFixHdfsOverlaps(fixHdfsOverlaps);
+    fsck.setFixHdfsOrphans(fixHdfsOrphans);
+    fsck.onlineHbck();
     return fsck;
   }
 
+
   public static void assertNoErrors(HBaseFsck fsck) throws Exception {
     List<ERROR_CODE> errs = fsck.getErrors().getErrorList();
-    assertEquals(0, errs.size());
+    assertEquals(new ArrayList<ERROR_CODE>(), errs);
   }
 
   public static void assertErrors(HBaseFsck fsck, ERROR_CODE[] expectedErrors) {
diff --git a/src/test/java/org/apache/hadoop/hbase/util/hbck/TestOfflineMetaRebuildBase.java b/src/test/java/org/apache/hadoop/hbase/util/hbck/TestOfflineMetaRebuildBase.java
index d6cf7b3..091b0d3 100644
--- a/src/test/java/org/apache/hadoop/hbase/util/hbck/TestOfflineMetaRebuildBase.java
+++ b/src/test/java/org/apache/hadoop/hbase/util/hbck/TestOfflineMetaRebuildBase.java
@@ -59,7 +59,7 @@ public class TestOfflineMetaRebuildBase extends OfflineMetaRebuildTestCore {
 
     // rebuild meta table from scratch
     HBaseFsck fsck = new HBaseFsck(conf);
-    assertTrue(fsck.rebuildMeta());
+    assertTrue(fsck.rebuildMeta(false));
 
     // bring up the minicluster
     TEST_UTIL.startMiniZKCluster(); // tables seem enabled by default
diff --git a/src/test/java/org/apache/hadoop/hbase/util/hbck/TestOfflineMetaRebuildHole.java b/src/test/java/org/apache/hadoop/hbase/util/hbck/TestOfflineMetaRebuildHole.java
index 97480c6..7ad2936 100644
--- a/src/test/java/org/apache/hadoop/hbase/util/hbck/TestOfflineMetaRebuildHole.java
+++ b/src/test/java/org/apache/hadoop/hbase/util/hbck/TestOfflineMetaRebuildHole.java
@@ -21,6 +21,7 @@ import static org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.assertErrors;
 import static org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.doFsck;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
 
 import java.util.Arrays;
 
@@ -61,7 +62,7 @@ public class TestOfflineMetaRebuildHole extends OfflineMetaRebuildTestCore {
 
     // attempt to rebuild meta table from scratch
     HBaseFsck fsck = new HBaseFsck(conf);
-    assertFalse(fsck.rebuildMeta());
+    assertFalse(fsck.rebuildMeta(false));
 
     // bring up the minicluster
     TEST_UTIL.startMiniZKCluster(); // tables seem enabled by default
diff --git a/src/test/java/org/apache/hadoop/hbase/util/hbck/TestOfflineMetaRebuildOverlap.java b/src/test/java/org/apache/hadoop/hbase/util/hbck/TestOfflineMetaRebuildOverlap.java
index 18c12bc..d48653c 100644
--- a/src/test/java/org/apache/hadoop/hbase/util/hbck/TestOfflineMetaRebuildOverlap.java
+++ b/src/test/java/org/apache/hadoop/hbase/util/hbck/TestOfflineMetaRebuildOverlap.java
@@ -66,7 +66,7 @@ public class TestOfflineMetaRebuildOverlap extends OfflineMetaRebuildTestCore {
 
     // attempt to rebuild meta table from scratch
     HBaseFsck fsck = new HBaseFsck(conf);
-    assertFalse(fsck.rebuildMeta());
+    assertFalse(fsck.rebuildMeta(false));
 
     Multimap<byte[], HbckInfo> problems = fsck.getOverlapGroups(table);
     assertEquals(1, problems.keySet().size());
-- 
1.7.0.4

