<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>1.2.&nbsp;Operating System</title><link rel="stylesheet" type="text/css" href="../css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"><link rel="home" href="configuration.html" title="Chapter&nbsp;1.&nbsp;Configuration"><link rel="up" href="configuration.html" title="Chapter&nbsp;1.&nbsp;Configuration"><link rel="prev" href="configuration.html" title="Chapter&nbsp;1.&nbsp;Configuration"><link rel="next" href="hadoop.html" title="1.3.&nbsp;Hadoop"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">1.2.&nbsp;Operating System</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="configuration.html">Prev</a>&nbsp;</td><th width="60%" align="center">&nbsp;</th><td width="20%" align="right">&nbsp;<a accesskey="n" href="hadoop.html">Next</a></td></tr></table><hr></div><div class="section" title="1.2.&nbsp;Operating System"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="os"></a>1.2.&nbsp;Operating System</h2></div></div></div><div class="section" title="1.2.1.&nbsp;ssh"><div class="titlepage"><div><div><h3 class="title"><a name="ssh"></a>1.2.1.&nbsp;ssh</h3></div></div></div><p><span class="command"><strong>ssh</strong></span> must be installed and
        <span class="command"><strong>sshd</strong></span> must be running to use Hadoop's scripts to
        manage remote Hadoop and HBase daemons. You must be able to ssh to all
        nodes, including your local node, using passwordless login (Google
        "ssh passwordless login").</p></div><div class="section" title="1.2.2.&nbsp;DNS"><div class="titlepage"><div><div><h3 class="title"><a name="dns"></a>1.2.2.&nbsp;DNS</h3></div></div></div><p>HBase uses the local hostname to self-report it's IP address.
        Both forward and reverse DNS resolving should work.</p><p>If your machine has multiple interfaces, HBase will use the
        interface that the primary hostname resolves to.</p><p>If this is insufficient, you can set
        <code class="varname">hbase.regionserver.dns.interface</code> to indicate the
        primary interface. This only works if your cluster configuration is
        consistent and every host has the same network interface
        configuration.</p><p>Another alternative is setting
        <code class="varname">hbase.regionserver.dns.nameserver</code> to choose a
        different nameserver than the system wide default.</p></div><div class="section" title="1.2.3.&nbsp;NTP"><div class="titlepage"><div><div><h3 class="title"><a name="ntp"></a>1.2.3.&nbsp;NTP</h3></div></div></div><p>The clocks on cluster members should be in basic alignments.
        Some skew is tolerable but wild skew could generate odd behaviors. Run
        <a class="link" href="http://en.wikipedia.org/wiki/Network_Time_Protocol" target="_top">NTP</a>
        on your cluster, or an equivalent.</p><p>If you are having problems querying data, or "weird" cluster
        operations, check system time!</p></div><div class="section" title="1.2.4.&nbsp; ulimit and nproc"><div class="titlepage"><div><div><h3 class="title"><a name="ulimit"></a>1.2.4.&nbsp;
          <code class="varname">ulimit</code><a class="indexterm" name="d2090e82"></a>
            and
          <code class="varname">nproc</code><a class="indexterm" name="d2090e88"></a>
        </h3></div></div></div><p>HBase is a database.  It uses a lot of files all at the same time.
        The default ulimit -n -- i.e. user file limit -- of 1024 on most *nix systems
        is insufficient (On mac os x its 256). Any significant amount of loading will
        lead you to <a class="xref" href="">???</a>.
        You may also notice errors such as... </p><pre class="programlisting">
      2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Exception increateBlockOutputStream java.io.EOFException
      2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_-6935524980745310745_1391901
      </pre><p> Do yourself a favor and change the upper bound on the
        number of file descriptors. Set it to north of 10k.  The math runs roughly as follows:  per ColumnFamily
        there is at least one StoreFile and possibly up to 5 or 6 if the region is under load.  Multiply the 
        average number of StoreFiles per ColumnFamily times the number of regions per RegionServer.  For example, assuming
        that a schema had 3 ColumnFamilies per region with an average of 3 StoreFiles per ColumnFamily, 
        and there are 100 regions per RegionServer, the JVM will open 3 * 3 * 100 = 900 file descriptors
        (not counting open jar files, config files, etc.)
        </p><p>You should also up the hbase users'
        <code class="varname">nproc</code> setting; under load, a low-nproc
        setting could manifest as <code class="classname">OutOfMemoryError</code>
        <sup>[<a name="d2090e107" href="#ftn.d2090e107" class="footnote">2</a>]</sup>
        <sup>[<a name="d2090e114" href="#ftn.d2090e114" class="footnote">3</a>]</sup>.
       </p><p>To be clear, upping the file descriptors and nproc for the user who is
        running the HBase process is an operating system configuration, not an
        HBase configuration. Also, a common mistake is that administrators
        will up the file descriptors for a particular user but for whatever
        reason, HBase will be running as some one else. HBase prints in its
        logs as the first line the ulimit its seeing. Ensure its correct.
        <sup>[<a name="d2090e126" href="#ftn.d2090e126" class="footnote">4</a>]</sup></p><div class="section" title="1.2.4.1.&nbsp;ulimit on Ubuntu"><div class="titlepage"><div><div><h4 class="title"><a name="ulimit_ubuntu"></a>1.2.4.1.&nbsp;<code class="varname">ulimit</code> on Ubuntu</h4></div></div></div><p>If you are on Ubuntu you will need to make the following
          changes:</p><p>In the file <code class="filename">/etc/security/limits.conf</code> add
          a line like: </p><pre class="programlisting">hadoop  -       nofile  32768</pre><p>
          Replace <code class="varname">hadoop</code> with whatever user is running
          Hadoop and HBase. If you have separate users, you will need 2
          entries, one for each user.  In the same file set nproc hard and soft
          limits.  For example: </p><pre class="programlisting">hadoop soft/hard nproc 32000</pre><p>.</p><p>In the file <code class="filename">/etc/pam.d/common-session</code> add
          as the last line in the file: </p><pre class="programlisting">session required  pam_limits.so</pre><p>
          Otherwise the changes in <code class="filename">/etc/security/limits.conf</code> won't be
          applied.</p><p>Don't forget to log out and back in again for the changes to
          take effect!</p></div></div><div class="section" title="1.2.5.&nbsp;Windows"><div class="titlepage"><div><div><h3 class="title"><a name="windows"></a>1.2.5.&nbsp;Windows</h3></div></div></div><p>HBase has been little tested running on Windows. Running a
        production install of HBase on top of Windows is not
        recommended.</p><p>If you are running HBase on Windows, you must install <a class="link" href="http://cygwin.com/" target="_top">Cygwin</a> to have a *nix-like
        environment for the shell scripts. The full details are explained in
        the <a class="link" href="http://hbase.apache.org/cygwin.html" target="_top">Windows
        Installation</a> guide. Also 
        <a class="link" href="http://search-hadoop.com/?q=hbase+windows&amp;fc_project=HBase&amp;fc_type=mail+_hash_+dev" target="_top">search our user mailing list</a> to pick
        up latest fixes figured by Windows users.</p></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d2090e107" href="#d2090e107" class="para">2</a>] </sup>See Jack Levin's <a class="link" href="" target="_top">major hdfs issues</a>
                note up on the user list.</p></div><div class="footnote"><p><sup>[<a id="ftn.d2090e114" href="#d2090e114" class="para">3</a>] </sup>The requirement that a database requires upping of system limits
        is not peculiar to HBase.  See for example the section
        <span class="emphasis"><em>Setting Shell Limits for the Oracle User</em></span> in
        <a class="link" href="http://www.akadia.com/services/ora_linux_install_10g.html" target="_top">
        Short Guide to install Oracle 10 on Linux</a>.</p></div><div class="footnote"><p><sup>[<a id="ftn.d2090e126" href="#d2090e126" class="para">4</a>] </sup>A useful read setting config on you hadoop cluster is Aaron
            Kimballs' Configuration
            Parameters: What can you just ignore?</p></div></div></div><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="configuration.html">Prev</a>&nbsp;</td><td width="20%" align="center">&nbsp;</td><td width="40%" align="right">&nbsp;<a accesskey="n" href="hadoop.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">Chapter&nbsp;1.&nbsp;Configuration&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="configuration.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;1.3.&nbsp;Hadoop</td></tr></table></div></body></html>