<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- Generated by Apache Maven Doxia at Jul 18, 2011 -->
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>HBase - 
    
      HBase Replication
    </title>
    <style type="text/css" media="all">
      @import url("./css/maven-base.css");
      @import url("./css/maven-theme.css");
      @import url("./css/site.css");
    </style>
    <link rel="stylesheet" href="./css/print.css" type="text/css" media="print" />
        <meta name="Date-Revision-yyyymmdd" content="20110718" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      </head>
  <body class="composite">
    <div id="banner">
                                              <a href="./" id="bannerLeft">
                                                <img src="images/hbase_logo_med.gif" alt="" />
                </a>
                              <a href="http://www.apache.org/" id="bannerRight">
                                                <img src="images/asf_logo_wide.png" alt="" />
                </a>
            <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="breadcrumbs">
            
                                  <div class="xright">        
                                 Last Published: 2011-07-18
              &nbsp;| Version: 0.90.3-cdh3u1
            </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="leftColumn">
      <div id="navcolumn">
             
                                                <h5>HBase Project</h5>
                  <ul>
                  <li class="none">
                  <a href="index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="license.html">License</a>
            </li>
                  <li class="none">
                  <a href="http://www.apache.org/dyn/closer.cgi/hbase/" class="externalLink">Downloads</a>
            </li>
                  <li class="none">
                  <a href="https://issues.apache.org/jira/browse/HBASE?report=com.atlassian.jira.plugin.system.project:changelog-panel" class="externalLink">Release Notes</a>
            </li>
                  <li class="none">
                  <a href="issue-tracking.html">Issue Tracking</a>
            </li>
                  <li class="none">
                  <a href="mail-lists.html">Mailing Lists</a>
            </li>
                  <li class="none">
                  <a href="source-repository.html">Source Repository</a>
            </li>
                  <li class="none">
                  <a href="team-list.html">Team</a>
            </li>
          </ul>
                       <h5>Documentation</h5>
                  <ul>
                  <li class="none">
                  <a href="quickstart.html">Getting Started: Quick</a>
            </li>
                  <li class="none">
                  <a href="notsoquick.html">Getting Started: Detailed</a>
            </li>
                  <li class="none">
                  <a href="apidocs/index.html">API</a>
            </li>
                  <li class="none">
                  <a href="xref/index.html">X-Ref</a>
            </li>
                  <li class="none">
                  <a href="book.html">Book</a>
            </li>
                  <li class="none">
                  <a href="faq.html">FAQ</a>
            </li>
                  <li class="none">
                  <a href="http://wiki.apache.org/hadoop/Hbase" class="externalLink">Wiki</a>
            </li>
                  <li class="none">
                  <a href="acid-semantics.html">ACID Semantics</a>
            </li>
                  <li class="none">
                  <a href="bulk-loads.html">Bulk Loads</a>
            </li>
                  <li class="none">
                  <a href="metrics.html">Metrics</a>
            </li>
                  <li class="none">
                  <a href="cygwin.html">HBase on Windows</a>
            </li>
                  <li class="none">
            <strong>Cluster replication</strong>
          </li>
                  <li class="none">
                  <a href="pseudo-distributed.html">Pseudo-Dist. Extras</a>
            </li>
          </ul>
                                 <a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy">
          <img alt="Built by Maven" src="./images/logos/maven-feather.png"/>
        </a>
                       
                            </div>
    </div>
    <div id="bodyColumn">
      <div id="contentBox">
        <!-- Copyright 2010 The Apache Software Foundation

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. -->
  
    <div class="section"><h2>Overview<a name="Overview"></a></h2>
      <p>
        HBase replication is a way to copy data between HBase deployments. It
        can serve as a disaster recovery solution and can contribute to provide
        higher availability at the HBase layer. It can also serve more practically;
        for example, as a way to easily copy edits from a web-facing cluster to a &quot;MapReduce&quot;
        cluster which will process old and new data and ship back the results
        automatically.
      </p>
      <p>
        The basic architecture pattern used for HBase replication is (HBase cluster) master-push;
        it is much easier to keep track of what&#x2019;s currently being replicated since
        each region server has its own write-ahead-log (aka WAL or HLog), just like
        other well known solutions like MySQL master/slave replication where
        there&#x2019;s only one bin log to keep track of. One master cluster can
        replicate to any number of slave clusters, and each region server will
        participate to replicate their own stream of edits. For more information
        on the different properties of master/slave replication and other types
        of replication, please consult <a class="externalLink" href="http://highscalability.com/blog/2009/8/24/how-google-serves-data-from-multiple-datacenters.html">
        How Google Serves Data From Multiple Datacenters</a>.
      </p>
      <p>
        The replication is done asynchronously, meaning that the clusters can
        be geographically distant, the links between them can be offline for
        some time, and rows inserted on the master cluster won&#x2019;t be
        available at the same time on the slave clusters (eventual consistency).
      </p>
      <p>
        The replication format used in this design is conceptually the same as
        <a class="externalLink" href="http://dev.mysql.com/doc/refman/5.1/en/replication-formats.html">
        MySQL&#x2019;s statement-based replication </a>. Instead of SQL statements, whole
        WALEdits (consisting of multiple cell inserts coming from the clients'
        Put and Delete) are replicated in order to maintain atomicity.
      </p>
      <p>
        The HLogs from each region server are the basis of HBase replication,
        and must be kept in HDFS as long as they are needed to replicate data
        to any slave cluster. Each RS reads from the oldest log it needs to
        replicate and keeps the current position inside ZooKeeper to simplify
        failure recovery. That position can be different for every slave 
        cluster, same for the queue of HLogs to process.
      </p>
      <p>
        The clusters participating in replication can be of asymmetric sizes
        and the master cluster will do its &#x201c;best effort&#x201d; to balance the stream
        of replication on the slave clusters by relying on randomization.
      </p>
      <img src="images/replication_overview.png" alt="" />
    </div>
    <div class="section"><h2>Enabling replication<a name="Enabling_replication"></a></h2>
      <p>
        The guide on enabling and using cluster replication is contained
        in the API documentation shipped with your HBase distribution.
      </p>
      <p>
        The most up-to-date documentation is
        <a href="apidocs/org/apache/hadoop/hbase/replication/package-summary.html#requirements">
        available at this address</a>.
      </p>
    </div>
    <div class="section"><h2>Life of a log edit<a name="Life_of_a_log_edit"></a></h2>
      <p>
        The following sections describe the life of a single edit going from a
        client that communicates with a master cluster all the way to a single
        slave cluster.
      </p>
      <div class="section"><h2>Normal processing<a name="Normal_processing"></a></h2>
        <p>
          The client uses a HBase API that sends a Put, Delete or ICV to a region
          server. The key values are transformed into a WALEdit by the region
          server and is inspected by the replication code that, for each family
          that is scoped for replication, adds the scope to the edit. The edit
          is appended to the current WAL and is then applied to its MemStore.
        </p>
        <p>
          In a separate thread, the edit is read from the log (as part of a batch)
          and only the KVs that are replicable are kept (that is, that they are part
          of a family scoped GLOBAL in the family's schema and non-catalog so not
          .META. or -ROOT-). When the buffer is filled, or the reader hits the
          end of the file, the buffer is sent to a random region server on the
          slave cluster.
        </p>
        <p>
          Synchronously, the region server that receives the edits reads them
          sequentially and separates each of them into buffers, one per table.
          Once all edits are read, each buffer is flushed using the normal HBase
          client (HTables managed by a HTablePool). This is done in order to
          leverage parallel insertion (MultiPut).
        </p>
        <p>
          Back in the master cluster's region server, the offset for the current
          WAL that's being replicated is registered in ZooKeeper.
        </p>
      </div>
      <div class="section"><h2>Non-responding slave clusters<a name="Non-responding_slave_clusters"></a></h2>
        <p>
          The edit is inserted in the same way.
        </p>
        <p>
          In the separate thread, the region server reads, filters and buffers
          the log edits the same way as during normal processing. The slave
          region server that's contacted doesn't answer to the RPC, so the master
          region server will sleep and retry up to a configured number of times.
          If the slave RS still isn't available, the master cluster RS will select a
          new subset of RS to replicate to and will retry sending the buffer of
          edits.
        </p>
        <p>
          In the mean time, the WALs will be rolled and stored in a queue in
          ZooKeeper. Logs that are archived by their region server (archiving is
          basically moving a log from the region server's logs directory to a
          central logs archive directory) will update their paths in the in-memory
          queue of the replicating thread.
        </p>
        <p>
          When the slave cluster is finally available, the buffer will be applied
          the same way as during normal processing. The master cluster RS will then
          replicate the backlog of logs.
        </p>
      </div>
    </div>
    <div class="section"><h2>Internals<a name="Internals"></a></h2>
      <p>
        This section describes in depth how each of replication's internal
        features operate.
      </p>
      <div class="section"><h2>Choosing region servers to replicate to<a name="Choosing_region_servers_to_replicate_to"></a></h2>
        <p>
          When a master cluster RS initiates a replication source to a slave cluster,
          it first connects to the slave's ZooKeeper ensemble using the provided
          cluster key (that key is composed of the value of hbase.zookeeper.quorum,
          zookeeper.znode.parent and hbase.zookeeper.property.clientPort). It
          then scans the &quot;rs&quot; directory to discover all the available sinks
          (region servers that are accepting incoming streams of edits to replicate)
          and will randomly choose a subset of them using a configured
          ratio (which has a default value of 10%). For example, if a slave
          cluster has 150 machines, 15 will be chosen as potential recipient for
          edits that this master cluster RS will be sending. Since this is done by all
          master cluster RSs, the probability that all slave RSs are used is very high,
          and this method works for clusters of any size. For example, a master cluster
          of 10 machines replicating to a slave cluster of 5 machines with a ratio
          of 10% means that the master cluster RSs will choose one machine each
          at random, thus the chance of overlapping and full usage of the slave
          cluster is higher.
        </p>
      </div>
      <div class="section"><h2>Keeping track of logs<a name="Keeping_track_of_logs"></a></h2>
        <p>
          Every master cluster RS has its own znode in the replication znodes hierarchy.
          It contains one znode per peer cluster (if 5 slave clusters, 5 znodes
          are created), and each of these contain a queue
          of HLogs to process. Each of these queues will track the HLogs created
          by that RS, but they can differ in size. For example, if one slave
          cluster becomes unavailable for some time then the HLogs should not be deleted,
          thus they need to stay in the queue (while the others are processed).
          See the section named &quot;Region server failover&quot; for an example.
        </p>
        <p>
          When a source is instantiated, it contains the current HLog that the
          region server is writing to. During log rolling, the new file is added
          to the queue of each slave cluster's znode just before it's made available.
          This ensures that all the sources are aware that a new log exists
          before HLog is able to append edits into it, but this operations is
          now more expensive.
          The queue items are discarded when the replication thread cannot read
          more entries from a file (because it reached the end of the last block)
          and that there are other files in the queue.
          This means that if a source is up-to-date and replicates from the log
          that the region server writes to, reading up to the &quot;end&quot; of the
          current file won't delete the item in the queue.
        </p>
        <p>
          When a log is archived (because it's not used anymore or because there's
          too many of them per hbase.regionserver.maxlogs typically because insertion
          rate is faster than region flushing), it will notify the source threads that the path
          for that log changed. If the a particular source was already done with
          it, it will just ignore the message. If it's in the queue, the path
          will be updated in memory. If the log is currently being replicated,
          the change will be done atomically so that the reader doesn't try to
          open the file when it's already moved. Also, moving a file is a NameNode
          operation so, if the reader is currently reading the log, it won't
          generate any exception.
        </p>
      </div>
      <div class="section"><h2>Reading, filtering and sending edits<a name="Reading_filtering_and_sending_edits"></a></h2>
        <p>
          By default, a source will try to read from a log file and ship log
          entries as fast as possible to a sink. This is first limited by the
          filtering of log entries; only KeyValues that are scoped GLOBAL and
          that don't belong to catalog tables will be retained. A second limit
          is imposed on the total size of the list of edits to replicate per slave,
          which by default is 64MB. This means that a master cluster RS with 3 slaves
          will use at most 192MB to store data to replicate. This doesn't account
          the data filtered that wasn't garbage collected.
        </p>
        <p>
          Once the maximum size of edits was buffered or the reader hits the end
          of the log file, the source thread will stop reading and will choose
          at random a sink to replicate to (from the list that was generated by
          keeping only a subset of slave RSs). It will directly issue a RPC to
          the chosen machine and will wait for the method to return. If it's
          successful, the source will determine if the current file is emptied
          or if it should continue to read from it. If the former, it will delete
          the znode in the queue. If the latter, it will register the new offset
          in the log's znode. If the RPC threw an exception, the source will retry
          10 times until trying to find a different sink.
        </p>
      </div>
      <div class="section"><h2>Cleaning logs<a name="Cleaning_logs"></a></h2>
        <p>
          If replication isn't enabled, the master's logs cleaning thread will
          delete old logs using a configured TTL. This doesn't work well with
          replication since archived logs passed their TTL may still be in a
          queue. Thus, the default behavior is augmented so that if a log is
          passed its TTL, the cleaning thread will lookup every queue until it
          finds the log (while caching the ones it finds). If it's not found,
          the log will be deleted. The next time it has to look for a log,
          it will first use its cache.
        </p>
      </div>
      <div class="section"><h2>Region server failover<a name="Region_server_failover"></a></h2>
        <p>
          As long as region servers don't fail, keeping track of the logs in ZK
          doesn't add any value. Unfortunately, they do fail, so since ZooKeeper
          is highly available we can count on it and its semantics to help us
          managing the transfer of the queues.
        </p>
        <p>
          All the master cluster RSs keep a watcher on every other one of them to be
          notified when one dies (just like the master does). When it happens,
          they all race to create a znode called &quot;lock&quot; inside the dead RS' znode
          that contains its queues. The one that creates it successfully will
          proceed by transferring all the queues to its own znode (one by one
          since ZK doesn't support the rename operation) and will delete all the
          old ones when it's done. The recovered queues' znodes will be named
          with the id of the slave cluster appended with the name of the dead
          server. 
        </p>
        <p>
          Once that is done, the master cluster RS will create one new source thread per
          copied queue, and each of them will follow the read/filter/ship pattern.
          The main difference is that those queues will never have new data since
          they don't belong to their new region server, which means that when
          the reader hits the end of the last log, the queue's znode will be
          deleted and the master cluster RS will close that replication source.
        </p>
        <p>
          For example, consider a master cluster with 3 region servers that's
          replicating to a single slave with id '2'. The following hierarchy
          represents what the znodes layout could be at some point in time. We
          can see the RSs' znodes all contain a &quot;peers&quot; znode that contains a
          single queue. The znode names in the queues represent the actual file
          names on HDFS in the form &quot;address,port.timestamp&quot;.
        </p>
        <div><pre>
/hbase/replication/rs/
                      1.1.1.1,60020,123456780/
                        peers/
                              2/
                                1.1.1.1,60020.1234  (Contains a position)
                                1.1.1.1,60020.1265
                      1.1.1.2,60020,123456790/
                        peers/
                              2/
                                1.1.1.2,60020.1214  (Contains a position)
                                1.1.1.2,60020.1248
                                1.1.1.2,60020.1312
                      1.1.1.3,60020,    123456630/
                        peers/
                              2/
                                1.1.1.3,60020.1280  (Contains a position)

        </pre></div>
        <p>
          Now let's say that 1.1.1.2 loses its ZK session. The survivors will race
          to create a lock, and for some reasons 1.1.1.3 wins. It will then start
          transferring all the queues to its local peers znode by appending the
          name of the dead server. Right before 1.1.1.3 is able to clean up the
          old znodes, the layout will look like the following:
        </p>
        <div><pre>
/hbase/replication/rs/
                      1.1.1.1,60020,123456780/
                        peers/
                              2/
                                1.1.1.1,60020.1234  (Contains a position)
                                1.1.1.1,60020.1265
                      1.1.1.2,60020,123456790/
                        lock
                        peers/
                              2/
                                1.1.1.2,60020.1214  (Contains a position)
                                1.1.1.2,60020.1248
                                1.1.1.2,60020.1312
                      1.1.1.3,60020,123456630/
                        peers/
                              2/
                                1.1.1.3,60020.1280  (Contains a position)

                              2-1.1.1.2,60020,123456790/
                                1.1.1.2,60020.1214  (Contains a position)
                                1.1.1.2,60020.1248
                                1.1.1.2,60020.1312
        </pre></div>
        <p>
          Some time later, but before 1.1.1.3 is able to finish replicating the
          last HLog from 1.1.1.2, let's say that it dies too (also some new logs
          were created in the normal queues). The last RS will then try to lock
          1.1.1.3's znode and will begin transferring all the queues. The new
          layout will be:
        </p>
        <div><pre>
/hbase/replication/rs/
                      1.1.1.1,60020,123456780/
                        peers/
                              2/
                                1.1.1.1,60020.1378  (Contains a position)

                              2-1.1.1.3,60020,123456630/
                                1.1.1.3,60020.1325  (Contains a position)
                                1.1.1.3,60020.1401

                              2-1.1.1.2,60020,123456790-1.1.1.3,60020,123456630/
                                1.1.1.2,60020.1312  (Contains a position)
                      1.1.1.3,60020,123456630/
                        lock
                        peers/
                              2/
                                1.1.1.3,60020.1325  (Contains a position)
                                1.1.1.3,60020.1401

                              2-1.1.1.2,60020,123456790/
                                1.1.1.2,60020.1312  (Contains a position)
        </pre></div>
      </div>
    </div>
    <div class="section"><h2>FAQ<a name="FAQ"></a></h2>
      <div class="section"><h2>GLOBAL means replicate? Any provision to replicate only to cluster X and not to cluster Y? or is that for later?<a name="GLOBAL_means_replicate_Any_provision_to_replicate_only_to_cluster_X_and_not_to_cluster_Y_or_is_that_for_later"></a></h2>
        <p>
          Yes, this is for much later.
        </p>
      </div>
      <div class="section"><h2>You need a bulk edit shipper? Something that allows you transfer 64MB of edits in one go?<a name="You_need_a_bulk_edit_shipper_Something_that_allows_you_transfer_64MB_of_edits_in_one_go"></a></h2>
        <p>
          You can use the HBase-provided utility called CopyTable from the package
          org.apache.hadoop.hbase.mapreduce in order to have a discp-like tool to
          bulk copy data.
        </p>
      </div>
      <div class="section"><h2>Is it a mistake that WALEdit doesn't carry Put and Delete objects, that we have to reinstantiate not only when replicating but when replaying edits also?<a name="Is_it_a_mistake_that_WALEdit_doesnt_carry_Put_and_Delete_objects_that_we_have_to_reinstantiate_not_only_when_replicating_but_when_replaying_edits_also"></a></h2>
        <p>
          Yes, this behavior would help a lot but it's not currently available
          in HBase (BatchUpdate had that, but it was lost in the new API).
        </p>
      </div>
    </div>
    <div class="section"><h2>Known bugs/missing features<a name="Known_bugsmissing_features"></a></h2>
      <p>
        Here's a list of all the jiras that relate to major issues or missing
        features in the replication implementation.
      </p>
      <ol style="list-style-type: decimal">
        <li>
            HBASE-2611, basically if a region server dies while recovering the
            queues of another dead RS, we will miss the data from the queues
            that weren't copied.
        </li>
        <li>
            HBASE-2196, a master cluster can only support a single slave, some
            refactoring is needed to support this.
        </li>
        <li>
            HBASE-2195, edits are applied disregard their home cluster, it should
            carry that data and check it.
        </li>
        <li>
            HBASE-3130, the master cluster needs to be restarted if its region
            servers lose their session with a slave cluster.
        </li>
      </ol>
    </div>
  

      </div>
    </div>
    <div class="clear">
      <hr/>
    </div>
    <div id="footer">
      <div class="xright">
        &#169;            2011
              Cloudera
            
                       - <a href="http://maven.apache.org/privacy-policy.html">Privacy Policy</a>.
        Apache Maven, Maven, Apache, the Apache feather logo, and the Apache Maven project logos are trademarks of The Apache Software Foundation.
      </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
  </body>
</html>
