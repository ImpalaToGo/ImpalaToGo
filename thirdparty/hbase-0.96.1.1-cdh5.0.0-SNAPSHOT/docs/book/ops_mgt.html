<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>Chapter&nbsp;15.&nbsp;Apache HBase Operational Management</title><link rel="stylesheet" type="text/css" href="../css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"><link rel="home" href="book.html" title="The Apache HBase&#153; Reference Guide"><link rel="up" href="book.html" title="The Apache HBase&#153; Reference Guide"><link rel="prev" href="casestudies.perftroub.html" title="14.3.&nbsp;Performance/Troubleshooting"><link rel="next" href="ops.regionmgt.html" title="15.2.&nbsp;Region Management"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">Chapter&nbsp;15.&nbsp;Apache HBase Operational Management</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="casestudies.perftroub.html">Prev</a>&nbsp;</td><th width="60%" align="center">&nbsp;</th><td width="20%" align="right">&nbsp;<a accesskey="n" href="ops.regionmgt.html">Next</a></td></tr></table><hr></div><div class="chapter" title="Chapter&nbsp;15.&nbsp;Apache HBase Operational Management"><div class="titlepage"><div><div><h2 class="title"><a name="ops_mgt"></a>Chapter&nbsp;15.&nbsp;Apache HBase Operational Management</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="ops_mgt.html#tools">15.1. HBase Tools and Utilities</a></span></dt><dd><dl><dt><span class="section"><a href="ops_mgt.html#health.check">15.1.1. Health Checker</a></span></dt><dt><span class="section"><a href="ops_mgt.html#driver">15.1.2. Driver</a></span></dt><dt><span class="section"><a href="ops_mgt.html#hbck">15.1.3. HBase <span class="application">hbck</span></a></span></dt><dt><span class="section"><a href="ops_mgt.html#hfile_tool2">15.1.4. HFile Tool</a></span></dt><dt><span class="section"><a href="ops_mgt.html#wal_tools">15.1.5. WAL Tools</a></span></dt><dt><span class="section"><a href="ops_mgt.html#compression.tool">15.1.6. Compression Tool</a></span></dt><dt><span class="section"><a href="ops_mgt.html#copytable">15.1.7. CopyTable</a></span></dt><dt><span class="section"><a href="ops_mgt.html#export">15.1.8. Export</a></span></dt><dt><span class="section"><a href="ops_mgt.html#import">15.1.9. Import</a></span></dt><dt><span class="section"><a href="ops_mgt.html#importtsv">15.1.10. ImportTsv</a></span></dt><dt><span class="section"><a href="ops_mgt.html#completebulkload">15.1.11. CompleteBulkLoad</a></span></dt><dt><span class="section"><a href="ops_mgt.html#walplayer">15.1.12. WALPlayer</a></span></dt><dt><span class="section"><a href="ops_mgt.html#rowcounter">15.1.13. RowCounter and CellCounter</a></span></dt><dt><span class="section"><a href="ops_mgt.html#mlockall">15.1.14. mlockall</a></span></dt><dt><span class="section"><a href="ops_mgt.html#compaction.tool">15.1.15. Offline Compaction Tool</a></span></dt></dl></dd><dt><span class="section"><a href="ops.regionmgt.html">15.2. Region Management</a></span></dt><dd><dl><dt><span class="section"><a href="ops.regionmgt.html#ops.regionmgt.majorcompact">15.2.1. Major Compaction</a></span></dt><dt><span class="section"><a href="ops.regionmgt.html#ops.regionmgt.merge">15.2.2. Merge</a></span></dt></dl></dd><dt><span class="section"><a href="node.management.html">15.3. Node Management</a></span></dt><dd><dl><dt><span class="section"><a href="node.management.html#decommission">15.3.1. Node Decommission</a></span></dt><dt><span class="section"><a href="node.management.html#rolling">15.3.2. Rolling Restart</a></span></dt><dt><span class="section"><a href="node.management.html#adding.new.node">15.3.3. Adding a New Node</a></span></dt></dl></dd><dt><span class="section"><a href="hbase_metrics.html">15.4. HBase Metrics</a></span></dt><dd><dl><dt><span class="section"><a href="hbase_metrics.html#metric_setup">15.4.1. Metric Setup</a></span></dt><dt><span class="section"><a href="hbase_metrics.html#rs_metrics_ganglia">15.4.2. Warning To Ganglia Users</a></span></dt><dt><span class="section"><a href="hbase_metrics.html#rs_metrics">15.4.3. Most Important RegionServer Metrics</a></span></dt><dt><span class="section"><a href="hbase_metrics.html#rs_metrics_other">15.4.4. Other RegionServer Metrics</a></span></dt></dl></dd><dt><span class="section"><a href="ops.monitoring.html">15.5. HBase Monitoring</a></span></dt><dd><dl><dt><span class="section"><a href="ops.monitoring.html#ops.monitoring.overview">15.5.1. Overview</a></span></dt><dt><span class="section"><a href="ops.monitoring.html#ops.slow.query">15.5.2. Slow Query Log</a></span></dt></dl></dd><dt><span class="section"><a href="cluster_replication.html">15.6. Cluster Replication</a></span></dt><dt><span class="section"><a href="ops.backup.html">15.7. HBase Backup</a></span></dt><dd><dl><dt><span class="section"><a href="ops.backup.html#ops.backup.fullshutdown">15.7.1. Full Shutdown Backup</a></span></dt><dt><span class="section"><a href="ops.backup.html#ops.backup.live.replication">15.7.2. Live Cluster Backup - Replication</a></span></dt><dt><span class="section"><a href="ops.backup.html#ops.backup.live.copytable">15.7.3. Live Cluster Backup - CopyTable</a></span></dt><dt><span class="section"><a href="ops.backup.html#ops.backup.live.export">15.7.4. Live Cluster Backup - Export</a></span></dt></dl></dd><dt><span class="section"><a href="ops.snapshots.html">15.8. HBase Snapshots</a></span></dt><dd><dl><dt><span class="section"><a href="ops.snapshots.html#ops.snapshots.configuration">15.8.1. Configuration</a></span></dt><dt><span class="section"><a href="ops.snapshots.html#ops.snapshots.takeasnapshot">15.8.2. Take a Snapshot</a></span></dt><dt><span class="section"><a href="ops.snapshots.html#ops.snapshots.list">15.8.3. Listing Snapshots</a></span></dt><dt><span class="section"><a href="ops.snapshots.html#ops.snapshots.delete">15.8.4. Deleting Snapshots</a></span></dt><dt><span class="section"><a href="ops.snapshots.html#ops.snapshots.clone">15.8.5. Clone a table from snapshot</a></span></dt><dt><span class="section"><a href="ops.snapshots.html#ops.snapshots.restore">15.8.6. Restore a snapshot</a></span></dt><dt><span class="section"><a href="ops.snapshots.html#ops.snapshots.acls">15.8.7. Snapshots operations and ACLs</a></span></dt><dt><span class="section"><a href="ops.snapshots.html#ops.snapshots.export">15.8.8. Export to another cluster</a></span></dt></dl></dd><dt><span class="section"><a href="ops.capacity.html">15.9. Capacity Planning and Region Sizing</a></span></dt><dd><dl><dt><span class="section"><a href="ops.capacity.html#ops.capacity.nodes">15.9.1. Node count and hardware/VM configuration</a></span></dt><dt><span class="section"><a href="ops.capacity.html#ops.capacity.regions">15.9.2. Determining region count and size</a></span></dt><dt><span class="section"><a href="ops.capacity.html#ops.capacity.config">15.9.3. Initial configuration and tuning</a></span></dt></dl></dd><dt><span class="section"><a href="table.rename.html">15.10. Table Rename</a></span></dt></dl></div>
  This chapter will cover operational tools and practices required of a running Apache HBase cluster.
  The subject of operations is related to the topics of <a class="xref" href="trouble.html" title="Chapter&nbsp;13.&nbsp;Troubleshooting and Debugging Apache HBase">Chapter&nbsp;13, <i>Troubleshooting and Debugging Apache HBase</i></a>, <a class="xref" href="performance.html" title="Chapter&nbsp;12.&nbsp;Apache HBase Performance Tuning">Chapter&nbsp;12, <i>Apache HBase Performance Tuning</i></a>,
  and <a class="xref" href="configuration.html" title="Chapter&nbsp;2.&nbsp;Apache HBase Configuration">Chapter&nbsp;2, <i>Apache HBase Configuration</i></a> but is a distinct topic in itself.

  <div class="section" title="15.1.&nbsp;HBase Tools and Utilities"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="tools"></a>15.1.&nbsp;HBase Tools and Utilities</h2></div></div></div><p>Here we list HBase tools for administration, analysis, fixup, and
    debugging.</p><div class="section" title="15.1.1.&nbsp;Health Checker"><div class="titlepage"><div><div><h3 class="title"><a name="health.check"></a>15.1.1.&nbsp;Health Checker</h3></div></div></div><p>You can configure HBase to run a script on a period and if it fails N times (configurable), have the server exit.
            See HBASE-7351 Periodic health check script for configurations and detail.
        </p></div><div class="section" title="15.1.2.&nbsp;Driver"><div class="titlepage"><div><div><h3 class="title"><a name="driver"></a>15.1.2.&nbsp;Driver</h3></div></div></div><p>There is a <code class="code">Driver</code> class that is executed by the HBase jar can be used to invoke frequently accessed utilities.  For example,
</p><pre class="programlisting">HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar
</pre><p>
... will return...
</p><pre class="programlisting">
An example program must be given as the first argument.
Valid program names are:
  completebulkload: Complete a bulk data load.
  copytable: Export a table from local cluster to peer cluster
  export: Write table data to HDFS.
  import: Import data written by Export.
  importtsv: Import data in TSV format.
  rowcounter: Count rows in HBase table
  verifyrep: Compare the data from tables in two different clusters. WARNING: It doesn't work for incrementColumnValues'd cells since the timestamp is chan
</pre><p>
... for allowable program names.
      </p></div><div class="section" title="15.1.3.&nbsp;HBase hbck"><div class="titlepage"><div><div><h3 class="title"><a name="hbck"></a>15.1.3.&nbsp;HBase <span class="application">hbck</span></h3></div><div><h4 class="subtitle">An <span class="emphasis"><em>fsck</em></span> for your HBase install</h4></div></div></div><p>To run <span class="application">hbck</span> against your HBase cluster run
        </p><pre class="programlisting">$ ./bin/hbase hbck</pre><p>
        At the end of the commands output it prints <span class="emphasis"><em>OK</em></span>
        or <span class="emphasis"><em>INCONSISTENCY</em></span>. If your cluster reports
        inconsistencies, pass <span class="command"><strong>-details</strong></span> to see more detail emitted.
        If inconsistencies, run <span class="command"><strong>hbck</strong></span> a few times because the
        inconsistency may be transient (e.g. cluster is starting up or a region is
        splitting).
        Passing <span class="command"><strong>-fix</strong></span> may correct the inconsistency (This latter
        is an experimental feature).
        </p><p>For more information, see <a class="xref" href="hbck.in.depth.html" title="Appendix&nbsp;B.&nbsp;hbck In Depth">Appendix&nbsp;B, <i>hbck In Depth</i></a>.
        </p></div><div class="section" title="15.1.4.&nbsp;HFile Tool"><div class="titlepage"><div><div><h3 class="title"><a name="hfile_tool2"></a>15.1.4.&nbsp;HFile Tool</h3></div></div></div><p>See <a class="xref" href="regions.arch.html#hfile_tool" title="9.7.6.2.2.&nbsp;HFile Tool">Section&nbsp;9.7.6.2.2, &#8220;HFile Tool&#8221;</a>.</p></div><div class="section" title="15.1.5.&nbsp;WAL Tools"><div class="titlepage"><div><div><h3 class="title"><a name="wal_tools"></a>15.1.5.&nbsp;WAL Tools</h3></div></div></div><div class="section" title="15.1.5.1.&nbsp;FSHLog tool"><div class="titlepage"><div><div><h4 class="title"><a name="hlog_tool"></a>15.1.5.1.&nbsp;<code class="classname">FSHLog</code> tool</h4></div></div></div><p>The main method on <code class="classname">FSHLog</code> offers manual
        split and dump facilities. Pass it WALs or the product of a split, the
        content of the <code class="filename">recovered.edits</code>. directory.</p><p>You can get a textual dump of a WAL file content by doing the
        following:</p><pre class="programlisting"> <code class="code">$ ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.FSHLog --dump hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/10.10.21.10%3A60020.1283973724012</code> </pre><p>The
        return code will be non-zero if issues with the file so you can test
        wholesomeness of file by redirecting <code class="varname">STDOUT</code> to
        <code class="code">/dev/null</code> and testing the program return.</p><p>Similarly you can force a split of a log file directory by
        doing:</p><pre class="programlisting"> $ ./<code class="code">bin/hbase org.apache.hadoop.hbase.regionserver.wal.FSHLog --split hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/</code></pre><div class="section" title="15.1.5.1.1.&nbsp;HLogPrettyPrinter"><div class="titlepage"><div><div><h5 class="title"><a name="hlog_tool.prettyprint"></a>15.1.5.1.1.&nbsp;<code class="classname">HLogPrettyPrinter</code></h5></div></div></div><p><code class="classname">HLogPrettyPrinter</code> is a tool with configurable options to print the contents of an HLog.
          </p></div></div></div><div class="section" title="15.1.6.&nbsp;Compression Tool"><div class="titlepage"><div><div><h3 class="title"><a name="compression.tool"></a>15.1.6.&nbsp;Compression Tool</h3></div></div></div><p>See <a class="xref" href="compression.html#compression.test" title="C.1.&nbsp;CompressionTest Tool">Section&nbsp;C.1, &#8220;CompressionTest Tool&#8221;</a>.</p></div><div class="section" title="15.1.7.&nbsp;CopyTable"><div class="titlepage"><div><div><h3 class="title"><a name="copytable"></a>15.1.7.&nbsp;CopyTable</h3></div></div></div><p>
            CopyTable is a utility that can copy part or of all of a table, either to the same cluster or another cluster. The usage is as follows:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable [--starttime=X] [--endtime=Y] [--new.name=NEW] [--peer.adr=ADR] tablename
</pre><p>
        </p><p>
        Options:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="varname">starttime</code>  Beginning of the time range.  Without endtime means starttime to forever.</li><li class="listitem"><code class="varname">endtime</code>  End of the time range.  Without endtime means starttime to forever.</li><li class="listitem"><code class="varname">versions</code>  Number of cell versions to copy.</li><li class="listitem"><code class="varname">new.name</code>  New table's name.</li><li class="listitem"><code class="varname">peer.adr</code>  Address of the peer cluster given in the format hbase.zookeeper.quorum:hbase.zookeeper.client.port:zookeeper.znode.parent</li><li class="listitem"><code class="varname">families</code>  Comma-separated list of ColumnFamilies to copy.</li><li class="listitem"><code class="varname">all.cells</code>  Also copy delete markers and uncollected deleted cells (advanced option).</li></ul></div><p>
         Args:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">tablename  Name of table to copy.</li></ul></div><p>
        </p><p>Example of copying 'TestTable' to a cluster that uses replication for a 1 hour window:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable
--starttime=1265875194289 --endtime=1265878794289
--peer.adr=server1,server2,server3:2181:/hbase TestTable</pre><p>
        </p><div class="note" title="Scanner Caching" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Scanner Caching</h3><p>Caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
        </p></div><p>
        See Jonathan Hsieh's <a class="link" href="http://www.cloudera.com/blog/2012/06/online-hbase-backups-with-copytable-2/" target="_top">Online HBase Backups with CopyTable</a> blog post for more on <span class="command"><strong>CopyTable</strong></span>.
        </p></div><div class="section" title="15.1.8.&nbsp;Export"><div class="titlepage"><div><div><h3 class="title"><a name="export"></a>15.1.8.&nbsp;Export</h3></div></div></div><p>Export is a utility that will dump the contents of table to HDFS in a sequence file.  Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]
</pre><p>
       </p><p>Note:  caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
        </p></div><div class="section" title="15.1.9.&nbsp;Import"><div class="titlepage"><div><div><h3 class="title"><a name="import"></a>15.1.9.&nbsp;Import</h3></div></div></div><p>Import is a utility that will load data that has been exported back into HBase.  Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;
</pre><p>
       </p></div><div class="section" title="15.1.10.&nbsp;ImportTsv"><div class="titlepage"><div><div><h3 class="title"><a name="importtsv"></a>15.1.10.&nbsp;ImportTsv</h3></div></div></div><p>ImportTsv is a utility that will load data in TSV format into HBase.  It has two distinct usages:  loading data from TSV format in HDFS
       into HBase via Puts, and preparing StoreFiles to be loaded via the <code class="code">completebulkload</code>.
       </p><p>To load data via Puts (i.e., non-bulk loading):
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;hdfs-inputdir&gt;
</pre><p>
       </p><p>To generate StoreFiles for bulk-loading:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c -Dimporttsv.bulk.output=hdfs://storefile-outputdir &lt;tablename&gt; &lt;hdfs-data-inputdir&gt;
</pre><p>
       </p><p>These generated StoreFiles can be loaded into HBase via <a class="xref" href="ops_mgt.html#completebulkload" title="15.1.11.&nbsp;CompleteBulkLoad">Section&nbsp;15.1.11, &#8220;CompleteBulkLoad&#8221;</a>.
       </p><div class="section" title="15.1.10.1.&nbsp;ImportTsv Options"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.options"></a>15.1.10.1.&nbsp;ImportTsv Options</h4></div></div></div>
       Running ImportTsv with no arguments prints brief usage information:
<pre class="programlisting">
Usage: importtsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;inputdir&gt;

Imports the given input directory of TSV data into the specified table.

The column names of the TSV data must be specified using the -Dimporttsv.columns
option. This option takes the form of comma-separated column names, where each
column name is either a simple column family, or a columnfamily:qualifier. The special
column name HBASE_ROW_KEY is used to designate that this column should be used
as the row key for each imported record. You must specify exactly one column
to be the row key, and you must specify a column name for every column that exists in the
input data.

By default importtsv will load data directly into HBase. To instead generate
HFiles of data to prepare for a bulk data load, pass the option:
  -Dimporttsv.bulk.output=/path/for/output
  Note: the target table will be created with default column family descriptors if it does not already exist.

Other options that may be specified with -D include:
  -Dimporttsv.skip.bad.lines=false - fail if encountering an invalid line
  '-Dimporttsv.separator=|' - eg separate on pipes instead of tabs
  -Dimporttsv.timestamp=currentTimeAsLong - use the specified timestamp for the import
  -Dimporttsv.mapper.class=my.Mapper - A user-defined Mapper to use instead of org.apache.hadoop.hbase.mapreduce.TsvImporterMapper
</pre></div><div class="section" title="15.1.10.2.&nbsp;ImportTsv Example"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.example"></a>15.1.10.2.&nbsp;ImportTsv Example</h4></div></div></div><p>For example, assume that we are loading data into a table called 'datatsv' with a ColumnFamily called 'd' with two columns "c1" and "c2".
         </p><p>Assume that an input file exists as follows:
</p><pre class="programlisting">
row1	c1	c2
row2	c1	c2
row3	c1	c2
row4	c1	c2
row5	c1	c2
row6	c1	c2
row7	c1	c2
row8	c1	c2
row9	c1	c2
row10	c1	c2
</pre><p>
         </p><p>For ImportTsv to use this imput file, the command line needs to look like this:
 </p><pre class="programlisting">
 HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar importtsv -Dimporttsv.columns=HBASE_ROW_KEY,d:c1,d:c2 -Dimporttsv.bulk.output=hdfs://storefileoutput datatsv hdfs://inputfile
 </pre><p>
         ... and in this example the first column is the rowkey, which is why the HBASE_ROW_KEY is used.  The second and third columns in the file will be imported as "d:c1" and "d:c2", respectively.
         </p></div><div class="section" title="15.1.10.3.&nbsp;ImportTsv Warning"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.warning"></a>15.1.10.3.&nbsp;ImportTsv Warning</h4></div></div></div><p>If you have preparing a lot of data for bulk loading, make sure the target HBase table is pre-split appropriately.
         </p></div><div class="section" title="15.1.10.4.&nbsp;See Also"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.also"></a>15.1.10.4.&nbsp;See Also</h4></div></div></div>
       For more information about bulk-loading HFiles into HBase, see <a class="xref" href="arch.bulk.load.html" title="9.8.&nbsp;Bulk Loading">Section&nbsp;9.8, &#8220;Bulk Loading&#8221;</a></div></div><div class="section" title="15.1.11.&nbsp;CompleteBulkLoad"><div class="titlepage"><div><div><h3 class="title"><a name="completebulkload"></a>15.1.11.&nbsp;CompleteBulkLoad</h3></div></div></div><p>The <code class="code">completebulkload</code> utility will move generated StoreFiles into an HBase table.  This utility is often used
	   in conjunction with output from <a class="xref" href="ops_mgt.html#importtsv" title="15.1.10.&nbsp;ImportTsv">Section&nbsp;15.1.10, &#8220;ImportTsv&#8221;</a>.
	   </p><p>There are two ways to invoke this utility, with explicit classname and via the driver:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles &lt;hdfs://storefileoutput&gt; &lt;tablename&gt;
</pre><p>
.. and via the Driver..
</p><pre class="programlisting">HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar completebulkload &lt;hdfs://storefileoutput&gt; &lt;tablename&gt;
</pre><p>
	  </p><div class="section" title="15.1.11.1.&nbsp;CompleteBulkLoad Warning"><div class="titlepage"><div><div><h4 class="title"><a name="completebulkload.warning"></a>15.1.11.1.&nbsp;CompleteBulkLoad Warning</h4></div></div></div><p>Data generated via MapReduce is often created with file permissions that are not compatible with the running HBase process. Assuming you're running HDFS with permissions enabled, those permissions will need to be updated before you run CompleteBulkLoad.
          </p></div><p>For more information about bulk-loading HFiles into HBase, see <a class="xref" href="arch.bulk.load.html" title="9.8.&nbsp;Bulk Loading">Section&nbsp;9.8, &#8220;Bulk Loading&#8221;</a>.
       </p></div><div class="section" title="15.1.12.&nbsp;WALPlayer"><div class="titlepage"><div><div><h3 class="title"><a name="walplayer"></a>15.1.12.&nbsp;WALPlayer</h3></div></div></div><p>WALPlayer is a utility to replay WAL files into HBase.
       </p><p>The WAL can be replayed for a set of tables or all tables, and a
           timerange can be provided (in milliseconds). The WAL is filtered to
           this set of tables. The output can optionally be mapped to another set of tables.
       </p><p>WALPlayer can also generate HFiles for later bulk importing, in that case
           only a single table and no mapping can be specified.
       </p><p>Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer [options] &lt;wal inputdir&gt; &lt;tables&gt; [&lt;tableMappings&gt;]&gt;
</pre><p>
       </p><p>For example:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer /backuplogdir oldTable1,oldTable2 newTable1,newTable2
</pre><p>
       </p><p>
           WALPlayer, by default, runs as a mapreduce job.  To NOT run WALPlayer as a mapreduce job on your cluster,
           force it to run all in the local process by adding the flags <code class="code">-Dmapred.job.tracker=local</code> on the command line.
       </p></div><div class="section" title="15.1.13.&nbsp;RowCounter and CellCounter"><div class="titlepage"><div><div><h3 class="title"><a name="rowcounter"></a>15.1.13.&nbsp;RowCounter and CellCounter</h3></div></div></div><p><a class="ulink" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/RowCounter.html" target="_top">RowCounter</a> is a
       mapreduce job to count all the rows of a table.  This is a good utility to use as a sanity check to ensure that HBase can read
       all the blocks of a table if there are any concerns of metadata inconsistency. It will run the mapreduce all in a single
       process but it will run faster if you have a MapReduce cluster in place for it to exploit.
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.RowCounter &lt;tablename&gt; [&lt;column1&gt; &lt;column2&gt;...]
</pre><p>
       </p><p>Note: caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
       </p><p>HBase ships another diagnostic mapreduce job called
         <a class="ulink" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/CellCounter.html" target="_top">CellCounter</a>. Like
         RowCounter, it gathers more fine-grained statistics about your table. The statistics gathered by RowCounter are more fine-grained
         and include:
         </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Total number of rows in the table.</li><li class="listitem">Total number of CFs across all rows.</li><li class="listitem">Total qualifiers across all rows.</li><li class="listitem">Total occurrence of each CF.</li><li class="listitem">Total occurrence of each qualifier.</li><li class="listitem">Total number of versions of each qualifier.</li></ul></div><p>
       </p><p>The program allows you to limit the scope of the run. Provide a row regex or prefix to limit the rows to analyze. Use
         <code class="code">hbase.mapreduce.scan.column.family</code> to specify scanning a single column family.
         </p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CellCounter &lt;tablename&gt; &lt;outputDir&gt; [regex or prefix]</pre><p>
       </p><p>Note: just like RowCounter, caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the
       job configuration. </p></div><div class="section" title="15.1.14.&nbsp;mlockall"><div class="titlepage"><div><div><h3 class="title"><a name="mlockall"></a>15.1.14.&nbsp;mlockall</h3></div></div></div><p>It is possible to optionally pin your servers in physical memory making them less likely
            to be swapped out in oversubscribed environments by having the servers call
            <a class="link" href="http://linux.die.net/man/2/mlockall" target="_top">mlockall</a> on startup.
            See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-4391" target="_top">HBASE-4391 Add ability to start RS as root and call mlockall</a>
            for how to build the optional library and have it run on startup.
        </p></div><div class="section" title="15.1.15.&nbsp;Offline Compaction Tool"><div class="titlepage"><div><div><h3 class="title"><a name="compaction.tool"></a>15.1.15.&nbsp;Offline Compaction Tool</h3></div></div></div><p>See the usage for the <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/CompactionTool.html" target="_top">Compaction Tool</a>.
            Run it like this <span class="command"><strong>./bin/hbase org.apache.hadoop.hbase.regionserver.CompactionTool</strong></span>
        </p></div></div></div><div id="disqus_thread"></div><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book';
    var disqus_identifier = 'ops_mgt';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="casestudies.perftroub.html">Prev</a>&nbsp;</td><td width="20%" align="center">&nbsp;</td><td width="40%" align="right">&nbsp;<a accesskey="n" href="ops.regionmgt.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">14.3.&nbsp;Performance/Troubleshooting&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="book.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;15.2.&nbsp;Region Management</td></tr></table></div></body></html>