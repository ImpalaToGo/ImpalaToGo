<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>Chapter&nbsp;1.&nbsp;Apache HBase Operational Management</title><link rel="stylesheet" type="text/css" href="css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="chapter" title="Chapter&nbsp;1.&nbsp;Apache HBase Operational Management"><div class="titlepage"><div><div><h2 class="title"><a name="ops_mgt"></a>Chapter&nbsp;1.&nbsp;Apache HBase Operational Management</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#tools">1.1. HBase Tools and Utilities</a></span></dt><dd><dl><dt><span class="section"><a href="#health.check">1.1.1. Health Checker</a></span></dt><dt><span class="section"><a href="#driver">1.1.2. Driver</a></span></dt><dt><span class="section"><a href="#hbck">1.1.3. HBase <span class="application">hbck</span></a></span></dt><dt><span class="section"><a href="#hfile_tool2">1.1.4. HFile Tool</a></span></dt><dt><span class="section"><a href="#wal_tools">1.1.5. WAL Tools</a></span></dt><dt><span class="section"><a href="#compression.tool">1.1.6. Compression Tool</a></span></dt><dt><span class="section"><a href="#copytable">1.1.7. CopyTable</a></span></dt><dt><span class="section"><a href="#export">1.1.8. Export</a></span></dt><dt><span class="section"><a href="#import">1.1.9. Import</a></span></dt><dt><span class="section"><a href="#importtsv">1.1.10. ImportTsv</a></span></dt><dt><span class="section"><a href="#completebulkload">1.1.11. CompleteBulkLoad</a></span></dt><dt><span class="section"><a href="#walplayer">1.1.12. WALPlayer</a></span></dt><dt><span class="section"><a href="#rowcounter">1.1.13. RowCounter and CellCounter</a></span></dt><dt><span class="section"><a href="#mlockall">1.1.14. mlockall</a></span></dt><dt><span class="section"><a href="#compaction.tool">1.1.15. Offline Compaction Tool</a></span></dt></dl></dd><dt><span class="section"><a href="#ops.regionmgt">1.2. Region Management</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.regionmgt.majorcompact">1.2.1. Major Compaction</a></span></dt><dt><span class="section"><a href="#ops.regionmgt.merge">1.2.2. Merge</a></span></dt></dl></dd><dt><span class="section"><a href="#node.management">1.3. Node Management</a></span></dt><dd><dl><dt><span class="section"><a href="#decommission">1.3.1. Node Decommission</a></span></dt><dt><span class="section"><a href="#rolling">1.3.2. Rolling Restart</a></span></dt><dt><span class="section"><a href="#adding.new.node">1.3.3. Adding a New Node</a></span></dt></dl></dd><dt><span class="section"><a href="#hbase_metrics">1.4. HBase Metrics</a></span></dt><dd><dl><dt><span class="section"><a href="#metric_setup">1.4.1. Metric Setup</a></span></dt><dt><span class="section"><a href="#rs_metrics_ganglia">1.4.2. Warning To Ganglia Users</a></span></dt><dt><span class="section"><a href="#rs_metrics">1.4.3. Most Important RegionServer Metrics</a></span></dt><dt><span class="section"><a href="#rs_metrics_other">1.4.4. Other RegionServer Metrics</a></span></dt></dl></dd><dt><span class="section"><a href="#ops.monitoring">1.5. HBase Monitoring</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.monitoring.overview">1.5.1. Overview</a></span></dt><dt><span class="section"><a href="#ops.slow.query">1.5.2. Slow Query Log</a></span></dt></dl></dd><dt><span class="section"><a href="#cluster_replication">1.6. Cluster Replication</a></span></dt><dt><span class="section"><a href="#ops.backup">1.7. HBase Backup</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.backup.fullshutdown">1.7.1. Full Shutdown Backup</a></span></dt><dt><span class="section"><a href="#ops.backup.live.replication">1.7.2. Live Cluster Backup - Replication</a></span></dt><dt><span class="section"><a href="#ops.backup.live.copytable">1.7.3. Live Cluster Backup - CopyTable</a></span></dt><dt><span class="section"><a href="#ops.backup.live.export">1.7.4. Live Cluster Backup - Export</a></span></dt></dl></dd><dt><span class="section"><a href="#ops.snapshots">1.8. HBase Snapshots</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.snapshots.configuration">1.8.1. Configuration</a></span></dt><dt><span class="section"><a href="#ops.snapshots.takeasnapshot">1.8.2. Take a Snapshot</a></span></dt><dt><span class="section"><a href="#ops.snapshots.list">1.8.3. Listing Snapshots</a></span></dt><dt><span class="section"><a href="#ops.snapshots.delete">1.8.4. Deleting Snapshots</a></span></dt><dt><span class="section"><a href="#ops.snapshots.clone">1.8.5. Clone a table from snapshot</a></span></dt><dt><span class="section"><a href="#ops.snapshots.restore">1.8.6. Restore a snapshot</a></span></dt><dt><span class="section"><a href="#ops.snapshots.acls">1.8.7. Snapshots operations and ACLs</a></span></dt><dt><span class="section"><a href="#ops.snapshots.export">1.8.8. Export to another cluster</a></span></dt></dl></dd><dt><span class="section"><a href="#ops.capacity">1.9. Capacity Planning and Region Sizing</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.capacity.nodes">1.9.1. Node count and hardware/VM configuration</a></span></dt><dt><span class="section"><a href="#ops.capacity.regions">1.9.2. Determining region count and size</a></span></dt><dt><span class="section"><a href="#ops.capacity.config">1.9.3. Initial configuration and tuning</a></span></dt></dl></dd><dt><span class="section"><a href="#table.rename">1.10. Table Rename</a></span></dt></dl></div>
  This chapter will cover operational tools and practices required of a running Apache HBase cluster.
  The subject of operations is related to the topics of <a class="xref" href="#">???</a>, <a class="xref" href="#">???</a>,
  and <a class="xref" href="#">???</a> but is a distinct topic in itself.

  <div class="section" title="1.1.&nbsp;HBase Tools and Utilities"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="tools"></a>1.1.&nbsp;HBase Tools and Utilities</h2></div></div></div><p>Here we list HBase tools for administration, analysis, fixup, and
    debugging.</p><div class="section" title="1.1.1.&nbsp;Health Checker"><div class="titlepage"><div><div><h3 class="title"><a name="health.check"></a>1.1.1.&nbsp;Health Checker</h3></div></div></div><p>You can configure HBase to run a script on a period and if it fails N times (configurable), have the server exit.
            See HBASE-7351 Periodic health check script for configurations and detail.
        </p></div><div class="section" title="1.1.2.&nbsp;Driver"><div class="titlepage"><div><div><h3 class="title"><a name="driver"></a>1.1.2.&nbsp;Driver</h3></div></div></div><p>There is a <code class="code">Driver</code> class that is executed by the HBase jar can be used to invoke frequently accessed utilities.  For example,
</p><pre class="programlisting">HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar
</pre><p>
... will return...
</p><pre class="programlisting">
An example program must be given as the first argument.
Valid program names are:
  completebulkload: Complete a bulk data load.
  copytable: Export a table from local cluster to peer cluster
  export: Write table data to HDFS.
  import: Import data written by Export.
  importtsv: Import data in TSV format.
  rowcounter: Count rows in HBase table
  verifyrep: Compare the data from tables in two different clusters. WARNING: It doesn't work for incrementColumnValues'd cells since the timestamp is chan
</pre><p>
... for allowable program names.
      </p></div><div class="section" title="1.1.3.&nbsp;HBase hbck"><div class="titlepage"><div><div><h3 class="title"><a name="hbck"></a>1.1.3.&nbsp;HBase <span class="application">hbck</span></h3></div><div><h4 class="subtitle">An <span class="emphasis"><em>fsck</em></span> for your HBase install</h4></div></div></div><p>To run <span class="application">hbck</span> against your HBase cluster run
        </p><pre class="programlisting">$ ./bin/hbase hbck</pre><p>
        At the end of the commands output it prints <span class="emphasis"><em>OK</em></span>
        or <span class="emphasis"><em>INCONSISTENCY</em></span>. If your cluster reports
        inconsistencies, pass <span class="command"><strong>-details</strong></span> to see more detail emitted.
        If inconsistencies, run <span class="command"><strong>hbck</strong></span> a few times because the
        inconsistency may be transient (e.g. cluster is starting up or a region is
        splitting).
        Passing <span class="command"><strong>-fix</strong></span> may correct the inconsistency (This latter
        is an experimental feature).
        </p><p>For more information, see <a class="xref" href="#">???</a>.
        </p></div><div class="section" title="1.1.4.&nbsp;HFile Tool"><div class="titlepage"><div><div><h3 class="title"><a name="hfile_tool2"></a>1.1.4.&nbsp;HFile Tool</h3></div></div></div><p>See <a class="xref" href="#">???</a>.</p></div><div class="section" title="1.1.5.&nbsp;WAL Tools"><div class="titlepage"><div><div><h3 class="title"><a name="wal_tools"></a>1.1.5.&nbsp;WAL Tools</h3></div></div></div><div class="section" title="1.1.5.1.&nbsp;FSHLog tool"><div class="titlepage"><div><div><h4 class="title"><a name="hlog_tool"></a>1.1.5.1.&nbsp;<code class="classname">FSHLog</code> tool</h4></div></div></div><p>The main method on <code class="classname">FSHLog</code> offers manual
        split and dump facilities. Pass it WALs or the product of a split, the
        content of the <code class="filename">recovered.edits</code>. directory.</p><p>You can get a textual dump of a WAL file content by doing the
        following:</p><pre class="programlisting"> <code class="code">$ ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.FSHLog --dump hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/10.10.21.10%3A60020.1283973724012</code> </pre><p>The
        return code will be non-zero if issues with the file so you can test
        wholesomeness of file by redirecting <code class="varname">STDOUT</code> to
        <code class="code">/dev/null</code> and testing the program return.</p><p>Similarly you can force a split of a log file directory by
        doing:</p><pre class="programlisting"> $ ./<code class="code">bin/hbase org.apache.hadoop.hbase.regionserver.wal.FSHLog --split hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/</code></pre><div class="section" title="1.1.5.1.1.&nbsp;HLogPrettyPrinter"><div class="titlepage"><div><div><h5 class="title"><a name="hlog_tool.prettyprint"></a>1.1.5.1.1.&nbsp;<code class="classname">HLogPrettyPrinter</code></h5></div></div></div><p><code class="classname">HLogPrettyPrinter</code> is a tool with configurable options to print the contents of an HLog.
          </p></div></div></div><div class="section" title="1.1.6.&nbsp;Compression Tool"><div class="titlepage"><div><div><h3 class="title"><a name="compression.tool"></a>1.1.6.&nbsp;Compression Tool</h3></div></div></div><p>See <a class="xref" href="#">???</a>.</p></div><div class="section" title="1.1.7.&nbsp;CopyTable"><div class="titlepage"><div><div><h3 class="title"><a name="copytable"></a>1.1.7.&nbsp;CopyTable</h3></div></div></div><p>
            CopyTable is a utility that can copy part or of all of a table, either to the same cluster or another cluster. The usage is as follows:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable [--starttime=X] [--endtime=Y] [--new.name=NEW] [--peer.adr=ADR] tablename
</pre><p>
        </p><p>
        Options:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="varname">starttime</code>  Beginning of the time range.  Without endtime means starttime to forever.</li><li class="listitem"><code class="varname">endtime</code>  End of the time range.  Without endtime means starttime to forever.</li><li class="listitem"><code class="varname">versions</code>  Number of cell versions to copy.</li><li class="listitem"><code class="varname">new.name</code>  New table's name.</li><li class="listitem"><code class="varname">peer.adr</code>  Address of the peer cluster given in the format hbase.zookeeper.quorum:hbase.zookeeper.client.port:zookeeper.znode.parent</li><li class="listitem"><code class="varname">families</code>  Comma-separated list of ColumnFamilies to copy.</li><li class="listitem"><code class="varname">all.cells</code>  Also copy delete markers and uncollected deleted cells (advanced option).</li></ul></div><p>
         Args:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">tablename  Name of table to copy.</li></ul></div><p>
        </p><p>Example of copying 'TestTable' to a cluster that uses replication for a 1 hour window:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable
--starttime=1265875194289 --endtime=1265878794289
--peer.adr=server1,server2,server3:2181:/hbase TestTable</pre><p>
        </p><div class="note" title="Scanner Caching" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Scanner Caching</h3><p>Caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
        </p></div><p>
        See Jonathan Hsieh's <a class="link" href="http://www.cloudera.com/blog/2012/06/online-hbase-backups-with-copytable-2/" target="_top">Online HBase Backups with CopyTable</a> blog post for more on <span class="command"><strong>CopyTable</strong></span>.
        </p></div><div class="section" title="1.1.8.&nbsp;Export"><div class="titlepage"><div><div><h3 class="title"><a name="export"></a>1.1.8.&nbsp;Export</h3></div></div></div><p>Export is a utility that will dump the contents of table to HDFS in a sequence file.  Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]
</pre><p>
       </p><p>Note:  caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
        </p></div><div class="section" title="1.1.9.&nbsp;Import"><div class="titlepage"><div><div><h3 class="title"><a name="import"></a>1.1.9.&nbsp;Import</h3></div></div></div><p>Import is a utility that will load data that has been exported back into HBase.  Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;
</pre><p>
       </p></div><div class="section" title="1.1.10.&nbsp;ImportTsv"><div class="titlepage"><div><div><h3 class="title"><a name="importtsv"></a>1.1.10.&nbsp;ImportTsv</h3></div></div></div><p>ImportTsv is a utility that will load data in TSV format into HBase.  It has two distinct usages:  loading data from TSV format in HDFS
       into HBase via Puts, and preparing StoreFiles to be loaded via the <code class="code">completebulkload</code>.
       </p><p>To load data via Puts (i.e., non-bulk loading):
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;hdfs-inputdir&gt;
</pre><p>
       </p><p>To generate StoreFiles for bulk-loading:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c -Dimporttsv.bulk.output=hdfs://storefile-outputdir &lt;tablename&gt; &lt;hdfs-data-inputdir&gt;
</pre><p>
       </p><p>These generated StoreFiles can be loaded into HBase via <a class="xref" href="#completebulkload" title="1.1.11.&nbsp;CompleteBulkLoad">Section&nbsp;1.1.11, &#8220;CompleteBulkLoad&#8221;</a>.
       </p><div class="section" title="1.1.10.1.&nbsp;ImportTsv Options"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.options"></a>1.1.10.1.&nbsp;ImportTsv Options</h4></div></div></div>
       Running ImportTsv with no arguments prints brief usage information:
<pre class="programlisting">
Usage: importtsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;inputdir&gt;

Imports the given input directory of TSV data into the specified table.

The column names of the TSV data must be specified using the -Dimporttsv.columns
option. This option takes the form of comma-separated column names, where each
column name is either a simple column family, or a columnfamily:qualifier. The special
column name HBASE_ROW_KEY is used to designate that this column should be used
as the row key for each imported record. You must specify exactly one column
to be the row key, and you must specify a column name for every column that exists in the
input data.

By default importtsv will load data directly into HBase. To instead generate
HFiles of data to prepare for a bulk data load, pass the option:
  -Dimporttsv.bulk.output=/path/for/output
  Note: the target table will be created with default column family descriptors if it does not already exist.

Other options that may be specified with -D include:
  -Dimporttsv.skip.bad.lines=false - fail if encountering an invalid line
  '-Dimporttsv.separator=|' - eg separate on pipes instead of tabs
  -Dimporttsv.timestamp=currentTimeAsLong - use the specified timestamp for the import
  -Dimporttsv.mapper.class=my.Mapper - A user-defined Mapper to use instead of org.apache.hadoop.hbase.mapreduce.TsvImporterMapper
</pre></div><div class="section" title="1.1.10.2.&nbsp;ImportTsv Example"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.example"></a>1.1.10.2.&nbsp;ImportTsv Example</h4></div></div></div><p>For example, assume that we are loading data into a table called 'datatsv' with a ColumnFamily called 'd' with two columns "c1" and "c2".
         </p><p>Assume that an input file exists as follows:
</p><pre class="programlisting">
row1	c1	c2
row2	c1	c2
row3	c1	c2
row4	c1	c2
row5	c1	c2
row6	c1	c2
row7	c1	c2
row8	c1	c2
row9	c1	c2
row10	c1	c2
</pre><p>
         </p><p>For ImportTsv to use this imput file, the command line needs to look like this:
 </p><pre class="programlisting">
 HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar importtsv -Dimporttsv.columns=HBASE_ROW_KEY,d:c1,d:c2 -Dimporttsv.bulk.output=hdfs://storefileoutput datatsv hdfs://inputfile
 </pre><p>
         ... and in this example the first column is the rowkey, which is why the HBASE_ROW_KEY is used.  The second and third columns in the file will be imported as "d:c1" and "d:c2", respectively.
         </p></div><div class="section" title="1.1.10.3.&nbsp;ImportTsv Warning"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.warning"></a>1.1.10.3.&nbsp;ImportTsv Warning</h4></div></div></div><p>If you have preparing a lot of data for bulk loading, make sure the target HBase table is pre-split appropriately.
         </p></div><div class="section" title="1.1.10.4.&nbsp;See Also"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.also"></a>1.1.10.4.&nbsp;See Also</h4></div></div></div>
       For more information about bulk-loading HFiles into HBase, see <a class="xref" href="#">???</a></div></div><div class="section" title="1.1.11.&nbsp;CompleteBulkLoad"><div class="titlepage"><div><div><h3 class="title"><a name="completebulkload"></a>1.1.11.&nbsp;CompleteBulkLoad</h3></div></div></div><p>The <code class="code">completebulkload</code> utility will move generated StoreFiles into an HBase table.  This utility is often used
	   in conjunction with output from <a class="xref" href="#importtsv" title="1.1.10.&nbsp;ImportTsv">Section&nbsp;1.1.10, &#8220;ImportTsv&#8221;</a>.
	   </p><p>There are two ways to invoke this utility, with explicit classname and via the driver:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles &lt;hdfs://storefileoutput&gt; &lt;tablename&gt;
</pre><p>
.. and via the Driver..
</p><pre class="programlisting">HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar completebulkload &lt;hdfs://storefileoutput&gt; &lt;tablename&gt;
</pre><p>
	  </p><div class="section" title="1.1.11.1.&nbsp;CompleteBulkLoad Warning"><div class="titlepage"><div><div><h4 class="title"><a name="completebulkload.warning"></a>1.1.11.1.&nbsp;CompleteBulkLoad Warning</h4></div></div></div><p>Data generated via MapReduce is often created with file permissions that are not compatible with the running HBase process. Assuming you're running HDFS with permissions enabled, those permissions will need to be updated before you run CompleteBulkLoad.
          </p></div><p>For more information about bulk-loading HFiles into HBase, see <a class="xref" href="#">???</a>.
       </p></div><div class="section" title="1.1.12.&nbsp;WALPlayer"><div class="titlepage"><div><div><h3 class="title"><a name="walplayer"></a>1.1.12.&nbsp;WALPlayer</h3></div></div></div><p>WALPlayer is a utility to replay WAL files into HBase.
       </p><p>The WAL can be replayed for a set of tables or all tables, and a
           timerange can be provided (in milliseconds). The WAL is filtered to
           this set of tables. The output can optionally be mapped to another set of tables.
       </p><p>WALPlayer can also generate HFiles for later bulk importing, in that case
           only a single table and no mapping can be specified.
       </p><p>Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer [options] &lt;wal inputdir&gt; &lt;tables&gt; [&lt;tableMappings&gt;]&gt;
</pre><p>
       </p><p>For example:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer /backuplogdir oldTable1,oldTable2 newTable1,newTable2
</pre><p>
       </p><p>
           WALPlayer, by default, runs as a mapreduce job.  To NOT run WALPlayer as a mapreduce job on your cluster,
           force it to run all in the local process by adding the flags <code class="code">-Dmapred.job.tracker=local</code> on the command line.
       </p></div><div class="section" title="1.1.13.&nbsp;RowCounter and CellCounter"><div class="titlepage"><div><div><h3 class="title"><a name="rowcounter"></a>1.1.13.&nbsp;RowCounter and CellCounter</h3></div></div></div><p><a class="ulink" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/RowCounter.html" target="_top">RowCounter</a> is a
       mapreduce job to count all the rows of a table.  This is a good utility to use as a sanity check to ensure that HBase can read
       all the blocks of a table if there are any concerns of metadata inconsistency. It will run the mapreduce all in a single
       process but it will run faster if you have a MapReduce cluster in place for it to exploit.
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.RowCounter &lt;tablename&gt; [&lt;column1&gt; &lt;column2&gt;...]
</pre><p>
       </p><p>Note: caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
       </p><p>HBase ships another diagnostic mapreduce job called
         <a class="ulink" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/CellCounter.html" target="_top">CellCounter</a>. Like
         RowCounter, it gathers more fine-grained statistics about your table. The statistics gathered by RowCounter are more fine-grained
         and include:
         </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Total number of rows in the table.</li><li class="listitem">Total number of CFs across all rows.</li><li class="listitem">Total qualifiers across all rows.</li><li class="listitem">Total occurrence of each CF.</li><li class="listitem">Total occurrence of each qualifier.</li><li class="listitem">Total number of versions of each qualifier.</li></ul></div><p>
       </p><p>The program allows you to limit the scope of the run. Provide a row regex or prefix to limit the rows to analyze. Use
         <code class="code">hbase.mapreduce.scan.column.family</code> to specify scanning a single column family.
         </p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CellCounter &lt;tablename&gt; &lt;outputDir&gt; [regex or prefix]</pre><p>
       </p><p>Note: just like RowCounter, caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the
       job configuration. </p></div><div class="section" title="1.1.14.&nbsp;mlockall"><div class="titlepage"><div><div><h3 class="title"><a name="mlockall"></a>1.1.14.&nbsp;mlockall</h3></div></div></div><p>It is possible to optionally pin your servers in physical memory making them less likely
            to be swapped out in oversubscribed environments by having the servers call
            <a class="link" href="http://linux.die.net/man/2/mlockall" target="_top">mlockall</a> on startup.
            See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-4391" target="_top">HBASE-4391 Add ability to start RS as root and call mlockall</a>
            for how to build the optional library and have it run on startup.
        </p></div><div class="section" title="1.1.15.&nbsp;Offline Compaction Tool"><div class="titlepage"><div><div><h3 class="title"><a name="compaction.tool"></a>1.1.15.&nbsp;Offline Compaction Tool</h3></div></div></div><p>See the usage for the <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/CompactionTool.html" target="_top">Compaction Tool</a>.
            Run it like this <span class="command"><strong>./bin/hbase org.apache.hadoop.hbase.regionserver.CompactionTool</strong></span>
        </p></div></div><div class="section" title="1.2.&nbsp;Region Management"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.regionmgt"></a>1.2.&nbsp;Region Management</h2></div></div></div><div class="section" title="1.2.1.&nbsp;Major Compaction"><div class="titlepage"><div><div><h3 class="title"><a name="ops.regionmgt.majorcompact"></a>1.2.1.&nbsp;Major Compaction</h3></div></div></div><p>Major compactions can be requested via the HBase shell or <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29" target="_top">HBaseAdmin.majorCompact</a>.
      </p><p>Note:  major compactions do NOT do region merges.  See <a class="xref" href="#">???</a> for more information about compactions.

      </p></div><div class="section" title="1.2.2.&nbsp;Merge"><div class="titlepage"><div><div><h3 class="title"><a name="ops.regionmgt.merge"></a>1.2.2.&nbsp;Merge</h3></div></div></div><p>Merge is a utility that can merge adjoining regions in the same table (see org.apache.hadoop.hbase.util.Merge).</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.util.Merge &lt;tablename&gt; &lt;region1&gt; &lt;region2&gt;
</pre><p>If you feel you have too many regions and want to consolidate them, Merge is the utility you need.  Merge must
      run be done when the cluster is down.
      See the <a class="link" href="http://ofps.oreilly.com/titles/9781449396107/performance.html" target="_top">O'Reilly HBase Book</a> for
      an example of usage.
      </p><p>You will need to pass 3 parameters to this application. The first one is the table name. The second one is the fully
      qualified name of the first region to merge, like "table_name,\x0A,1342956111995.7cef47f192318ba7ccc75b1bbf27a82b.". The third one
      is the fully qualified name for the second region to merge.
      </p><p>Additionally, there is a Ruby script attached to <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1621" target="_top">HBASE-1621</a>
      for region merging.
      </p></div></div><div class="section" title="1.3.&nbsp;Node Management"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="node.management"></a>1.3.&nbsp;Node Management</h2></div></div></div><div class="section" title="1.3.1.&nbsp;Node Decommission"><div class="titlepage"><div><div><h3 class="title"><a name="decommission"></a>1.3.1.&nbsp;Node Decommission</h3></div></div></div><p>You can stop an individual RegionServer by running the following
            script in the HBase directory on the particular  node:
            </p><pre class="programlisting">$ ./bin/hbase-daemon.sh stop regionserver</pre><p>
            The RegionServer will first close all regions and then shut itself down.
            On shutdown, the RegionServer's ephemeral node in ZooKeeper will expire.
            The master will notice the RegionServer gone and will treat it as
            a 'crashed' server; it will reassign the nodes the RegionServer was carrying.
            </p><div class="note" title="Disable the Load Balancer before Decommissioning a node" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Disable the Load Balancer before Decommissioning a node</h3><p>If the load balancer runs while a node is shutting down, then
                 there could be contention between the Load Balancer and the
                 Master's recovery of the just decommissioned RegionServer.
                 Avoid any problems by disabling the balancer first.
                 See <a class="xref" href="#lb" title="Load Balancer">Load Balancer</a> below.
             </p></div><p>
        </p><p>
        A downside to the above stop of a RegionServer is that regions could be offline for
        a good period of time.  Regions are closed in order.  If many regions on the server, the
        first region to close may not be back online until all regions close and after the master
        notices the RegionServer's znode gone.  In Apache HBase 0.90.2, we added facility for having
        a node gradually shed its load and then shutdown itself down. Apache HBase 0.90.2 added the
            <code class="filename">graceful_stop.sh</code> script.  Here is its usage:
            </p><pre class="programlisting">$ ./bin/graceful_stop.sh
Usage: graceful_stop.sh [--config &amp;conf-dir&gt;] [--restart] [--reload] [--thrift] [--rest] &amp;hostname&gt;
 thrift      If we should stop/start thrift before/after the hbase stop/start
 rest        If we should stop/start rest before/after the hbase stop/start
 restart     If we should restart after graceful stop
 reload      Move offloaded regions back on to the stopped server
 debug       Move offloaded regions back on to the stopped server
 hostname    Hostname of server we are to stop</pre><p>
        </p><p>
            To decommission a loaded RegionServer, run the following:
            </p><pre class="programlisting">$ ./bin/graceful_stop.sh HOSTNAME</pre><p>
            where <code class="varname">HOSTNAME</code> is the host carrying the RegionServer
            you would decommission.
            </p><div class="note" title="On HOSTNAME" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">On <code class="varname">HOSTNAME</code></h3><p>The <code class="varname">HOSTNAME</code> passed to <code class="filename">graceful_stop.sh</code>
            must match the hostname that hbase is using to identify RegionServers.
            Check the list of RegionServers in the master UI for how HBase is
            referring to servers. Its usually hostname but can also be FQDN.
            Whatever HBase is using, this is what you should pass the
            <code class="filename">graceful_stop.sh</code> decommission
            script.  If you pass IPs, the script is not yet smart enough to make
            a hostname (or FQDN) of it and so it will fail when it checks if server is
            currently running; the graceful unloading of regions will not run.
            </p></div><p> The <code class="filename">graceful_stop.sh</code> script will move the regions off the
            decommissioned RegionServer one at a time to minimize region churn.
            It will verify the region deployed in the new location before it
            will moves the next region and so on until the decommissioned server
            is carrying zero regions.  At this point, the <code class="filename">graceful_stop.sh</code>
            tells the RegionServer <span class="command"><strong>stop</strong></span>.  The master will at this point notice the
            RegionServer gone but all regions will have already been redeployed
            and because the RegionServer went down cleanly, there will be no
            WAL logs to split.
            </p><div class="note" title="Load Balancer" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="lb"></a>Load Balancer</h3><p>
                It is assumed that the Region Load Balancer is disabled while the
                <span class="command"><strong>graceful_stop</strong></span> script runs (otherwise the balancer
                and the decommission script will end up fighting over region deployments).
                Use the shell to disable the balancer:
                </p><pre class="programlisting">hbase(main):001:0&gt; balance_switch false
true
0 row(s) in 0.3590 seconds</pre><p>
This turns the balancer OFF.  To reenable, do:
                </p><pre class="programlisting">hbase(main):001:0&gt; balance_switch true
false
0 row(s) in 0.3590 seconds</pre><p>
            </p><p>The <span class="command"><strong>graceful_stop</strong></span> will check the balancer
                and if enabled, will turn it off before it goes to work.  If it
                exits prematurely because of error, it will not have reset the
                balancer.  Hence, it is better to manage the balancer apart from
                <span class="command"><strong>graceful_stop</strong></span> reenabling it after you are done
                w/ graceful_stop.
            </p></div><p>
        </p><div class="section" title="1.3.1.1.&nbsp;Decommissioning several Regions Servers concurrently"><div class="titlepage"><div><div><h4 class="title"><a name="draining.servers"></a>1.3.1.1.&nbsp;Decommissioning several Regions Servers concurrently</h4></div></div></div><p>If you have a large cluster, you may want to
            decommission more than one machine at a time by gracefully
            stopping mutiple RegionServers concurrently.
            To gracefully drain multiple regionservers at the
	    same time, RegionServers can be put into a "draining"
	    state.  This is done by marking a RegionServer as a
	    draining node by creating an entry in ZooKeeper under the
        <code class="filename">hbase_root/draining</code> znode.  This znode has format
        </p><pre class="programlisting">name,port,startcode</pre><p> just like the regionserver entries
        under <code class="filename">hbase_root/rs</code> znode.
	    </p><p>Without this facility, decommissioning mulitple nodes
	    may be non-optimal because regions that are being drained
	    from one region server may be moved to other regionservers that
	    are also draining.  Marking RegionServers to be in the
        draining state prevents this from happening<sup>[<a name="d4279e528" href="#ftn.d4279e528" class="footnote">1</a>]</sup>.
	    </p></div><div class="section" title="1.3.1.2.&nbsp;Bad or Failing Disk"><div class="titlepage"><div><div><h4 class="title"><a name="bad.disk"></a>1.3.1.2.&nbsp;Bad or Failing Disk</h4></div></div></div><p>It is good having <a class="xref" href="#">???</a> set if you have a decent number of disks
            per machine for the case where a disk plain dies.  But usually disks do the "John Wayne" -- i.e. take a while
            to go down spewing errors in <code class="filename">dmesg</code> -- or for some reason, run much slower than their
            companions.  In this case you want to decommission the disk.  You have two options.  You can
            <a class="link" href="http://wiki.apache.org/hadoop/FAQ#I_want_to_make_a_large_cluster_smaller_by_taking_out_a_bunch_of_nodes_simultaneously._How_can_this_be_done.3F" target="_top">decommission the datanode</a>
            or, less disruptive in that only the bad disks data will be rereplicated, can stop the datanode,
            unmount the bad volume (You can't umount a volume while the datanode is using it), and then restart the
            datanode (presuming you have set dfs.datanode.failed.volumes.tolerated &gt; 0).  The regionserver will
            throw some errors in its logs as it recalibrates where to get its data from -- it will likely
            roll its WAL log too -- but in general but for some latency spikes, it should keep on chugging.
            </p><div class="note" title="Short Circuit Reads" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Short Circuit Reads</h3><p>If you are doing short-circuit reads, you will have to move the regions off the regionserver
                    before you stop the datanode; when short-circuiting reading, though chmod'd so regionserver cannot
                    have access, because it already has the files open, it will be able to keep reading the file blocks
                    from the bad disk even though the datanode is down.  Move the regions back after you restart the
                datanode.</p></div><p>
            </p></div></div><div class="section" title="1.3.2.&nbsp;Rolling Restart"><div class="titlepage"><div><div><h3 class="title"><a name="rolling"></a>1.3.2.&nbsp;Rolling Restart</h3></div></div></div><p>
            You can also ask this script to restart a RegionServer after the shutdown
            AND move its old regions back into place.  The latter you might do to
            retain data locality.  A primitive rolling restart might be effected by
            running something like the following:
            </p><pre class="programlisting">$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &amp;&gt; /tmp/log.txt &amp;
            </pre><p>
            Tail the output of <code class="filename">/tmp/log.txt</code> to follow the scripts
            progress. The above does RegionServers only.  The script will also disable the
            load balancer before moving the regions.  You'd need to do the master
            update separately.  Do it before you run the above script.
            Here is a pseudo-script for how you might craft a rolling restart script:
            </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Untar your release, make sure of its configuration and
                        then rsync it across the cluster. If this is 0.90.2, patch it
                        with HBASE-3744 and HBASE-3756.
                    </p></li><li class="listitem"><p>Run hbck to ensure the cluster consistent
                        </p><pre class="programlisting">$ ./bin/hbase hbck</pre><p>
                    Effect repairs if inconsistent.
                    </p></li><li class="listitem"><p>Restart the Master: </p><pre class="programlisting">$ ./bin/hbase-daemon.sh stop master; ./bin/hbase-daemon.sh start master</pre><p>
                    </p></li><li class="listitem"><p>Run the <code class="filename">graceful_stop.sh</code> script per RegionServer.  For example:
            </p><pre class="programlisting">$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &amp;&gt; /tmp/log.txt &amp;
            </pre><p>
                     If you are running thrift or rest servers on the RegionServer, pass --thrift or --rest options (See usage
                     for <code class="filename">graceful_stop.sh</code> script).
                 </p></li><li class="listitem"><p>Restart the Master again.  This will clear out dead servers list and reenable the balancer.
                    </p></li><li class="listitem"><p>Run hbck to ensure the cluster is consistent.
                    </p></li></ol></div><p>
        </p><p>It is important to drain HBase regions slowly when
	restarting regionservers. Otherwise, multiple regions go
	offline simultaneously as they are re-assigned to other
	nodes. Depending on your usage patterns, this might not be
	desirable.
	</p></div><div class="section" title="1.3.3.&nbsp;Adding a New Node"><div class="titlepage"><div><div><h3 class="title"><a name="adding.new.node"></a>1.3.3.&nbsp;Adding a New Node</h3></div></div></div><p>Adding a new regionserver in HBase is essentially free, you simply start it like this:
              </p><pre class="programlisting">$ ./bin/hbase-daemon.sh start regionserver</pre><p>
              and it will register itself with the master. Ideally you also started a DataNode on the same
              machine so that the RS can eventually start to have local files. If you rely on ssh to start your
              daemons, don't forget to add the new hostname in <code class="filename">conf/regionservers</code> on the master.
        </p><p>At this point the region server isn't serving data because no regions have moved to it yet. If the balancer is
              enabled, it will start moving regions to the new RS. On a small/medium cluster this can have a very adverse effect
              on latency as a lot of regions will be offline at the same time. It is thus recommended to disable the balancer
              the same way it's done when decommissioning a node and move the regions manually (or even better, using a script
              that moves them one by one).
        </p><p>The moved regions will all have 0% locality and won't have any blocks in cache so the region server will have
              to use the network to serve requests. Apart from resulting in higher latency, it may also be able to use all of
              your network card's capacity. For practical purposes, consider that a standard 1GigE NIC won't be able to read
              much more than <span class="emphasis"><em>100MB/s</em></span>. In this case, or if you are in a OLAP environment and require having
              locality, then it is recommended to major compact the moved regions.
        </p></div></div><div class="section" title="1.4.&nbsp;HBase Metrics"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase_metrics"></a>1.4.&nbsp;HBase Metrics</h2></div></div></div><div class="section" title="1.4.1.&nbsp;Metric Setup"><div class="titlepage"><div><div><h3 class="title"><a name="metric_setup"></a>1.4.1.&nbsp;Metric Setup</h3></div></div></div><p>See <a class="link" href="http://hbase.apache.org/metrics.html" target="_top">Metrics</a> for
  an introduction and how to enable Metrics emission.  Still valid for HBase 0.94.x.
  </p><p>For HBase 0.95.x and up, see <a class="link" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/metrics2/package-summary.html" target="_top">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/metrics2/package-summary.html</a>
  </p></div><div class="section" title="1.4.2.&nbsp;Warning To Ganglia Users"><div class="titlepage"><div><div><h3 class="title"><a name="rs_metrics_ganglia"></a>1.4.2.&nbsp;Warning To Ganglia Users</h3></div></div></div><p>Warning to Ganglia Users:  by default, HBase will emit a LOT of metrics per RegionServer which may swamp your installation.
     Options include either increasing Ganglia server capacity, or configuring HBase to emit fewer metrics.
     </p></div><div class="section" title="1.4.3.&nbsp;Most Important RegionServer Metrics"><div class="titlepage"><div><div><h3 class="title"><a name="rs_metrics"></a>1.4.3.&nbsp;Most Important RegionServer Metrics</h3></div></div></div><div class="section" title="1.4.3.1.&nbsp;blockCacheExpressCachingRatio (formerly blockCacheHitCachingRatio)"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheHitCachingRatio"></a>1.4.3.1.&nbsp;<code class="varname">blockCacheExpressCachingRatio (formerly blockCacheHitCachingRatio)</code></h4></div></div></div><p>Block cache hit caching ratio (0 to 100).  The cache-hit ratio for reads configured to look in the cache (i.e., cacheBlocks=true). </p></div><div class="section" title="1.4.3.2.&nbsp;callQueueLength"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.callQueueLength"></a>1.4.3.2.&nbsp;<code class="varname">callQueueLength</code></h4></div></div></div><p>Point in time length of the RegionServer call queue.  If requests arrive faster than the RegionServer handlers can process
          them they will back up in the callQueue.</p></div><div class="section" title="1.4.3.3.&nbsp;compactionQueueLength (formerly compactionQueueSize)"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.compactionQueueSize"></a>1.4.3.3.&nbsp;<code class="varname">compactionQueueLength (formerly compactionQueueSize)</code></h4></div></div></div><p>Point in time length of the compaction queue.  This is the number of Stores in the RegionServer that have been targeted for compaction.</p></div><div class="section" title="1.4.3.4.&nbsp;flushQueueSize"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.flushQueueSize"></a>1.4.3.4.&nbsp;<code class="varname">flushQueueSize</code></h4></div></div></div><p>Point in time number of enqueued regions in the MemStore awaiting flush.</p></div><div class="section" title="1.4.3.5.&nbsp;hdfsBlocksLocalityIndex"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.hdfsBlocksLocalityIndex"></a>1.4.3.5.&nbsp;<code class="varname">hdfsBlocksLocalityIndex</code></h4></div></div></div><p>Point in time percentage of HDFS blocks that are local to this RegionServer.  The higher the better.  </p></div><div class="section" title="1.4.3.6.&nbsp;memstoreSizeMB"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.memstoreSizeMB"></a>1.4.3.6.&nbsp;<code class="varname">memstoreSizeMB</code></h4></div></div></div><p>Point in time sum of all the memstore sizes in this RegionServer (MB).  Watch for this nearing or exceeding
          the configured high-watermark for MemStore memory in the RegionServer. </p></div><div class="section" title="1.4.3.7.&nbsp;numberOfOnlineRegions"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.regions"></a>1.4.3.7.&nbsp;<code class="varname">numberOfOnlineRegions</code></h4></div></div></div><p>Point in time number of regions served by the RegionServer.  This is an important metric to track for RegionServer-Region density.
          </p></div><div class="section" title="1.4.3.8.&nbsp;readRequestsCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.readRequestsCount"></a>1.4.3.8.&nbsp;<code class="varname">readRequestsCount</code></h4></div></div></div><p>Number of read requests for this RegionServer since startup.  Note:  this is a 32-bit integer and can roll. </p></div><div class="section" title="1.4.3.9.&nbsp;slowHLogAppendCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.slowHLogAppendCount"></a>1.4.3.9.&nbsp;<code class="varname">slowHLogAppendCount</code></h4></div></div></div><p>Number of slow HLog append writes for this RegionServer since startup, where "slow" is &gt; 1 second.  This is
          a good "canary" metric for HDFS. </p></div><div class="section" title="1.4.3.10.&nbsp;usedHeapMB"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.usedHeapMB"></a>1.4.3.10.&nbsp;<code class="varname">usedHeapMB</code></h4></div></div></div><p>Point in time amount of memory used by the RegionServer (MB).</p></div><div class="section" title="1.4.3.11.&nbsp;writeRequestsCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.writeRequestsCount"></a>1.4.3.11.&nbsp;<code class="varname">writeRequestsCount</code></h4></div></div></div><p>Number of write requests for this RegionServer since startup.  Note:  this is a 32-bit integer and can roll. </p></div></div><div class="section" title="1.4.4.&nbsp;Other RegionServer Metrics"><div class="titlepage"><div><div><h3 class="title"><a name="rs_metrics_other"></a>1.4.4.&nbsp;Other RegionServer Metrics</h3></div></div></div><div class="section" title="1.4.4.1.&nbsp;blockCacheCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheCount"></a>1.4.4.1.&nbsp;<code class="varname">blockCacheCount</code></h4></div></div></div><p>Point in time block cache item count in memory.  This is the number of blocks of StoreFiles (HFiles) in the cache.</p></div><div class="section" title="1.4.4.2.&nbsp;blockCacheEvictedCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheEvictedCount"></a>1.4.4.2.&nbsp;<code class="varname">blockCacheEvictedCount</code></h4></div></div></div><p>Number of blocks that had to be evicted from the block cache due to heap size constraints by RegionServer since startup.</p></div><div class="section" title="1.4.4.3.&nbsp;blockCacheFreeMB"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheFree"></a>1.4.4.3.&nbsp;<code class="varname">blockCacheFreeMB</code></h4></div></div></div><p>Point in time block cache memory available (MB).</p></div><div class="section" title="1.4.4.4.&nbsp;blockCacheHitCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheHitCount"></a>1.4.4.4.&nbsp;<code class="varname">blockCacheHitCount</code></h4></div></div></div><p>Number of blocks of StoreFiles (HFiles) read from the cache by RegionServer since startup.</p></div><div class="section" title="1.4.4.5.&nbsp;blockCacheHitRatio"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheHitRatio"></a>1.4.4.5.&nbsp;<code class="varname">blockCacheHitRatio</code></h4></div></div></div><p>Block cache hit ratio (0 to 100) from RegionServer startup.  Includes all read requests, although those with cacheBlocks=false
           will always read from disk and be counted as a "cache miss", which means that full-scan MapReduce jobs can affect
           this metric significantly.</p></div><div class="section" title="1.4.4.6.&nbsp;blockCacheMissCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheMissCount"></a>1.4.4.6.&nbsp;<code class="varname">blockCacheMissCount</code></h4></div></div></div><p>Number of blocks of StoreFiles (HFiles) requested but not read from the cache from RegionServer startup.</p></div><div class="section" title="1.4.4.7.&nbsp;blockCacheSizeMB"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheSize"></a>1.4.4.7.&nbsp;<code class="varname">blockCacheSizeMB</code></h4></div></div></div><p>Point in time block cache size in memory (MB).  i.e., memory in use by the BlockCache</p></div><div class="section" title="1.4.4.8.&nbsp;fsPreadLatency*"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsPreadLatency"></a>1.4.4.8.&nbsp;<code class="varname">fsPreadLatency*</code></h4></div></div></div><p>There are several filesystem positional read latency (ms) metrics, all measured from RegionServer startup.</p></div><div class="section" title="1.4.4.9.&nbsp;fsReadLatency*"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsReadLatency"></a>1.4.4.9.&nbsp;<code class="varname">fsReadLatency*</code></h4></div></div></div><p>There are several filesystem read latency (ms) metrics, all measured from RegionServer startup.  The issue with
          interpretation is that ALL reads go into this metric (e.g., single-record Gets, full table Scans), including
          reads required for compactions.  This metric is only interesting "over time" when comparing
          major releases of HBase or your own code.</p></div><div class="section" title="1.4.4.10.&nbsp;fsWriteLatency*"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsWriteLatency"></a>1.4.4.10.&nbsp;<code class="varname">fsWriteLatency*</code></h4></div></div></div><p>There are several filesystem write latency (ms) metrics, all measured from RegionServer startup.  The issue with
          interpretation is that ALL writes go into this metric (e.g., single-record Puts, full table re-writes due to compaction).
          This metric is only interesting "over time" when comparing
          major releases of HBase or your own code.</p></div><div class="section" title="1.4.4.11.&nbsp;NumberOfStores"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.stores"></a>1.4.4.11.&nbsp;<code class="varname">NumberOfStores</code></h4></div></div></div><p>Point in time number of Stores open on the RegionServer.  A Store corresponds to a ColumnFamily.  For example,
          if a table (which contains the column family) has 3 regions on a RegionServer, there will be 3 stores open for that
          column family. </p></div><div class="section" title="1.4.4.12.&nbsp;NumberOfStorefiles"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.storeFiles"></a>1.4.4.12.&nbsp;<code class="varname">NumberOfStorefiles</code></h4></div></div></div><p>Point in time number of StoreFiles open on the RegionServer.  A store may have more than one StoreFile (HFile).</p></div><div class="section" title="1.4.4.13.&nbsp;requestsPerSecond"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.requests"></a>1.4.4.13.&nbsp;<code class="varname">requestsPerSecond</code></h4></div></div></div><p>Point in time number of read and write requests.  Requests correspond to RegionServer RPC calls,
           thus a single Get will result in 1 request, but a Scan with caching set to 1000 will result in 1 request for each 'next' call
            (i.e., not each row).  A bulk-load request will constitute 1 request per HFile.
            This metric is less interesting than readRequestsCount and writeRequestsCount in terms of measuring activity
            due to this metric being periodic. </p></div><div class="section" title="1.4.4.14.&nbsp;storeFileIndexSizeMB"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.storeFileIndexSizeMB"></a>1.4.4.14.&nbsp;<code class="varname">storeFileIndexSizeMB</code></h4></div></div></div><p>Point in time sum of all the StoreFile index sizes in this RegionServer (MB)</p></div></div></div><div class="section" title="1.5.&nbsp;HBase Monitoring"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.monitoring"></a>1.5.&nbsp;HBase Monitoring</h2></div></div></div><div class="section" title="1.5.1.&nbsp;Overview"><div class="titlepage"><div><div><h3 class="title"><a name="ops.monitoring.overview"></a>1.5.1.&nbsp;Overview</h3></div></div></div><p>The following metrics are arguably the most important to monitor for each RegionServer for
      "macro monitoring", preferably with a system like <a class="link" href="http://opentsdb.net/" target="_top">OpenTSDB</a>.
      If your cluster is having performance issues it's likely that you'll see something unusual with
      this group.
      </p><p>HBase:
      </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">See <a class="xref" href="#rs_metrics" title="1.4.3.&nbsp;Most Important RegionServer Metrics">Section&nbsp;1.4.3, &#8220;Most Important RegionServer Metrics&#8221;</a></li></ul></div><p>
      </p><p>OS:
      </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">IO Wait</li><li class="listitem">User CPU</li></ul></div><p>
      </p><p>Java:
      </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">GC</li></ul></div><p>
      </p><p>
      </p><p>
      For more information on HBase metrics, see <a class="xref" href="#hbase_metrics" title="1.4.&nbsp;HBase Metrics">Section&nbsp;1.4, &#8220;HBase Metrics&#8221;</a>.
      </p></div><div class="section" title="1.5.2.&nbsp;Slow Query Log"><div class="titlepage"><div><div><h3 class="title"><a name="ops.slow.query"></a>1.5.2.&nbsp;Slow Query Log</h3></div></div></div><p>The HBase slow query log consists of parseable JSON structures describing the properties of those client operations (Gets, Puts, Deletes, etc.) that either took too long to run, or produced too much output. The thresholds for "too long to run" and "too much output" are configurable, as described below. The output is produced inline in the main region server logs so that it is easy to discover further details from context with other logged events. It is also prepended with identifying tags <code class="constant">(responseTooSlow)</code>, <code class="constant">(responseTooLarge)</code>, <code class="constant">(operationTooSlow)</code>, and <code class="constant">(operationTooLarge)</code> in order to enable easy filtering with grep, in case the user desires to see only slow queries.
</p><div class="section" title="1.5.2.1.&nbsp;Configuration"><div class="titlepage"><div><div><h4 class="title"><a name="d4279e852"></a>1.5.2.1.&nbsp;Configuration</h4></div></div></div><p>There are two configuration knobs that can be used to adjust the thresholds for when queries are logged.
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="varname">hbase.ipc.warn.response.time</code> Maximum number of milliseconds that a query can be run without being logged. Defaults to 10000, or 10 seconds. Can be set to -1 to disable logging by time.
</li><li class="listitem"><code class="varname">hbase.ipc.warn.response.size</code> Maximum byte size of response that a query can return without being logged. Defaults to 100 megabytes. Can be set to -1 to disable logging by size.
</li></ul></div></div><div class="section" title="1.5.2.2.&nbsp;Metrics"><div class="titlepage"><div><div><h4 class="title"><a name="d4279e866"></a>1.5.2.2.&nbsp;Metrics</h4></div></div></div><p>The slow query log exposes to metrics to JMX.
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="varname">hadoop.regionserver_rpc_slowResponse</code> a global metric reflecting the durations of all responses that triggered logging.</li><li class="listitem"><code class="varname">hadoop.regionserver_rpc_methodName.aboveOneSec</code> A metric reflecting the durations of all responses that lasted for more than one second.</li></ul></div><p>
</p></div><div class="section" title="1.5.2.3.&nbsp;Output"><div class="titlepage"><div><div><h4 class="title"><a name="d4279e881"></a>1.5.2.3.&nbsp;Output</h4></div></div></div><p>The output is tagged with operation e.g. <code class="constant">(operationTooSlow)</code> if the call was a client operation, such as a Put, Get, or Delete, which we expose detailed fingerprint information for. If not, it is tagged <code class="constant">(responseTooSlow)</code> and still produces parseable JSON output, but with less verbose information solely regarding its duration and size in the RPC itself. <code class="constant">TooLarge</code> is substituted for <code class="constant">TooSlow</code> if the response size triggered the logging, with <code class="constant">TooLarge</code> appearing even in the case that both size and duration triggered logging.
</p></div><div class="section" title="1.5.2.4.&nbsp;Example"><div class="titlepage"><div><div><h4 class="title"><a name="d4279e901"></a>1.5.2.4.&nbsp;Example</h4></div></div></div><p>
</p><pre class="programlisting">2011-09-08 10:01:25,824 WARN org.apache.hadoop.ipc.HBaseServer: (operationTooSlow): {"tables":{"riley2":{"puts":[{"totalColumns":11,"families":{"actions":[{"timestamp":1315501284459,"qualifier":"0","vlen":9667580},{"timestamp":1315501284459,"qualifier":"1","vlen":10122412},{"timestamp":1315501284459,"qualifier":"2","vlen":11104617},{"timestamp":1315501284459,"qualifier":"3","vlen":13430635}]},"row":"cfcd208495d565ef66e7dff9f98764da:0"}],"families":["actions"]}},"processingtimems":956,"client":"10.47.34.63:33623","starttimems":1315501284456,"queuetimems":0,"totalPuts":1,"class":"HRegionServer","responsesize":0,"method":"multiPut"}</pre><p>
</p><p>Note that everything inside the "tables" structure is output produced by MultiPut's fingerprint, while the rest of the information is RPC-specific, such as processing time and client IP/port. Other client operations follow the same pattern and the same general structure, with necessary differences due to the nature of the individual operations. In the case that the call is not a client operation, that detailed fingerprint information will be completely absent.
</p><p>This particular example, for example, would indicate that the likely cause of slowness is simply a very large (on the order of 100MB) multiput, as we can tell by the "vlen," or value length, fields of each put in the multiPut.
</p></div></div></div><div class="section" title="1.6.&nbsp;Cluster Replication"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="cluster_replication"></a>1.6.&nbsp;Cluster Replication</h2></div></div></div><p>See <a class="link" href="http://hbase.apache.org/replication.html" target="_top">Cluster Replication</a>.
    </p></div><div class="section" title="1.7.&nbsp;HBase Backup"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.backup"></a>1.7.&nbsp;HBase Backup</h2></div></div></div><p>There are two broad strategies for performing HBase backups: backing up with a full cluster shutdown, and backing up on a live cluster.
    Each approach has pros and cons.
    </p><p>For additional information, see <a class="link" href="http://blog.sematext.com/2011/03/11/hbase-backup-options/" target="_top">HBase Backup Options</a> over on the Sematext Blog.
    </p><div class="section" title="1.7.1.&nbsp;Full Shutdown Backup"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.fullshutdown"></a>1.7.1.&nbsp;Full Shutdown Backup</h3></div></div></div><p>Some environments can tolerate a periodic full shutdown of their HBase cluster, for example if it is being used a back-end analytic capacity
      and not serving front-end web-pages.  The benefits are that the NameNode/Master are RegionServers are down, so there is no chance of missing
      any in-flight changes to either StoreFiles or metadata.  The obvious con is that the cluster is down.  The steps include:
      </p><div class="section" title="1.7.1.1.&nbsp;Stop HBase"><div class="titlepage"><div><div><h4 class="title"><a name="ops.backup.fullshutdown.stop"></a>1.7.1.1.&nbsp;Stop HBase</h4></div></div></div><p>
        </p></div><div class="section" title="1.7.1.2.&nbsp;Distcp"><div class="titlepage"><div><div><h4 class="title"><a name="ops.backup.fullshutdown.distcp"></a>1.7.1.2.&nbsp;Distcp</h4></div></div></div><p>Distcp could be used to either copy the contents of the HBase directory in HDFS to either the same cluster in another directory, or
        to a different cluster.
        </p><p>Note:  Distcp works in this situation because the cluster is down and there are no in-flight edits to files.
        Distcp-ing of files in the HBase directory is not generally recommended on a live cluster.
        </p></div><div class="section" title="1.7.1.3.&nbsp;Restore (if needed)"><div class="titlepage"><div><div><h4 class="title"><a name="ops.backup.fullshutdown.restore"></a>1.7.1.3.&nbsp;Restore (if needed)</h4></div></div></div><p>The backup of the hbase directory from HDFS is copied onto the 'real' hbase directory via distcp.  The act of copying these files
        creates new HDFS metadata, which is why a restore of the NameNode edits from the time of the HBase backup isn't required for this kind of
        restore, because it's a restore (via distcp) of a specific HDFS directory (i.e., the HBase part) not the entire HDFS file-system.
        </p></div></div><div class="section" title="1.7.2.&nbsp;Live Cluster Backup - Replication"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.live.replication"></a>1.7.2.&nbsp;Live Cluster Backup - Replication</h3></div></div></div><p>This approach assumes that there is a second cluster.
      See the HBase page on <a class="link" href="http://hbase.apache.org/replication.html" target="_top">replication</a> for more information.
      </p></div><div class="section" title="1.7.3.&nbsp;Live Cluster Backup - CopyTable"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.live.copytable"></a>1.7.3.&nbsp;Live Cluster Backup - CopyTable</h3></div></div></div><p>The <a class="xref" href="#copytable" title="1.1.7.&nbsp;CopyTable">Section&nbsp;1.1.7, &#8220;CopyTable&#8221;</a> utility could either be used to copy data from one table to another on the
      same cluster, or to copy data to another table on another cluster.
      </p><p>Since the cluster is up, there is a risk that edits could be missed in the copy process.
      </p></div><div class="section" title="1.7.4.&nbsp;Live Cluster Backup - Export"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.live.export"></a>1.7.4.&nbsp;Live Cluster Backup - Export</h3></div></div></div><p>The <a class="xref" href="#export" title="1.1.8.&nbsp;Export">Section&nbsp;1.1.8, &#8220;Export&#8221;</a> approach dumps the content of a table to HDFS on the same cluster.  To restore the data, the
      <a class="xref" href="#import" title="1.1.9.&nbsp;Import">Section&nbsp;1.1.9, &#8220;Import&#8221;</a> utility would be used.
      </p><p>Since the cluster is up, there is a risk that edits could be missed in the export process.
      </p></div></div><div class="section" title="1.8.&nbsp;HBase Snapshots"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.snapshots"></a>1.8.&nbsp;HBase Snapshots</h2></div></div></div><p>HBase Snapshots allow you to take a snapshot of a table without too much impact on Region Servers.
      Snapshot, Clone and restore operations don't involve data copying.
      Also, Exporting the snapshot to another cluster doesn't have impact on the Region Servers.
    </p><p>Prior to version 0.94.6, the only way to backup or to clone a table is to use CopyTable/ExportTable,
      or to copy all the hfiles in HDFS after disabling the table.
      The disadvantages of these methods are that you can degrade region server performance
      (Copy/Export Table) or you need to disable the table, that means no reads or writes;
      and this is usually unacceptable.
    </p><div class="section" title="1.8.1.&nbsp;Configuration"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.configuration"></a>1.8.1.&nbsp;Configuration</h3></div></div></div><p>To turn on the snapshot support just set the
        <code class="varname">hbase.snapshot.enabled</code> property to true.
        (Snapshots are enabled by default in 0.95+ and off by default in 0.94.6+)
        </p><pre class="programlisting">
  &lt;property&gt;
    &lt;name&gt;hbase.snapshot.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
        </pre><p>
      </p></div><div class="section" title="1.8.2.&nbsp;Take a Snapshot"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.takeasnapshot"></a>1.8.2.&nbsp;Take a Snapshot</h3></div></div></div><p>You can take a snapshot of a table regardless of whether it is enabled or disabled.
        The snapshot operation doesn't involve any data copying.
        </p><pre class="programlisting">
    $ ./bin/hbase shell
    hbase&gt; snapshot 'myTable', 'myTableSnapshot-122112'
        </pre><p>
      </p></div><div class="section" title="1.8.3.&nbsp;Listing Snapshots"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.list"></a>1.8.3.&nbsp;Listing Snapshots</h3></div></div></div><p>List all snapshots taken (by printing the names and relative information).
        </p><pre class="programlisting">
    $ ./bin/hbase shell
    hbase&gt; list_snapshots
        </pre><p>
      </p></div><div class="section" title="1.8.4.&nbsp;Deleting Snapshots"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.delete"></a>1.8.4.&nbsp;Deleting Snapshots</h3></div></div></div><p>You can remove a snapshot, and the files retained for that snapshot will be removed
        if no longer needed.
        </p><pre class="programlisting">
    $ ./bin/hbase shell
    hbase&gt; delete_snapshot 'myTableSnapshot-122112'
        </pre><p>
      </p></div><div class="section" title="1.8.5.&nbsp;Clone a table from snapshot"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.clone"></a>1.8.5.&nbsp;Clone a table from snapshot</h3></div></div></div><p>From a snapshot you can create a new table (clone operation) with the same data
      that you had when the snapshot was taken.
      The clone operation, doesn't involve data copies, and a change to the cloned table
      doesn't impact the snapshot or the original table.
        </p><pre class="programlisting">
    $ ./bin/hbase shell
    hbase&gt; clone_snapshot 'myTableSnapshot-122112', 'myNewTestTable'
        </pre><p>
      </p></div><div class="section" title="1.8.6.&nbsp;Restore a snapshot"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.restore"></a>1.8.6.&nbsp;Restore a snapshot</h3></div></div></div><p>The restore operation requires the table to be disabled, and the table will be
      restored to the state at the time when the snapshot was taken,
      changing both data and schema if required.
        </p><pre class="programlisting">
    $ ./bin/hbase shell
    hbase&gt; disable 'myTable'
    hbase&gt; restore_snapshot 'myTableSnapshot-122112'
        </pre><p>
      </p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>Since Replication works at log level and snapshots at file-system level,
      after a restore, the replicas will be in a different state from the master.
      If you want to use restore, you need to stop replication and redo the bootstrap.
        </p></div><p>In case of partial data-loss due to misbehaving client, instead of a full restore
      that requires the table to be disabled, you can clone the table from the snapshot
      and use a Map-Reduce job to copy the data that you need, from the clone to the main one.
      </p></div><div class="section" title="1.8.7.&nbsp;Snapshots operations and ACLs"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.acls"></a>1.8.7.&nbsp;Snapshots operations and ACLs</h3></div></div></div>
    If you are using security with the AccessController Coprocessor (See <a class="xref" href="#">???</a>),
    only a global administrator can take, clone, or restore a snapshot, and these actions do not capture the ACL rights.
    This means that restoring a table preserves the ACL rights of the existing table,
    while cloning a table creates a new table that has no ACL rights until the administrator adds them.
    </div><div class="section" title="1.8.8.&nbsp;Export to another cluster"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.export"></a>1.8.8.&nbsp;Export to another cluster</h3></div></div></div><p>The ExportSnapshot tool copies all the data related to a snapshot (hfiles, logs, snapshot metadata) to another cluster.
        The tool executes a Map-Reduce job, similar to distcp, to copy files between the two clusters,
        and since it works at file-system level the hbase cluster does not have to be online.
        </p><p>To copy a snapshot called MySnapshot to an HBase cluster srv2 (hdfs:///srv2:8082/hbase) using 16 mappers:
</p><pre class="programlisting">$ bin/hbase class org.apache.hadoop.hbase.snapshot.tool.ExportSnapshot -snapshot MySnapshot -copy-to hdfs:///srv2:8082/hbase -mappers 16</pre><p>
        </p><p>
      </p></div></div><div class="section" title="1.9.&nbsp;Capacity Planning and Region Sizing"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.capacity"></a>1.9.&nbsp;Capacity Planning and Region Sizing</h2></div></div></div><p>There are several considerations when planning the capacity for an HBase cluster and performing the initial configuration. Start with a solid understanding of how HBase handles data internally.</p><div class="section" title="1.9.1.&nbsp;Node count and hardware/VM configuration"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.nodes"></a>1.9.1.&nbsp;Node count and hardware/VM configuration</h3></div></div></div><div class="section" title="1.9.1.1.&nbsp;Physical data size"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.nodes.datasize"></a>1.9.1.1.&nbsp;Physical data size</h4></div></div></div><p>Physical data size on disk is distinct from logical size of your data and is affected by the following:
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Increased by HBase overhead
<div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">See <a class="xref" href="#">???</a> and <a class="xref" href="#">???</a>. At least 24 bytes per key-value (cell), can be more. Small keys/values means more relative overhead.</li><li class="listitem">KeyValue instances are aggregated into blocks, which are indexed. Indexes also have to be stored. Blocksize is configurable on a per-ColumnFamily basis. See <a class="xref" href="#">???</a>.</li></ul></div></li><li class="listitem">Decreased by <a class="xref" href="#">???</a> and data block encoding, depending on data. See also <a class="ulink" href="http://search-hadoop.com/m/lL12B1PFVhp1" target="_top">this thread</a>. You might want to test what compression and encoding (if any) make sense for your data.</li><li class="listitem">Increased by size of region server <a class="xref" href="#">???</a> (usually fixed and negligible - less than half of RS memory size, per RS).</li><li class="listitem">Increased by HDFS replication - usually x3.</li></ul></div><p>Aside from the disk space necessary to store the data, one RS may not be able to serve arbitrarily large amounts of data due to some practical limits on region count and size (see <a class="xref" href="#ops.capacity.regions" title="1.9.2.&nbsp;Determining region count and size">below</a>).</p></div><div class="section" title="1.9.1.2.&nbsp;Read/Write throughput"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.nodes.throughput"></a>1.9.1.2.&nbsp;Read/Write throughput</h4></div></div></div><p>Number of nodes can also be driven by required thoughput for reads and/or writes. The  throughput one can get per node depends a lot on data (esp. key/value sizes) and request patterns, as well as node and system configuration. Planning should be done for peak load if it is likely that the load would be the main driver of the increase of the node count. PerformanceEvaluation and <a class="xref" href="#">???</a> tools can be used to test single node or a test cluster.</p><p>For write, usually 5-15Mb/s per RS can be expected, since every region server has only one active WAL. There's no good estimate for reads, as it depends vastly on data, requests, and cache hit rate. <a class="xref" href="#">???</a> might be helpful.</p></div><div class="section" title="1.9.1.3.&nbsp;JVM GC limitations"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.nodes.gc"></a>1.9.1.3.&nbsp;JVM GC limitations</h4></div></div></div><p>RS cannot currently utilize very large heap due to cost of GC. There's also no good way of running multiple RS-es per server (other than running several VMs per machine). Thus, ~20-24Gb or less memory dedicated to one RS is recommended. GC tuning is required for large heap sizes. See <a class="xref" href="#">???</a>, <a class="xref" href="#">???</a> and elsewhere (TODO: where?)</p></div></div><div class="section" title="1.9.2.&nbsp;Determining region count and size"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.regions"></a>1.9.2.&nbsp;Determining region count and size</h3></div></div></div><p>Generally less regions makes for a smoother running cluster (you can always manually split the big regions later (if necessary) to spread the data, or request load, over the cluster); 20-200 regions per RS is a reasonable range. The number of regions cannot be configured directly (unless you go for fully <a class="xref" href="#">???</a>); adjust the region size to achieve the target region size given table size.</p><p>When configuring regions for multiple tables, note that most region settings can be set on a per-table basis via <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html" target="_top">HTableDescriptor</a>, as well as shell commands. These settings will override the ones in <code class="varname">hbase-site.xml</code>. That is useful if your tables have different workloads/use cases.</p><p>Also note that in the discussion of region sizes here, <span class="bold"><strong>HDFS replication factor is not (and should not be) taken into account, whereas other factors <a class="xref" href="#ops.capacity.nodes.datasize" title="1.9.1.1.&nbsp;Physical data size">above</a> should be.</strong></span> So, if your data is compressed and replicated 3 ways by HDFS, "9 Gb region" means 9 Gb of compressed data. HDFS replication factor only affects your disk usage and is invisible to most HBase code.</p><div class="section" title="1.9.2.1.&nbsp;Number of regions per RS - upper bound"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.count"></a>1.9.2.1.&nbsp;Number of regions per RS - upper bound</h4></div></div></div><p>In production scenarios, where you have a lot of data, you are normally concerned with the maximum number of regions you can have per server. <a class="xref" href="#">???</a> has technical discussion on the subject; in short, maximum number of regions is mostly determined by memstore memory usage. Each region has its own memstores; these grow up to a configurable size; usually in 128-256Mb range, see <a class="xref" href="#">???</a>. There's one memstore per column family (so there's only one per region if there's one CF in the table). RS dedicates some fraction of total memory (see <a class="xref" href="#">???</a>) to region memstores. If this memory is exceeded (too much memstore usage), undesirable consequences such as unresponsive server, or later compaction storms, can result. Thus, a good starting point for the number of regions per RS (assuming one table) is </p><pre class="programlisting">(RS memory)*(total memstore fraction)/((memstore size)*(# column families))</pre><p>
E.g. if RS has 16Gb RAM, with default settings, it is 16384*0.4/128 ~ 51 regions per RS is a starting point. The formula can be extended to multiple tables; if they all have the same configuration, just use total number of families.</p><p>This number can be adjusted; the formula above assumes all your regions are filled at approximately the same rate. If only a fraction of your regions are going to be actively written to, you can divide the result by that fraction to get a larger region count. Then, even if all regions are written to, all region memstores are not filled evenly, and eventually jitter appears even if they are (due to limited number of concurrent flushes). Thus, one can have as many as 2-3 times more regions than the starting point; however, increased numbers carry increased risk.</p><p>For write-heavy workload, memstore fraction can be increased in configuration at the expense of block cache; this will also allow one to have more regions.</p></div><div class="section" title="1.9.2.2.&nbsp;Number of regions per RS - lower bound"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.mincount"></a>1.9.2.2.&nbsp;Number of regions per RS - lower bound</h4></div></div></div><p>HBase scales by having regions across many servers. Thus if you have 2 regions for 16GB data, on a 20 node machine your data will be concentrated on just a few machines - nearly the entire        cluster will be idle. This really can't be stressed enough, since a common problem is loading 200MB data into HBase and then wondering why your awesome 10 node cluster isn't doing anything.</p><p>On the other hand, if you have a very large amount of data, you may also want to go for a larger number of regions to avoid having regions that are too large.</p></div><div class="section" title="1.9.2.3.&nbsp;Maximum region size"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.size"></a>1.9.2.3.&nbsp;Maximum region size</h4></div></div></div><p>For large tables in production scenarios, maximum region size is mostly limited by compactions - very large compactions, esp. major, can degrade cluster performance. Currently, the recommended maximum region size is 10-20Gb, and 5-10Gb is optimal. For older 0.90.x codebase, the upper-bound of regionsize is about 4Gb, with a default of 256Mb.</p><p>The size at which the region is split into two is generally configured via <a class="xref" href="#">???</a>; for details, see <a class="xref" href="#">???</a>.</p><p>If you cannot estimate the size of your tables well, when starting off, it's probably best to stick to the default region size, perhaps going smaller for hot tables (or manually split hot regions to spread the load over the cluster), or go with larger region sizes if your cell sizes tend to be largish (100k and up).</p><p>In HBase 0.98, experimental stripe compactions feature was added that would allow for larger regions, especially for log data. See <a class="xref" href="#">???</a>.</p></div><div class="section" title="1.9.2.4.&nbsp;Total data size per region server"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.total"></a>1.9.2.4.&nbsp;Total data size per region server</h4></div></div></div><p>According to above numbers for region size and number of regions per region server, in an optimistic estimate 10 GB x 100 regions per RS will give up to 1TB served per region server, which is in line with some of the reported multi-PB use cases. However, it is important to think about the data vs cache size ratio at the RS level. With 1TB of data per server and 10 GB block cache, only 1% of the data will be cached, which may barely cover all block indices.</p></div></div><div class="section" title="1.9.3.&nbsp;Initial configuration and tuning"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.config"></a>1.9.3.&nbsp;Initial configuration and tuning</h3></div></div></div><p>First, see <a class="xref" href="#">???</a>. Note that some configurations, more than others, depend on specific scenarios. Pay special attention to 
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><a class="xref" href="#">???</a> - request handler thread count, vital for high-throughput workloads.</li><li class="listitem"><a class="xref" href="#">???</a> - the blocking number of WAL files depends on your memstore configuration and should be set accordingly to prevent potential blocking when doing high volume of writes.</li></ul></div><p>Then, there are some considerations when setting up your cluster and tables.</p><div class="section" title="1.9.3.1.&nbsp;Compactions"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.config.compactions"></a>1.9.3.1.&nbsp;Compactions</h4></div></div></div><p>Depending on read/write volume and latency requirements, optimal compaction settings may be different. See <a class="xref" href="#">???</a> for some details.</p><p>When provisioning for large data sizes, however, it's good to keep in mind that compactions can affect write throughput. Thus, for write-intensive workloads, you may opt for less frequent compactions and more store files per regions. Minimum number of files for compactions (<code class="varname">hbase.hstore.compaction.min</code>) can be set to higher value; <a class="xref" href="#">???</a> should also be increased, as more files might accumulate in such case. You may also consider manually managing compactions: <a class="xref" href="#">???</a></p></div><div class="section" title="1.9.3.2.&nbsp;Pre-splitting the table"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.config.presplit"></a>1.9.3.2.&nbsp;Pre-splitting the table</h4></div></div></div><p>Based on the target number of the regions per RS (see <a class="xref" href="#ops.capacity.regions.count" title="1.9.2.1.&nbsp;Number of regions per RS - upper bound">above</a>) and number of RSes, one can pre-split the table at creation time. This would both avoid some costly splitting as the table starts to fill up, and ensure that the table starts out already distributed across many servers.</p><p>If the table is expected to grow large enough to justify that, at least one region per RS should be created. It is not recommended to split immediately into the full target number of regions (e.g. 50 * number of RSes), but a low intermediate value can be chosen. For multiple tables, it is recommended to be conservative with presplitting (e.g. pre-split 1 region per RS at most), especially if you don't know how much each table will grow. If you split too much, you may end up with too many regions, with some tables having too many small regions.</p><p>For pre-splitting howto, see <a class="xref" href="#">???</a>.</p></div></div></div><div class="section" title="1.10.&nbsp;Table Rename"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="table.rename"></a>1.10.&nbsp;Table Rename</h2></div></div></div><p>In versions 0.90.x of hbase and earlier, we had a simple script that would rename the hdfs
          table directory and then do an edit of the .META. table replacing all mentions of the old
          table name with the new.  The script was called <span class="command"><strong>./bin/rename_table.rb</strong></span>.
          The script was deprecated and removed mostly because it was unmaintained and the operation
          performed by the script was brutal.
      </p><p>
          As of hbase 0.94.x, you can use the snapshot facility renaming a table.  Here is how you would
do it using the hbase shell:
</p><pre class="programlisting">hbase shell&gt; disable 'tableName'
hbase shell&gt; snapshot 'tableName', 'tableSnapshot'
hbase shell&gt; clone 'tableSnapshot', 'newTableName'
hbase shell&gt; delete_snapshot 'tableSnapshot'
hbase shell&gt; drop 'tableName'</pre><p>
or in code it would be as follows:
</p><pre class="programlisting">void rename(HBaseAdmin admin, String oldTableName, String newTableName) {
    String snapshotName = randomName();
    admin.disableTable(oldTableName);
    admin.snapshot(snapshotName, oldTableName);
    admin.cloneSnapshot(snapshotName, newTableName);
    admin.deleteSnapshot(snapshotName);
    admin.deleteTable(oldTableName);
}</pre><p>
      </p></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d4279e528" href="#d4279e528" class="para">1</a>] </sup>See
	    this <a class="link" href="http://inchoate-clatter.blogspot.com/2012/03/hbase-ops-automation.html" target="_top">blog
            post</a> for more details.</p></div></div></div><div id="disqus_thread"></div><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book';
    var disqus_identifier = 'ops_mgt';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></body></html>